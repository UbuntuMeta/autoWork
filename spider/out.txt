
里氏替换原则来源：
        我们都知道面向对象有三大特性：封装、继承、多态。所以我们在实际开发过程中，子类在继承父类后，根据多态的特性，可能是图一时方便，经常任意重写父类的方法，那么这种方式会大大增加代码出问题的几率。比如下面场景：类C实现了某项功能F1。现在需要对功能F1作修改扩展，将功能F1扩展为F，其中F由原有的功能F1和新功能F2组成。新功能F由类C的子类C1来完成，则子类C1在完成功能F的同时，有可能会导致类C的原功能F1发生故障。这时就有人提出了里氏替换原则。里氏替换原则这项原则最早是在1988年，由麻省理工学院一位姓里的女士（Liskov）提出来的。
里氏替换原则定义：
        严格的定义：如果对每一个类型为T1的对象o1，都有类型为T2的对象o2，使得以T1定义的所有程序P在所有的对象o1都换成o2时，程序P的行为没有变化，那么类型T2是类型T1的子类型。 
　　通俗的定义：所有引用基类的地方必须能透明地使用其子类的对象。
        更通俗的定义：子类可以扩展父类的功能，但不能改变父类原有的功能。
也就是说，在面向对象设计中，如果用一个基类对象替换一个子类对象，程序不会发生错误，但是软件实体使用用一个子类对象，但是改实体不一定能使用该子类的基类对象，如果使用，程序有可能就会出问题。
 例如有两个类，一个类为CLassA，另一个是CLassB类，并且CLassB类是CLassA类的子类，那么一个方法如果可以接受一个CLassA类型的基类对象base的话，如：MethodA(base)，那么它必然可以接受一个CLassA类型的子类对象sub，MethodA(sub)能够正常运行。反过来的代换不成立，如一个方法MethodB接受CLassA类型的子类对象sub为参数：MethodB(sub)，那么一般而言不可以有MethodB(base)，除非是重载方法。
例子：沪深A股板块数据请求的里氏替换简析：
                                              
                

        在本实例中，可以考虑增加一个新的抽象类StockBlock，而将ShangA和ShenA类作为其子类，数据请求类Quote类针对抽象客户类StockBlock编程，根据里氏代换原则，能够接受基类对象的地方必然能够接受子类对象，因此将Quote中的Request()方法的参数类型改为Code，如果需要增加新类型的客户，只需将其作为String类的子类即可。重构后的结构如图2所示：
                                         

 里氏替换使用总结：
          里氏替换包括以下四层含义：

(1).子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法。
(2).子类中可以增加自己特有的方法。
(3).当子类的方法重载父类的方法时，方法的前置条件（即方法的形参）要比父类方法的输入参数更宽松。
(4).当子类的方法实现父类的抽象方法时，方法的后置条件（即方法的返回值）要比父类更严格。



单一职责原则来源：

        定义：单一职责就是一个类负责一项职责.就一个类而言，应该只专注于做一件事和仅有一个引起它变化的原因。

        所谓职责，我们可以理解为功能，就是设计的这个类功能应该只有一个，而不是两个或更多。也可以理解为引用变化的原因，当你发现有两个变化会要求我们修改这个类，那么你就要考虑撤分这个类了。因为职责是变化的一个轴线，当需求变化时，该变化会反映类的职责的变化。

       单一职责原则，很太简单。大多数开发人员，在设计软件时也会自觉的遵守这一重要原则，因为这是常识。在软件编程中，谁也不希望因为修改了一个功能导致其他的功能发生故障。而避免出现这一问题的方法便是遵循单一职责原则。虽然单一职责原则如此简单，并且被认为是常识，但是即便是经验丰富的程序员写出的程序，也会有违背这一原则的代码存在。为什么会出现这种现象呢？因为有职责扩散。所谓职责扩散，就是因为某种原因，职责A被分化为粒度更细的职责A1和A2。有时候，需求变更需要我们对一个类做出更细粒度的划分，这时候就可能破坏之前构思的单一职责的类！

单一职责原则使用总结：

     优点：

(1).消除耦合，减小因需求变化引起代码僵化。

(2). 提高类的可读性，提高系统的可维护性；

(3). 变更引起的风险降低，变更是必然的，如果单一职责原则遵守的好，当修改一个功能时，可以显著降低对其他功能的影响。


优秀程序设计的18大原则

1、避免重复原则(DRY - Don’t repeat yourself)

编程的最基本原则是避免重复。在程序代码中总会有很多结构体，如循环、函数、类等等。一旦你重复某个语句或概念，就很容易形成一个抽象体。

2、抽象原则(Abstraction Principle)

与DRY原则相关。要记住，程序代码中每一个重要的功能，只能出现在源代码的一个位置。

3、简单原则(Keep It Simple and Stupid)

简单是软件设计的目标，简单的代码占用时间少，漏洞少，并且易于修改。

4、避免创建你不要的代码(Avoid Creating a YAGNI (You aren’t going to need it))

除非你需要它，否则别创建新功能。

5、尽可能做可运行的最简单的事(Do the simplest thing that could possibly work)

尽可能做可运行的最简单的事。在编程中，一定要保持简单原则。作为一名程序员不断的反思“如何在工作中做到简化呢?”这将有助于在设计中保持简单的路径。

6、别让我思考(Don’t make me think)

这是Steve Krug一本书的标题，同时也和编程有关。所编写的代码一定要易于读易于理解，这样别人才会欣赏，也能够给你提出合理化的建议。相反，若是繁杂难解的程序，其他人总是会避而远之的。

7、开闭原则(Open/Closed Principle)

你所编写的软件实体(类、模块、函数等)最好是开源的，这样别人可以拓展开发。不过，对于你的代码，得限定别人不得修改。换句话说，别人可以基于你的代码进行拓展编写，但却不能修改你的代码。

8、代码维护(Write Code for the Maintainer)

一个优秀的代码，应当使本人或是他人在将来都能够对它继续编写或维护。代码维护时，或许本人会比较容易，但对他人却比较麻烦。因此你写的代码要尽可能保证他人能够容易维护。用书中原话说“如果一个维护者不再继续维护你的代码，很可能他就有想杀了你的冲动。”

9、最小惊讶原则(Principle of least astonishment)

最小惊讶原则通常是在用户界面方面引用，但同样适用于编写的代码。代码应该尽可能减少让读者惊喜。也就是说，你编写的代码只需按照项目的要求来编写。其他华丽的功能就不必了，以免弄巧成拙。

10、单一责任原则(Single Responsibility Principle)

某个代码的功能，应该保证只有单一的明确的执行任务。

11、低耦合原则(Minimize Coupling)

代码的任何一个部分应该减少对其他区域代码的依赖关系。尽量不要使用共享参数。低耦合往往是完美结构系统和优秀设计的标志。

12、最大限度凝聚原则(Maximize Cohesion)

相似的功能代码应尽量放在一个部分。

13、隐藏实现细节(Hide Implementation Details)

隐藏实现细节原则，当其他功能部分发生变化时，能够尽可能降低对其他组件的影响。

14、迪米特法则又叫作最少知识原则(Law of Demeter)

该代码只和与其有直接关系的部分连接。(比如：该部分继承的类，包含的对象，参数传递的对象等)。

15、避免过早优化(Avoid Premature Optimization)

除非你的代码运行的比你想像中的要慢，否则别去优化。假如你真的想优化，就必须先想好如何用数据证明，它的速度变快了。

“过早的优化是一切罪恶的根源”——Donald Knuth

16、代码重用原则(Code Reuse is Good)

重用代码能提高代码的可读性，缩短开发时间。

17、关注点分离(Separation of Concerns)

不同领域的功能，应该由不同的代码和最小重迭的模块组成。

18、拥抱改变(Embrace Change)

这是Kent Beck一本书的标题，同时也被认为是极限编程和敏捷方法的宗旨。

Interpreter 模式的来源：
     Interpreter(解释器)模式是一种特殊的设计模式，它建立一个解释器（Interpreter），对于特定的计算机程序设计语言，用来解释预先定义的文法。简单地说，Interpreter模式是一种简单的语法解释器构架。解释器模式属于行为模式，给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子。
Interpreter 模式作用：
    正如其名，此模式大多用来解释一些(自定义的)独特语法，例如某些游戏开发引擎中读取XML文件，或是WindowsPhone开发中的XAML文件，都是使用此模式来进行的。与其说是一种模式，不如说是一种具有通用规范的行为更为准确。

Interpreter 模式UML结构图如图1所示：
                                



Interpreter 模式构成：
(1).抽象表达式角色(AbstractExpression)： 声明一个抽象的解释操作，这个接口为所有具体表达式角色都要实现的。
(2).终结符表达式角色(TerminalExpression)： 实现与文法中的元素相关联的解释操作，通常一个解释器模式中只有一个终结符表达式，但有多个实例对应不同的终结符，
(3).终结符就是语言中用到的基本元素：一般不能再被分解，如: x -&gt; xa， 这里a是终结符，因为没有别的规则可以把a变成别的符号，不过x可以变成别的符号，所以x是非终结符。
(4).非终结符表达式角色(NonterminalExpression)： 文法中的每条规则对应于一个非终结表达式， 非终结表达式根据逻辑的复杂程度而增加，原则上每个文法规则都对应一个非终结符表达式。
(5).环境角色(Context)：包含解释器之外的一些全局信息。
Interpreter 模式代码示例：

Interpreter.h

#ifndef _INTERPRETER_H_
#define _INTERPRETER_H_
#include&lt;string&gt;

typedef std::string Context ;

class AbstractExpression
{
public:
	AbstractExpression(){}
	~AbstractExpression(){}

	virtual void Interpreter(const Context&amp; context)=0;
};

class TerminalExpression : public AbstractExpression
{
public:
	TerminalExpression(char kc);
	~TerminalExpression(){}

	virtual void Interpreter(const Context&amp; context);

private:
	char keyChar;
};

class NonterminalExpression : public AbstractExpression
{
public:
	NonterminalExpression(AbstractExpression* ae);
	~NonterminalExpression(){}

	virtual void Interpreter(const Context&amp; context);

private:
	AbstractExpression* terminalExpression;
};

#endif

Interpreter.cpp

#include "Interpreter.h"
#include&lt;iostream&gt;

TerminalExpression::TerminalExpression(char kc)
{
	keyChar = kc;
}

void TerminalExpression::Interpreter(const Context&amp; context)
{
	int length = 0;
	while (context[length] != '\0')
	{
		length++;
	}

	int numberArray[100];
	int index = 0;

	for(int i = 0;i &lt; length;i++)
	{
		if(keyChar  == context[i])
			numberArray[index++] = i;
	}

	for(int i = 0;i &lt; index;i++)
	{
		std::cout&lt;&lt;numberArray[i];
	}

	std::cout&lt;&lt;std::endl;
}

NonterminalExpression::NonterminalExpression(AbstractExpression* ae)
{
	terminalExpression = ae;
}

void NonterminalExpression::Interpreter(const Context&amp; context)
{
	std::cout&lt;&lt;"The password is: ";
	terminalExpression-&gt;Interpreter(context);
}

Main.cpp

#include "Interpreter.h"
#include&lt;iostream&gt;

using namespace std;

int main()
{
	AbstractExpression* terminal = new TerminalExpression('c');
	AbstractExpression* nonTerminal = new NonterminalExpression(terminal);

	//First test.
	string contextFirst = "character of the cars in this chart.";
	cout&lt;&lt;"Context: "&lt;&lt;contextFirst&lt;&lt;endl;
	nonTerminal-&gt;Interpreter(contextFirst);

	cout&lt;&lt;endl;

	//Second test.
	terminal = new TerminalExpression('o');
	nonTerminal = new NonterminalExpression(terminal);

	string contextSecond = "Thanks everyone for your long time supporting.";
	cout&lt;&lt;"Context: "&lt;&lt;contextSecond&lt;&lt;endl;
	nonTerminal-&gt;Interpreter(contextSecond);

	return 0;
}
Interpreter 模式适用性：


(1).可以将一个需要解释执行的语言中的句子表示为一个抽象语法树。
(2). 一些重复出现的问题可以用一种简单的语言来进行表达。
(3).一个语言的文法较为简单。
(4).执行效率不是关键问题。（注：高效的解释器通常不是通过直接解释抽象语法树来实现的，而是需要将它们转换成其他形式，使用解释器模式的执行效率并不高。）

Interpreter 模式优缺点总结：
Interpreter 模式优点：
(1).易于改变和扩展文法。由于在解释器模式中使用类来表示语言的文法规则，因此可以通过继承等机制来改变或扩展文法。
(2).每一条文法规则都可以表示为一个类，因此可以方便地实现一个简单的语言。
(3).实现文法较为容易。在抽象语法树中每一个表达式节点类的实现方式都是相似的，这些类的代码编写都不会特别复杂，还可以通过一些工具自动生成节点类代码。
(4).增加新的解释表达式较为方便。如果用户需要增加新的解释表达式只需要对应增加一个新的终结符表达式或非终结符表达式类，原有表达式类代码无须修改，符合“开闭原则”。
Interpreter 模式缺点：


(1).对于复杂文法难以维护。在解释器模式中，每一条规则至少需要定义一个类，因此如果一个语言包含太多文法规则，类的个数将会急剧增加，导致系统难以管理和维护，此时可以考虑使用语法分析程序等方式来取代解释器模式。
(2).执行效率较低。由于在解释器模式中使用了大量的循环和递归调用，因此在解释较为复杂的句子时其速度很慢，而且代码的调试过程也比较麻烦。


Interpreter 模式使用总结：
        尽量不要在重要模块中使用解释器模式，因为维护困难。在项目中，可以使用脚本语言来代替解释器模式。




Iterator模式来源：
        迭代器（Iterator）模式，又叫做游标（Cursor）模式。GOF给出的定义为：提供一种方法访问一个容器（container）对象中各个元素，而又不需暴露该对象的内部细节。从定义可见，迭代器模式是为容器而生。
Iterator模式作用：
(1).它支持以不同的方式遍历一个聚合复杂的聚合可用多种方式进行遍历，如二叉树的遍历，可以采用前序、中序或后序遍历。迭代器模式使得改变遍历算法变得很容易: 仅需用一个不同的迭代器的实例代替原先的实例即可，你也可以自己定义迭代器的子类以支持新的遍历，或者可以在遍历中增加一些逻辑，如有条件的遍历等。
(2).迭代器简化了聚合的接口有了迭代器的遍历接口，聚合本身就不再需要类似的遍历接口了，这样就简化了聚合的接口。
(3).在同一个聚合上可以有多个遍历每个迭代器保持它自己的遍历状态，因此你可以同时进行多个遍历。
(4).此外，Iterator模式可以为遍历不同的聚合结构（需拥有相同的基类）提供一个统一的接口，即支持多态迭代。
简单说来，迭代器模式也是Delegate原则的一个应用，它将对集合进行遍历的功能封装成独立的Iterator，不但简化了集合的接口，也使得修改、增加遍历方式变得简单。从这一点讲，该模式与Bridge模式、Strategy模式有一定的相似性，但Iterator模式所讨论的问题与集合密切相关，造成在Iterator在实现上具有一定的特殊性
Iterator模式UML结构图如图1所示：
                         
Iterator模式构成：
(1).迭代器类（lterator）：迭代器类负责定义访问和遍历元素的接口。
(2).具体迭代器类（ConcreteIterator）：具体迭代器类要实现迭代器接口，并要记录遍历中的当前位置。
(3).容器类（Aggregate）：容器类负责提供创建具体迭代器类的接口。
(4).具体容器类（Concretelterator）：具体容器类实现创建具体迭代器类的接口——这个具体迭代器类与该容器的结构相关。
Iterator模式使用场景：
(1).访问一个容器对象的内容而无需暴露它的内部表示。
(2).支持对容器对象的多种遍历。
(3).为遍历不同的容器结构提供一个统一的接口（多态迭代）。
Iterator模式代码示例：
Iterator.h

#include &lt;iostream&gt;
#include &lt;string&gt;
#include &lt;vector&gt;

using namespace std;

class Iterator
{
public:
    Iterator(){};
    virtual ~Iterator(){};
    virtual string First() = 0;
    virtual string Next() = 0;
    virtual string GetCur() = 0;
    virtual bool IsEnd() = 0;
};

class Aggregate
{
public:
    virtual int Count() = 0;
    virtual void Push(const string&amp; strValue)=0;
    virtual string Pop(const int nIndex)=0;
    virtual Iterator* CreateIterator() = 0;
};

class ConcreteIterator : public Iterator
{
public:
    ConcreteIterator(Aggregate* pAggregate):m_nCurrent(0),Iterator()
    {
        m_Aggregate = pAggregate;
    }
    string First()
    {
        return m_Aggregate-&gt;Pop(0);
    }
    string Next()
    {
        string strRet;
        m_nCurrent++;
        if(m_nCurrent &lt; m_Aggregate-&gt;Count())
        {
            strRet = m_Aggregate-&gt;Pop(m_nCurrent);
        }
        return strRet;
    }
    string GetCur()
    {
        return m_Aggregate-&gt;Pop(m_nCurrent);
    }
    bool IsEnd()
    {
        return ((m_nCurrent &gt;= m_Aggregate-&gt;Count()) ? true: false);
    }
private:
    Aggregate* m_Aggregate;
    int m_nCurrent;
};

class ConcreteAggregate : public Aggregate
{
public:
    ConcreteAggregate():m_pIterator(NULL)
    {
        m_vecItems.clear();
    }
    ~ConcreteAggregate()
    {
        if(NULL != m_pIterator)
        {
            delete m_pIterator;
            m_pIterator = NULL;
        }
    }
    Iterator* CreateIterator()
    {
        if(NULL == m_pIterator)
        {
            m_pIterator = new ConcreteIterator(this);
        }
        return m_pIterator;
    }
    int Count()
    {
        return m_vecItems.size();
    }
    void Push(const string&amp; strValue)
    {
        m_vecItems.push_back(strValue);
    }
    string Pop(const int nIndex)
    {
        string strRet;
        if(nIndex &lt; Count())
        {
            strRet = m_vecItems[nIndex];
        }
        return strRet;
    }
private:
    vector&lt;string&gt; m_vecItems;
    Iterator* m_pIterator;


};Main.cpp
#include "iterator.h"

int main()
{
    ConcreteAggregate* pName = NULL;
    pName = new ConcreteAggregate();
    if(NULL != pName)
    {
        pName-&gt;Push("hello");
        pName-&gt;Push("word");
        pName-&gt;Push("cxue");
    }
    Iterator* iter = NULL;
    iter = pName-&gt;CreateIterator();
    if(NULL != iter)
    {
        string strItem = iter-&gt;First();
        while(!iter-&gt;IsEnd())
        {
            cout &lt;&lt; iter-&gt;GetCur() &lt;&lt; " is ok" &lt;&lt; endl;
            iter-&gt;Next();
        }
    }
    system("pause");

    return 0;
}

 Iterator模式优缺点总结：
Iterator模式优点：
(1).迭代器模式它将对集合进行遍历的功能封装成独立的Iterator，不但简化了集合的接口，也使得修改、增加遍历方式变得简单。
(2).避免了容器的细节暴露给外部访问者。使得设计符合“单一职责原则”。
(3).在迭代器模式中，具体迭代器角色和具体容器角色是耦合在一起的——遍历算法是与容器的内部细节紧密相关的。为了使客户程序从与具体迭代器角色耦合的困境中脱离出来，避免具体迭代器角色的更换给客户程序带来的修改，迭代器模式抽象了具体迭代器角色，使得客户程序更具一般性和重用性。这被称为多态迭代。
Iterator模式缺点：
(1).在一般的底层集合支持类中，我们往往不愿“避轻就重”将集合设计成集合
 + Iterator 的形式，而是将遍历的功能直接交由集合完成，以免犯了“过度设计”的诟病，但是，如果我们的集合类确实需要支持多种遍历方式（仅此一点仍不一定需要考虑
 Iterator模式，直接交由集合完成往往更方便），或者，为了与系统提供或使用的其它机制，如STL算法，保持一致时，Iterator模式才值得考虑。
Iterator模式使用总结：
       如果系统涉及的容器较多，可以考虑使用迭代器模式，简化接口，如果系统较简单，直接用模板比较好。iterator模式的目的是提供一种方法访问一个容器对象中各个元素，而又不需暴露该对象的内部细节。也就是说提供一套可以访问不同容器的遍历方法而避免容器内部细节的暴露，也符合设计的“单一职责原则！

Chain of Responsibility模式来源：
        熟悉VC/MFC的都知道，VC是“基于消息，事件驱动”，消息在VC开发中起着举足轻重的作用。在MFC中，消息是通过一个向上递交的方式进行处理，例如个WM_COMMAND消息的处理流程:
  注明：有关MFC消息处理更加详细信息，请参考候捷先生的《深入浅出MFC》。
        MFC提供了消息的处理的链式处理策略，处理消息的请求将沿着预先定义好的路径依次进行处理。消息的发送者并不知道该消息最后是由那个具体对象处理的，当然它也无须也不想知道，但是结构是该消息被某个对象处理了，或者一直到一个终极的对象进行处理了。Chain of Responsibility模式描述其实就是这样一类问题将可能处理一个请求的对象链接成一个链，并将请求在这个链上传递，直到有对象处理该请求（可能需要提供一个默认处理所有请求的类，例如MFC中的CwinApp类。
       实际上现实中也有很多这样的例子，比如员工请假审核，每个公司都有自己的请假制度。我们不妨假设：请假半天只要和部门主管说一声就行了，请假在1天要通过人事部门，而请假超过2天就不那么好申请了，这时可能要总经理或者更高级别的人同意才行了。如果不考虑设计模式直接写代码，要完成这个逻辑就可能用到if—else或者多个if了，看到这个代码，就知道了存在许多的问题，比如现在要扩展怎么办呢？比如又有了董事长，可以处理一个月的请假，这时就要修改这么Manager类中的deal_holiday方法了，显然不符合单一职责原则（SRP）和开放-封闭原则（OCP），这种类自然也是不合理的类了。现在可以说一说职责链模式了，实际上这种情形是适合职责链模式的：现在有一个请假信息需要处理，首先是主管处理；主管处理不了，交给人事部门处理；人事部门处理不了，交给总经理处理（如果总经理是最后可能处理请假信息的管理者，那么总经理一定会给出一个答复）。这就构成了一个职责链。我们就可以选择Chain
 of Responsibility模式来实现。如果我们把每个管理者都写一个类，每个管理者都负责处理请假信息，但是只有当主管处理不了时，才交给人事部门处理，以此类推。那么就是可以写出来一个职责链模式的代码。

Chain of Responsibility模式作用：
         使多个对象都有机会处理请求，从而避免请求的发送者和接受者之间的耦合关系。将这个对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理他为止。使系统的扩展时更方便，只需要在增加管理者类，并且这个"职责链”是在客户端动态确定的。


Chain of Responsibility模式UML结构图：
                     

Chain of Responsibility模式结构：
        抽象处理者角色(Handler)：定义出一个处理请求的接口。如果需要，接口可以定义出一个方法，以设定和返回对后继的引用。这个角色通常由一个抽象类或接口实现。
         具体处理者角色(ConcreteHandler)：具体处理者接到请求后，可以选择将请求处理掉，或者将请求传给后继，没有的话就自己处理。由于具体处理者持有对后继的引用，因此，如果需要，具体处理者可以访问后继。
Chain of Responsibility模式代码示例：


ChainMode.h

#include &lt;iostream&gt;
#include &lt;string&gt;

using namespace std;

//管理者类
class Manager
{
private:
	string name;
	Manager *nextManager;
public:
	Manager(string n, Manager *m = NULL) : name(n), nextManager(m){};
	void set_name(string name);
	string get_name();
	
	//设置下一个要处理请假信息的管理者
	void set_next_manager(Manager *m);
	
	Manager* get_next_manager();

	//处理请假
	virtual void deal_holiday(int day);
};
//主管类
class ZhuGuan : public Manager
{
public:
	ZhuGuan(string n, Manager *m = NULL) :Manager(n, m){};
	virtual void deal_holiday(int day);
};
//人事部门类
class RenShi : public Manager
{
public:
	RenShi(string n, Manager *m = NULL) :Manager(n, m){};
	virtual void deal_holiday(int day);
};
class ZongJingLi : public Manager
{
public:
	ZongJingLi(string n, Manager *m = NULL) :Manager(n, m){};
	virtual void deal_holiday(int day);
};


ChainMode.cpp

#include &lt;iostream&gt;
#include &lt;string&gt;
#include "ChainMode.h"
using namespace std;


void ZhuGuan::deal_holiday(int day)
{
	if (day &lt;= 0.5)
	{
		cout &lt;&lt; "主管：" &lt;&lt; get_name() &lt;&lt; "同意请假!!" &lt;&lt; endl;
	}
	else if (get_next_manager() != NULL)
	{
		get_next_manager()-&gt;deal_holiday(day);
	}
}

void RenShi::deal_holiday(int day)
{
	if (day &lt; 2)
	{
		cout &lt;&lt; "人事部：" &lt;&lt; get_name() &lt;&lt; "同意请假!!" &lt;&lt; endl;
	}
	else if (get_next_manager() != NULL)
	{
		get_next_manager()-&gt;deal_holiday(day);
	}
}
void ZongJingLi::deal_holiday(int day)
{
	if (day &lt; 30)
	{
		cout &lt;&lt; "总经理：" &lt;&lt; get_name() &lt;&lt; "同意请假!!" &lt;&lt; endl;
	}
	//总经理室最后一个进行处理的，所以一定要给出一个答复
	else
	{
		cout &lt;&lt; "总经理：" &lt;&lt; get_name() &lt;&lt; "关于你说的请假的事情，由于时间太长，以后再说吧!!" &lt;&lt; endl;
	}
}


Main.cpp
#include &lt;iostream&gt;
#include &lt;string&gt;
#include "ChainMode.h"

int main()
{
	//主管Mike
	ZhuGuan Z_G("Mike");
	//人事部Tom
	RenShi R_S("Tom");
	//总经理Vincent
	ZongJingLi Z_J_L("Vincent");

	Z_G.set_next_manager(&amp;R_S);
	R_S.set_next_manager(&amp;Z_J_L);

	Z_G.deal_holiday(0.5);
	Z_G.deal_holiday(1);
	Z_G.deal_holiday(3);
	Z_G.deal_holiday(31);
	return 0;
}
Chain of Responsibility模式的优缺点：
          Chain of Responsibility模式的优点：

(1).增强了系统的可扩展性。
(2).使用职责链模式可以避免众多的if或者if-else语句。
(3).使用职责链模式可以减小对象之间的耦合性。使得对象之间的联系减小。
(4).可以根据需要自由组合工作流程。如工作流程发生变化，可以通过重新分配对象链便可适应新的工作流程。
(5).责任的分担。每个类只需要处理自己该处理的工作（不该处理的传递给下一个对象完成），明确各类的责任范围，符合类的最小封装原则。
          Chain of Responsibility模式的缺点：
(1).使用职责链模式的时候会有多个处理者对象，但是实际使用的处理者对象却只有一个，这在某种程度讲是资源的浪费。
(2).同时职责链的建立的合理性要靠客户端来保证，增加了程序的复杂性，也有可能由于职责链导致出错。
Chain of Responsibility模式的使用总结：
          Chain of Responsibility模式的最大的一个优点就是给系统降低了耦合性，请求的发送者完全不必知道该请求会被哪个应答对象处理，极大地降低了系统的耦合性。

Visitor模式来源：

       在面向对象系统的开发和设计过程，经常会遇到一种情况就是需求变更（RequirementChanging），经常我们做好的一个设计、实现了一个系统原型，咱们的客户又会有了新的需求。我们又因此不得不去修改已有的设计，最常见就是解决方案就是给已经设计、实现好的类添加新的方法去实现客户新的需求，这样就陷入了设计变更的梦魇：不停地打补丁，其带来的后果就是设计根本就不可能封闭、编译永远都是整个系统代码。Visitor模式则提供了一种解决方案。
Visitor模式作用：
 
     将更新（变更）封装到一个类中（访问操作），并由待更改类提供一个接收接口，则可达到效果。

Visitor模式UML结构图如图1所示：

 
                            

Visitor模式构成：


抽象访问者(Visitor)：抽象出访问元素的动作
具体访问者(ConcreteVisitor)：实现访问元素的动作
抽象元素(Element)：定义一个接受访问的操作，其参数为访问者
具体元素(ConcreteElement)：实现接受访问操作
对象结构类(ObjectStructure)：可以枚举元素，并且管理元素
客户端(Client) ：定义元素集合，然后接受不同访问者的访问
Visitor模式源码示例：

Visitor.h

#pragma once
class CCommonEmployee;
class CManager;
class IVisitor
{
public:

    IVisitor(void)
    {
    }

    virtual ~IVisitor(void)
    {
    }
    virtual void Visit(CCommonEmployee commonEmployee) = 0;
    virtual void Visit(CManager manager) = 0;
    virtual int GetTotalSalary() = 0;
};

//BaseVisitor.h
#pragma once
#include "ivisitor.h"
#include "CommonEmployee.h"
#include "Manager.h"
#include &lt;iostream&gt;
using std::string;
class CBaseVisitor :
    public IVisitor
{
public:
    CBaseVisitor(void);
    ~CBaseVisitor(void);
    void Visit(CCommonEmployee commonEmployee);
    void Visit(CManager manager);
    int GetTotalSalary();
private:
    string GetBasicInfo(CEmployee *pemployee);
    string GetManagerInfo(CManager manager);
    string GetCommonEmployee(CCommonEmployee employee);
    static const int MANAGER_COEFFICENT = 5;
    static const int COMMONEMPLOYEE_COEFFICENT = 2;
    int m_commonTotal;
    int m_managerTotal;
    void CalCommonSalary(int salary);
    void CalManagerSalary(int salary);
};

//BaseVisitor.cpp
#include "StdAfx.h"
#include "..\CommonDeclare\Convert.h"
#include "BaseVisitor.h"
#include &lt;iostream&gt;
using std::string;
using std::cout;
using std::endl;

CBaseVisitor::CBaseVisitor(void)
{
    m_commonTotal = 0;
    m_managerTotal = 0;
}

CBaseVisitor::~CBaseVisitor(void)
{
}

void CBaseVisitor::Visit(CCommonEmployee commonEmployee)
{
    cout &lt;&lt; this-&gt;GetCommonEmployee(commonEmployee).c_str() &lt;&lt; endl;
    this-&gt;CalCommonSalary(commonEmployee.GetSalary());
}

void CBaseVisitor::Visit(CManager manager)
{
    cout &lt;&lt; this-&gt;GetManagerInfo(manager).c_str() &lt;&lt; endl;
    this-&gt;CalManagerSalary(manager.GetSalary());
}

string CBaseVisitor::GetBasicInfo(CEmployee *pemployee)
{
    string info = "";
    info.append("姓名：");
    info.append(pemployee-&gt;GetName());
    info.append("\t");
    info.append("性别：");
    info.append(CConvert::ToString(pemployee-&gt;GetSex()));
    info.append("\t");
    info.append("薪水：");
    info.append(CConvert::ToString(pemployee-&gt;GetSalary()));
    info.append("\t");
    return info;
}

string CBaseVisitor::GetManagerInfo(CManager manager)
{
    string basicInfo = this-&gt;GetBasicInfo(&amp;manager);
    string otherInfo = "";
    otherInfo.append("业绩：");
    otherInfo.append(manager.GetPerformance());
    otherInfo.append("\t");
    basicInfo.append(otherInfo);
    return basicInfo;
}

string CBaseVisitor::GetCommonEmployee(CCommonEmployee employee)
{
    string basicInfo = this-&gt;GetBasicInfo(&amp;employee);
    string otherInfo = "";
    otherInfo.append("工作：");
    otherInfo.append(employee.GetJob());
    otherInfo.append("\t");
    basicInfo.append(otherInfo);
    return basicInfo;
}

int CBaseVisitor::GetTotalSalary()
{
    return this-&gt;m_commonTotal + this-&gt;m_managerTotal;
}

void CBaseVisitor::CalCommonSalary(int salary)
{
    this-&gt;m_commonTotal += salary;
}

void CBaseVisitor::CalManagerSalary(int salary)
{
    this-&gt;m_managerTotal += salary;
}


//Employee.h
#pragma once
#include "Visitor.h"
#include &lt;iostream&gt;
using std::string;
class CEmployee
{
public:
static int MALE;
static int FEMALE;
CEmployee(void);
virtual ~CEmployee(void);
string GetName();
void SetName(string name);
int GetSalary();
void SetSalary(int v);
int GetSex();
void SetSex(int v);
virtual void Accept(IVisitor *pVisitor) = 0;
private:
string m_name;
int m_salary;
int m_sex;
};

//Employee.cpp
#include "StdAfx.h"
#include "Employee.h"
int CEmployee::MALE = 0;
int CEmployee::FEMALE = 1;

CEmployee::CEmployee(void)
{
}

CEmployee::~CEmployee(void)
{
}

string CEmployee::GetName()
{
return m_name;
}

void CEmployee::SetName( string name )
{
m_name = name;
}

int CEmployee::GetSalary()
{
return m_salary;
}

void CEmployee::SetSalary( int v )
{
m_salary = v;
}

int CEmployee::GetSex()
{
return m_sex;
}

void CEmployee::SetSex( int v )
{
m_sex = v;
}

//Manager.h
#pragma once
#include "employee.h"
#include "IVisitor.h"
#include &lt;iostream&gt;
using std::string;
class CManager :
public CEmployee
{
public:
CManager(void);
~CManager(void);
string GetPerformance();
void SetPerformance(string per);
void Accept(IVisitor *pVisitor);
protected:
string GetOtherInfo();
private:
string m_performance;
};

//Manager.cpp
#include "StdAfx.h"
#include "Manager.h"
#include &lt;iostream&gt;
using std::string;

CManager::CManager(void)
{
this-&gt;m_performance = "";
}

CManager::~CManager(void)
{
}

string CManager::GetPerformance()
{
return m_performance;
}

void CManager::SetPerformance(string per)
{
this-&gt;m_performance = per;
}

string CManager::GetOtherInfo()
{
string info = "";
info.append("业绩：");
info.append(this-&gt;m_performance);
info.append("\t");
return info;
}

void CManager::Accept(IVisitor *pVisitor)
{
pVisitor-&gt;Visit(*this);
}

// Visitor.cpp : 定义控制台应用程序的入口点。
//
#include "stdafx.h"
#include "Employee.h"
#include "CommonEmployee.h"
#include "Manager.h"
#include "BaseVisitor.h"
#include "..\CommonDeclare\Convert.h"
#include &lt;vector&gt;
#include &lt;iostream&gt;
using std::vector;
using std::cout;
using std::endl;

void MockEmployee(vector&lt;CEmployee*&gt; *pvce)
{
CCommonEmployee *pZhangSan = new CCommonEmployee();
pZhangSan-&gt;SetJob("编写Java程序，绝对的蓝领、苦工加搬运工");
pZhangSan-&gt;SetName("张三");
pZhangSan-&gt;SetSalary(1800);
pZhangSan-&gt;SetSex(CEmployee::MALE);
pvce-&gt;push_back(pZhangSan);

CCommonEmployee *pLiSi = new CCommonEmployee();
pLiSi-&gt;SetJob("页面美工，审美素质太不流行了！");
pLiSi-&gt;SetName("李四");
pLiSi-&gt;SetSalary(1900);
pLiSi-&gt;SetSex(CEmployee::FEMALE);
pvce-&gt;push_back(pLiSi);

CManager *pWangWu = new CManager();
pWangWu-&gt;SetPerformance("基本上是负值，但是我会拍马屁呀");
pWangWu-&gt;SetName("王五");
pWangWu-&gt;SetSalary(1900);
pWangWu-&gt;SetSex(CEmployee::FEMALE);
pvce-&gt;push_back(pWangWu);
}

void DoIt()
{
vector&lt;CEmployee*&gt; vce;
MockEmployee(&amp;vce);
vector&lt;CEmployee*&gt;::const_iterator readIt = vce.begin();

CBaseVisitor visitor;
for (; readIt != vce.end(); readIt ++)
{
(*readIt)-&gt;Accept(&amp;visitor);
}
cout &lt;&lt; "本公司的月工资总额是：" &lt;&lt; CConvert::ToString(visitor.GetTotalSalary()).c_str() &lt;&lt; endl;

vector&lt;CEmployee*&gt;::reverse_iterator delIt = vce.rbegin();
for (; delIt != vce.rend(); delIt++)
{
delete (*delIt);
(*delIt) = NULL;
}
vce.clear();
}



Main.cpp
main(int argc, _TCHAR* argv[])
{
DoIt();
return 0;
}Visitor模式优缺点总结：
Visitor模式优点：
(1).实现简单
Visitor模式缺点：

(1).破坏了封装性。Visitor模式要求Visitor可以从外部修改Element对象的状态，这一般通过两个方式来实现：a）Element提供足够的public接口，使得Visitor可以通过调用这些接口达到修改Element状态的目的；b）Element暴露更多的细节给Visitor，或者让Element提供public的实现给Visitor（当然也给了系统中其他的对象），或者将Visitor声明为Element的friend类，仅将细节暴露给Visitor。但是无论那种情况，特别是后者都将是破坏了封装性原则（实际上就是C++的friend机制得到了很多的面向对象专家的诟病）。
(2).ConcreteElement的扩展很困难：每增加一个Element的子类，就要修改Visitor的接口，使得可以提供给这个新增加的子类的访问机制。从上面我们可以看到，或者增加一个用于处理新增类的Visit（）接口，或者重载一个处理新增类的Visit（）操作，或者要修改RTTI方式实现的Visit（）实现。无论那种方式都给扩展新的Element子类带来了困难。





Command模式来源：
       命令模式又称为行动（Action）模式或事务（Transaction）模式，Command模式通过将请求封装到一个对象（Command）中，并将请求的接受者存放到具体的ConcreteCommand类中（Receiver）中，从而实现调用操作的对象和操作的具体实现者之间的解耦。

Command模式作用：


        将一个请求封装为一个对象，从而使你可用不同的请求对客户进行参数化；对请求排队或记录请求日志，以及支持可撤销的操作。由于“行为请求者”与“行为实现者”的紧耦合，使用命令模式，可以对请求排队或记录请求日志，以及支持可撤销的操作。 
Command模式UML结构图如图1所示：

                     

Command模式构成：

ConcreteCommand将一个接收者对象绑定于一个操作，调用接收者相应的操作，以实现Execute。
客户（Client）类：创建了一个具体命令(ConcreteCommand)对象并确定其接收者。

命令（Command）类：声明了一个给所有具体命令类的抽象接口。这是一个抽象角色。
具体命令（ConcreteCommand）类：定义一个接受者和行为之间的弱耦合；实现Execute()方法，负责调用接收考的相应操作。Execute()方法通常叫做执方法。
请求者（Invoker）类：负责调用命令对象执行请求，相关的方法叫做行动方法。
接收者（Receiver）类：负责具体实施和执行一个请求。任何一个类都可以成为接收者，实施和执行请求的方法叫做行动方法。
Command模式代码示例：


Command.h

#ifndef _COMMAND_H_
#define _COMMAND_H_

class Command
{
public:
    virtual ~Command();
    virtual void Execute()=0;
protected:
    Command();
private:
};

class Receiver;

class ConcreteCommand : public Command
{
public:
    ConcreteCommand(Receiver* pReceiver);
    ~ConcreteCommand();
    virtual void Execute();
protected:
private:
    Receiver* _recv;
};

class Invoker
{
public:
    Invoker(Command* pCommand);
    ~Invoker();
    void Invoke();
protected:
private:
    Command* _cmd;
};

class Receiver
{
public:
    Receiver();
    ~Receiver();
    void Action();
protected:
private:
};
#endif

Command.cpp

#include "Command.h"
#include &lt;iostream&gt;

using namespace std;

Command::Command()
{}

Command::~Command()
{}

ConcreteCommand::ConcreteCommand(Receiver* pReceiver)
{
    this-&gt;_recv = pReceiver;
}

ConcreteCommand::~ConcreteCommand()
{}

void ConcreteCommand::Execute()
{
    this-&gt;_recv-&gt;Action();
}

Receiver::Receiver()
{}

Receiver::~Receiver()
{}

void Receiver::Action()
{
    cout &lt;&lt; "Receiver::Action" &lt;&lt; endl;
}

Invoker::Invoker(Command* pCommand)
{
    this-&gt;_cmd = pCommand;
}

Invoker::~Invoker()
{}

void Invoker::Invoke()
{
    this-&gt;_cmd-&gt;Execute();
}

Main.cpp
#include "Command.h"

int main()
{
    //创建具体命令对象pCmd并设定它的接收者pRev
    Receiver* pRev = new Receiver();
    Command* pCmd = new ConcreteCommand(pRev);
    //请求绑定命令
    Invoker* pInv = new Invoker(pCmd);
    pInv-&gt;Invoke();

    return 0;
}&lt;/span&gt;

Command模式适用性：
在下面的情况下应当考虑使用命令模式：
(1).使用命令模式作为"CallBack"在面向对象系统中的替代。"CallBack"讲的便是先将一个函数登记上，然后在以后调用此函数。
(2).需要在不同的时间指定请求、将请求排队。一个命令对象和原先的请求发出者可以有不同的生命期。换言之，原先的请求发出者可能已经不在了，而命令对象本身仍然是活动的。这时命令的接收者可以是在本地，也可以在网络的另外一个地址。命令对象可以在串形化之后传送到另外一台机器上去。
(3).系统需要支持命令的撤消(undo)。命令对象可以把状态存储起来，等到客户端需要撤销命令所产生的效果时，可以调用undo()方法，把命令所产生的效果撤销掉。命令对象还可以提供redo()方法，以供客户端在需要时，再重新实施命令效果。
(4).如果一个系统要将系统中所有的数据更新到日志里，以便在系统崩溃时，可以根据日志里读回所有的数据更新命令，重新调用Execute()方法一条一条执行这些命令，从而恢复系统在崩溃前所做的数据更新。
Command模式优缺点：


Command模式的优点：
(1).它能较容易地设计一个命令队列；
(2).在需要的情况下，可以较容易地将命令记入日志；
(3).允许接收请求的一方决定是否要否决请求；
(4).可以容易地实现对请求的撤销和重做；
(5).由于加进新的具体命令类不影响其他的类，因此增加新的具体命令类很容易。
Command模式的缺点：


(1).使用命令模式可能会导致某些系统有过多的具体命令类。因为针对每一个命令都需要设计一个具体命令类，因此某些系统可能需要大量具体命令类，这将影响命令模式的使用。
Command模式使用总结：

    Command模式把请求一个操作的对象与知道怎么执行一个操作的对象分割开。Command模式关键就是讲一个请求封装到一个类中(Command），再提供处理对象（Receiver），最后Command命令由Invoker激活。另外，我们可以将请求接收者的处理抽象出来作为参数传给Command对象，实际也就是回调的机制来实现这一点。也就是讲处理操作方法地址通过参数传递给Command对象，Command对象在适当的时候再调用该函数。
    Command模式将调用操作的对象和知道如何实现该操作的对象解耦，在上面Command的结构图中,Invoker对象根本就不知道具体的是哪个对象在处理Execute操作（当然要知道是Command类别的对象）。
    在Command要增加新的处理操作对象很容易，我们可以通过创建新的继承自Command的子类来实现这一点。
    Command模式可以和Memento模式结合起来，支持取消的操作。

    Command模式的本质是对命令进行封装，将发出命令的责任和执行命令的责任分割开。每一个命令都是一个操作：请求的一方发出请求，要求执行一个操作；接收的一方收到请求，并执行操作。
    Command模式允许请求的一方和接收的一方独立开来，使得请求的一方不必知道接收请求的一方的接口，更不必知道请求是怎么被接收，以及操作是否被执行、何时被执行，以及是怎么被执行的。
    Command模式使请求本身成为一个对象，这个对象和其他对象一样可以被存储和传递。
    Command模式的关键在于引入了抽象命令接口，且发送者针对抽象命令接口编程，只有实现了抽象命令接口的具体命令才能与接收者相关联。






Mediator模式来源：
         在面向对象系统的设计和开发过程中，对象之间的交互和通信是最为常见的情况，因为对象间的交互本身就是一种通信。在系统比较小的时候，可能对象间的通信不是很多、对象也比较少，我们可以直接硬编码到各个对象的方法中。但是当系统规模变大，对象的量变引起系统复杂度的急剧增加，对象间的通信也变得越来越复杂，这时候我们就要提供一个专门处理对象间交互和通信的类，这个中介者就是Mediator模式。Mediator模式提供将对象间的交互和通讯封装在一个类中，各个对象间的通信不必显势去声明和引用，大大降低了系统的复杂性能（了解一个对象总比深入熟悉n个对象要好）。另外Mediator模式还带来了系统对象间的松耦合。

Mediator模式作用：  

         Mediator模式是一种很有用并且很常用的模式，它通过将对象间的通信封装到一个类中，将多对多的通信转化为一对多的通信，降低了系统的复杂性。Mediator还获得系统解耦的特性，通过Mediator，各个Colleague就不必维护各自通信的对象和通信协议，降低了系统的耦合性，Mediator和各个Colleague就可以相互独立地修改了。
        Mediator模式还有一个很显著额特点就是将控制集中，集中的优点就是便于管理，也正式符合了OO设计中的每个类的职责要单一和集中的原则。
Mediator模式UML模式结构图：

                                        
Mediator模式的构成：


        Colleage抽象同事类：而ConcreteColleage是具体同事类，每个具体同事只知道自己的行为，而不了解其他同事类的情况，但它们却都认识中介者对象，Mediator是抽象中介者，定义了同事对象到中介者对象的接口，ConcreteMediator是具体中介者对象，实现抽象类的方法，它需要知道所有具体同事类，并从具体同事接受消息，向具体同事对象发出命令。
        Colleage类：抽象同事类。
        Mediator类：抽象中介者类。
Mediator模式的代码示例：

Mediator.h

#ifndef _MEDIATOR_H_
#define _MEDIATOR_H_

#include &lt;string&gt;

using namespace std;

class Mediator;

class Colleage
{
public:
    virtual ~Colleage();
    virtual void SetMediator(Mediator*);
    virtual void SendMsg(string) = 0;
    virtual void GetMsg(string) = 0;
protected:
    Colleage(Mediator*);
    Mediator* _mediator;
private:
    
};

class ConcreteColleageA : public Colleage
{
public:
    ~ConcreteColleageA();
    ConcreteColleageA(Mediator*);
    virtual void SendMsg(string msg);
    virtual void GetMsg(string);
protected:
private:
};

class ConcreteColleageB : public Colleage
{
public:
    ~ConcreteColleageB();
    ConcreteColleageB(Mediator*);
    virtual void SendMsg(string msg);
    virtual void GetMsg(string);
protected:
private:
};

class Mediator
{
public:
    virtual ~Mediator();
    virtual void SendMsg(string,Colleage*) = 0;
protected:
    Mediator();
private:
};

class ConcreteMediator : public Mediator
{
public:
    ConcreteMediator();
    ~ConcreteMediator();
    void SetColleageA(Colleage*);
    void SetColleageB(Colleage*);
    virtual void SendMsg(string msg,Colleage*);
protected:
private:
    Colleage* m_ColleageA;
    Colleage* m_ColleageB;
};
#endif

Mediator.cpp

#include "Mediator.h"
#include &lt;iostream&gt;
#include &lt;string&gt;

using namespace std;

Colleage::Colleage(Mediator* pMediator)
{
    this-&gt;_mediator = pMediator;
}

Colleage::~Colleage()
{}

void Colleage::SetMediator(Mediator* pMediator)
{
    this-&gt;_mediator = pMediator;
}

ConcreteColleageA::ConcreteColleageA(Mediator* pMediator) : Colleage(pMediator)
{
}

ConcreteColleageA::~ConcreteColleageA()
{
}

void ConcreteColleageA::SendMsg(string msg)
{
    this-&gt;_mediator-&gt;SendMsg(msg,this);
}

void ConcreteColleageA::GetMsg(string msg)
{
    cout &lt;&lt; "ConcreteColleageA Receive:"&lt;&lt; msg &lt;&lt; endl;
}

ConcreteColleageB::ConcreteColleageB(Mediator* pMediator) : Colleage(pMediator)
{
}

ConcreteColleageB::~ConcreteColleageB()
{
}

void ConcreteColleageB::SendMsg(string msg)
{
    this-&gt;_mediator-&gt;SendMsg(msg,this);
}

void ConcreteColleageB::GetMsg(string msg)
{
    cout &lt;&lt; "ConcreteColleageB Receive:" &lt;&lt; msg &lt;&lt; endl;
}

Mediator::Mediator()
{}

Mediator::~Mediator()
{}

ConcreteMediator::ConcreteMediator()
{}

ConcreteMediator::~ConcreteMediator()
{}

void ConcreteMediator::SetColleageA(Colleage* p)
{
    this-&gt;m_ColleageA = p;
}

void ConcreteMediator::SetColleageB(Colleage* p)
{
    this-&gt;m_ColleageB = p;
}

void ConcreteMediator::SendMsg(string msg,Colleage* p)
{
    if(p == this-&gt;m_ColleageA)
    {
        this-&gt;m_ColleageB-&gt;GetMsg(msg);
    }
    else if(p == this-&gt;m_ColleageB)
    {
        this-&gt;m_ColleageA-&gt;GetMsg(msg);
    }
}

Main.h

#include "Mediator.h"

int main()
{
    ConcreteMediator* pMediator = new ConcreteMediator();

    Colleage* p1 = new ConcreteColleageA(pMediator);
    Colleage* p2 = new ConcreteColleageB(pMediator);

    pMediator-&gt;SetColleageA(p1);
    pMediator-&gt;SetColleageB(p2);

    p1-&gt;SendMsg("xxx");
    p2-&gt;SendMsg("ooo");
    return 0;
}
Mediator模式的代码说明：


         Mediator模式的实现关键就是将对象Colleague之间的通信封装到一个类种单独处理，为了模拟Mediator模式的功能，这里给每个Colleague对象一个string型别以记录其状态，并通过状态改变来演示对象之间的交互和通信。注意：两个Colleague对象并不知道它交互的对象，并且也不是显示地处理交互过程，这一切都是通过Mediator对象完成的，示例程序运行的结果也正是证明了这一点。
Mediator模式的优缺点总结：


（1）.优点是，各个 Colleague 减少了耦合。
（2）.缺点是，由于 Mediator 控制了集中化，于是就把 Colleague 之间的交互复杂性变为了中介者的复杂性，也就是中介者会变的比任何一个 Colleague 都复杂。
Mediator模式使用总结：
       Mediator模式是一种很有用并且很常用的模式，它通过将对象间的通信封装到一个类中，将多对多的通信转化为一对多的通信，降低了系统的复杂性。Mediator还获得系统解耦的特性，通过Mediator，各个Colleague就不必维护各自通信的对象和通信协议，降低了系统的耦合性，Mediator和各个Colleague就可以相互独立地修改了。
       Mediator模式还有一个很显著额特点就是将控制集中，集中的优点就是便于管理，也正式符合了OO设计中的每个类的职责要单一和集中的原则。

        Mediator中介者模式很容易在系统中应用，也很容易在系统中误用。当系统中出现了“多对多”交互复杂的对象群时，不要急于使用中介者模式，而要先反思你的系统在设计上是不是合理。
      









以上网站架构广泛运用中大型网站中，本文从架构每一层分析所用主流技术和解决手段，有助于初入网站运维朋友们，进一步对网站架构认识，从而自己形成一套架构概念。

第一层：CDN

国内网络分布主要南电信北联通，造成跨地区访问延迟大问题，对于有一定访问量网站来说，增加CDN（内容分发网络）层可有效改善此现象，也是网站加 速的最好选择。CDN把网站页面缓存到全国分布的节点上，用户访问时从最近的机房获取数据，这样大大减少网络访问的路径。如果想自己搭建CDN，不建议这 么做，因为什么呢？其实说白了，就是什么事别往运维上拦。CDN架构部署不复杂，影响效果的因素却很多，后期管理维护也比较复杂，想达到预期的效果确非易 事，这是一个费力不讨好的活，最后老板还是感觉是你能力不足。建议找专做CDN的公司，费用也不贵，有抗流量攻击能力，效果也很好，运维也少很多事，何乐
 而不为呢！

第二层：反向代理（网页缓存）

如果CDN没有缓存要请求的数据则向这层发起请求，在代理服务器配置缓存功能（本地），代理服务器就查找本地缓存是否有CDN请求的数据，如果有就直接返回给CDN，如果没有则请求后端负载均衡器然后转发给WEB服务器返回数据给代理服务器，代理服务器再将结果给CDN。代理服务器一般缓存不经常变动的静态页面，如image、js、css、html等，主流的缓存软件有Squid、Varnish、Nginx。

第三层：负载均衡

访问量较大的网站都会用到负载均衡，因为这是解决单台服务器性能瓶颈的最好办法。反向代理将请求转发给负载均衡器，负载均衡器根据算法（轮训、负载 情况选择后端等）交给后端WEB服务处理，WEB服务处理完成后直接返回数据给反向代理服务器。负载均衡合理分配请求给后端多台WEB服务器，减轻单台服 务器并发负载，并保证服务可用性。主流的负载均衡软件有LVS、HAProxy、Nginx。

第四层：WEB服务

WEB服务是处理用户请求的，WEB服务处理效率，直接影响到访问速度，为避免这层因素造成访问慢，应对其进行调优，让WEB服务发挥到最佳状态。常见的WEB服务有Apache和Nginx。

Apache优化：

1).mod_deflate压缩模块

查看是否加载：

# apachectl M |grep deflate

如果没有安装使用apxs编译进去：

# /usr/local/apache/bin/apxs c I A apache源码目录/modules/mod_deflate.c

deflate配置参数：
DeflateCompressionLevel6      #压缩等级（1-9），数值越大效率越高，消耗CPU也就越高 SetOutputFilterDEFLATE      #启用压缩 AddOutputFilterByTypeDEFLATE text/html text/plain text/xml #压缩类型 AddOutputFilterByTypeDEFLATE css js html htm xml php 

2).mod_expires缓存模块

查看是否加载：

# apachectl M |grep expires

如果没有安装使用apxs编译进去：

# /usr/local/apache/bin/apxs c I A apache源码目录/modules/mod_expires.c

再在httpd.conf启用模块：LoadModule expires_module modules/mod_expires.so

缓存机制有三种用法：全局、目录和虚拟主机

全局配置，在配置文件末尾添加：

ExpiresActiveon       #启用有效期控制，会自动清除已过期的缓存，然后从服务器获取新的
ExpiresDefault "accessplus 1 days"       #默认任意格式的文档都是1天后过期
ExpiresByTypetext/html "access plus 12 months"  
ExpiresByTypeimage/jpg "access plus 12 months"   #jpg格式图片缓存12月

3).工作模式选择及优化

apache有两种常见工作模式，worker和prefork，默认是worker，是混合型的MPM（多路处理模块），支持多进程和多线程，由 线程来处理请求，所以可以处理更多请求，提高并发能力，系统资源开销也小于基于进程的MPM，由于线程使用进程内存空间，进程崩溃会导致其下线程崩溃。而 prefork是非线程型MPM，进程占用系统资源也比worker多，由于进程处理连接，在工作效率上也比worker更稳定。可通过apache2 l查看当前工作模式，在编译时使用—with-mpm参数指定工作模式。根据自己业务需求选择不同工作模式，再适当增加工作模式相关参数，可提高处理能
 力。

配置参数说明：
StartServers      8   #默认启动8个httpd进程 MinSpareServers    5    #最小的空闲进程数 MaxSpareServers    20   #最大的空闲进程数，如果大于这个值，apache会自动kill一些进程 ServerLimit      256   #服务器允许进程数的上限 MaxClients       256  #同时最多发起多少个访问，超过则进入队列等待 MaxRequestsPerChild  4000  #每个进程启动的最大线程 

Nginx优化：

1).gzip压缩模块
http {     ……     gzip on;     gzip_min_length 1k;   #允许压缩的页面最小字节数，默认是0，多大都压缩，小于1k的可能适得其反     gzip_buffers 4 16k;   #gzip申请内存的大小，按数据大小的4倍去申请内存     gzip_http_version 1.0;  #识别http协议版本     gzip_comp_level 2;    #压缩级别，1压缩比最小，处理速度最快，9压缩比最大，处理速度最慢     gzip_types text/plainapplication/x-javascripttext/css application/xml image/jpg;  #压缩数据类型     gzip_vary on;      #根据客户端的http头来判断，是否需要压缩 } 

2).expires缓存模块
server {     location ~ .*.(gif|jpg|png|bmp|swf)$   #缓存数据后缀类型     {       expires 30d;   #使用expires缓存模块，缓存到客户端30天     }     location ~ .*.( jsp|js|css)?$     {       expires 1d;     } } 

3).fastcgi优化

nginx不支持直接调用或者解析动态程序（php），必须通过fastcgi（通用网关接口）来启动php-fpm进程来解析php脚本。也就是 说用户请求先到nginx，nginx再将动态解析交给fastcgi，fastcgi启动php-fpm解析php脚本。所以我们有必要对 fastcgi和php-fpm进行适当的参数优化。


http {     ……     fastcgi_cache_path/usr/local/nginx/fastcgi_cache levels=1:2 keys_zone=TEST:10m inactive=5m;      # FastCGI缓存指定一个文件路径、目录结构等级、关键字区域存储时间和非活动删除时间     fastcgi_connect_timeout 300;    #指定连接到后端FastCGI的超时时间     fastcgi_send_timeout 300;     #指定向FastCGI传送请求的超时时间     fastcgi_read_timeout 300;     #指定接收FastCGI应答的超时时间     fastcgi_buffer_size 64k;      #指定读取FastCGI应答第一部分需要多大的缓冲区     fastcgi_buffers 4 64k;      #指定本地需要用多少盒多大的缓冲区来缓冲FastCGI的应答请求     fastcgi_busy_buffers_size 128k;       fastcgi_temp_file_write_size 128k;  #表示在写入缓存文件时使用多大的数据块，默认值是fastcgi_buffers的两倍     fastcgi_cache TEST;          #开启fastcgi_cache缓存并指定一个TEST名称     fastcgi_cache_valid 200 302 1h;   #指定200、302应答代码的缓存1小时     fastcgi_cache_valid 301 1d;     #将301应答代码缓存1天     fastcgi_cache_valid any 1m;     #将其他应答均缓存1分钟 {  php-fpm.conf配置参数：  pm =dynamic        #两种控制子进程方式（static和dynamic） pm.max_children= 5     #同一时间存活的最大子进程数 pm.start_servers= 2    #启动时创建的进程数 pm.min_spare_servers= 1  #最小php-fpm进程数 pm.max_spare_servers= 3  #最大php-fpm进程数 

4).proxy_cache本地缓存模块
http {         ……    proxy_temp_path  /usr/local/nginx/proxy_cache/temp;    #缓存临时目录    proxy_cache_path /usr/local/nginx/proxy_cache/cache levels=1:2 keys_zone=one:10m inactive=1d max_size=1g;    #缓存文件实际目录，levels定义层级目录，1:2说明1是一级目录，2是二级目录，keys_zone存储元数据，并分配10M内存空间。inctive表示1天没有被访问的缓存就删除，默认10分钟。max_size是最大分配磁盘空间    server {       listen 80;       server_name 192.168.1.10;       location / {         proxy_cache one;   #调用缓存区         #proxy_cache_valid 200 304 12h; #可根据HTTP状态码设置不同的缓存时间         proxy_cache_valid any  10m;    #缓存有效期为10分钟       }       #清除URL缓存，允许来自哪个网段的IP可以清除缓存（需要安装第三方模块"ngx_cache_purge"）,清除URL缓存方法：访问http://192.168.1.10/purge/文件名       location ~ /purge(/.*){         allow 127.0.0.1;         allow 192.168.1.0/24;         deny all;         proxy_cache_purge cache_one$host$1$is_args$args;       } } 

小结：

启用压缩模块可以节省一部分带宽，会增加WEB端CPU处理，但在上图网站架构中，WEB端启用压缩模块并没有起到作用，因为传输到上层走的是局域 网。对于直接面向用户的架构还是要启用的。WEB也不用启用expires模块，因为有了反向代理服务器和CDN，所以到不了用户浏览器，开启起不到作 用。

如果反向代理使用nginx做代理，可开启expires模块，将静态文件缓存到用户浏览器，浏览器发起请求时，先判断本地缓存是否有请求的数据，如果有再判断是否过期，如果不过期就直接浏览缓存数据，哪怕服务器资源已经改变，所以要根据业务情况合理设置过期时间。

5. 利用PHP缓存器提高代码执行效率

php程序在没有使用缓存器情况下，每次请求php页面，php都会对此页面进行代码编译，这就意味着重复的编译工作会增加服务器负载。有了缓存器 就会把每次编译后的数据缓存到共享内存中，下次访问直接使用缓冲区已编译好的代码，从而避免重复的编译过程，以加快其执行效率。因此PHP网站使用缓存器 是完全有必要的！主流的PHP缓存器有：eAccelerator、XCache

第五层：动静分离

动静分离，顾名思义，是将动态页面和静态页面分离到不同服务器上处理，比如使用web是nginx，可以让fastcgi部署到单独一台服务器，专 门解析php动态页面，静态页面默认由nginx处理，并做好缓存策略。再比如一个商城网站，会有大量的图片，可以考虑增加文件服务器组，将请求图片和上 传图片的都交给文件服务器处理。文件服务器主流使用NFS，存在单点故障，可以DRBD+HeartBeat+NFS部署高可用，如果单台压力过大，考虑 使用分布式文件系统，如GlusterFS、MooseFS等。

《DRBD + Heratbeat + NFS 高可用文件共享存储》：http://blog.jobbole.com/94718/

第六层：数据库缓存

利用缓存技术，把热数据缓存到内存中，如果请求的数据在缓存中，就直接返回，否则去数据库中取，并更新把拿的数据更新到缓存系统，提高读性能，降低 数据库压力。缓存实现有本地缓存和分布式缓存，本地缓存是将数据缓存到本地服务器内存中或者文件中。分布式缓存是将数据缓存到内存中，是分布式的，可以缓 存海量数据，扩展性好。主流的分布式缓存系统有Memcached和Redis，Memcached性能稳定，速度很快，QPS可达8w左右。如果想数据 持久化就选择用Redis，性能不低于Memcached。

第七层：数据库

这层在整个网站架构中起着主导型作用，直接决定用户体验，相对架构优化也比较复杂。

核心思路：减少请求层，尽可能让前端层返回用户请求的数据，减少后端服务器访问频率，最重要是数据库层。

参考：http://blog.jobbole.com/94718/

Memento模式来源：
       我们在进行软件系统的设计时候是要给用户后悔的权利（实际上可能也是用户要求的权利），我们对一些关键性的操作肯定需要提供诸如撤销（Undo）的操作。那这个后悔药就是Memento模式提供的。
Memento模式作用：


      在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，这样以后就可将该对象恢复到原先保存的状态。
Memento模式UML结构图如图1所示：

                     

Memento模式构成：


      Originator：负责创建一个备忘录Memento，用以记录当前时刻它的内部状态，并可使用备忘录恢复内部状态。Originator可根据需要决定Memento存储Originator的哪些内部状态。
      Memento:负责存储Originator对象的内部状态，并可防止Originator以外的其他对象访问备忘录Memento。备忘录有两个接口，Caretaker只能看到备忘录的窄接口，它只能将备忘录传递给其他对象。Originator能够看到一个宽接口，允许它访问返回到先前状态所需的所有数据。
      Caretaker：负责保存好备忘录Memento,不能对备忘录的内容进行操作或检查。
Memento模式代码示例：


Memento.h

#ifndef _MEMENTO_H_
#define _MEMENTO_H_
#include &lt;string&gt;

using namespace std;

//负责存储Originator对象的内部状态，并可防止Originator以外的其他对象访问备忘录Memento。
//备忘录有两个接口，Caretaker只能看到备忘录的窄接口，它只能将备忘录传递给其他对象。Originator能够看到一个宽接口，允许它访问返回到先前状态所需的所有数据。
class Memento
{
private:
    //将Originator为friend类，可以访问内部信息，但是其他类不能访问
    friend class Originator;
    Memento(const string&amp; state);
    ~Memento();
    void SetState(const string&amp; state);
    string GetState();
    string _state;
};

//负责创建一个备忘录Memento，用以记录当前时刻它的内部状态，并可使用备忘录恢复内部状态
class Originator
{
public:
    Originator();
    Originator(const string&amp; state);
    ~Originator();
    void RestoreToMemento(Memento* pMemento);
    Memento* CreateMemento();
    void SetState(const string&amp; state);
    string GetState();
    void show();
protected:
private:
    string _state;
};

//负责保存好备忘录Memento,不能对备忘录的内容进行操作或检查
class Caretaker
{
public:
    Caretaker();
    ~Caretaker();
    void SetMemento(Memento*);
    Memento* GetMemento();
private:
    Memento* _memento;
};

#endif&lt;/span&gt;

Memento.cpp

#include "Memento.h"
#include &lt;iostream&gt;
#include &lt;string&gt;

using namespace std;

Memento::Memento(const string&amp; state)
{
    this-&gt;_state = state;
}

Memento::~Memento()
{}

string Memento::GetState()
{
    return this-&gt;_state;
}

void Memento::SetState(const string&amp; state)
{
    this-&gt;_state = state;
}

Originator::Originator()
{}

Originator::Originator(const string&amp; state)
{
    this-&gt;_state = state;
}

Originator::~Originator()
{}

string Originator::GetState()
{
    return this-&gt;_state;
}

void Originator::show()
{
    cout &lt;&lt; this-&gt;_state &lt;&lt; endl;
}

void Originator::SetState(const string&amp; state)
{
    this-&gt;_state = state;
}

Memento* Originator::CreateMemento()
{
    return new Memento(this-&gt;_state);
}

void Originator::RestoreToMemento(Memento* pMemento)
{
    this-&gt;_state = pMemento-&gt;GetState();
}

Caretaker::Caretaker()
{}

Caretaker::~Caretaker()
{}

Memento* Caretaker::GetMemento()
{
    return this-&gt;_memento;
}

void Caretaker::SetMemento(Memento* pMemento)
{
    this-&gt;_memento = pMemento;
}&lt;/span&gt;

Main.cpp
#include "Memento.h"

int main()
{
    //初始化对象，状态为“Old”
    Originator* o = new Originator("Old");
    o-&gt;show();

    //建立并保存Memento
    Caretaker* pTaker = new Caretaker();
    pTaker-&gt;SetMemento(o-&gt;CreateMemento());

    //改变状态
    o-&gt;SetState("New");
    o-&gt;show();

    //恢复状态
    o-&gt;RestoreToMemento(pTaker-&gt;GetMemento());
    o-&gt;show();

    return 0;
}&lt;/span&gt;Memento模式的适用性：
(1).Memento模式比较适用于功能比较复杂的，但需要维护或记录历史属性的类，或者需要保存的属性只是众多属性中的一小部分时，Originator可以根据保存的Memento信息还原到前一状态。如果在某个系统中使用命令模式时，需要实现命令的撤销功能，那么命令模式可以使用备忘录模式来存储可撤销操作的状态。
Memento模式的优缺点：

优点：
(1).当发起人角色的状态有改变时，有可能是个错误的改变，我们使用备忘录模式就可以把这个错误改变还原。
(2).备份的状态是保存在发起人角色之外的，这样，发起人角色就不需要对各个备份的状态进行管理。
缺点：
(1).如果备份的对象存在大量的信息或者创建、恢复操作非常频繁，则可能造成很大的性能开销。
     Memento模式的使用总结：Memento模式的关键就是要在不破坏封装行的前提下，捕获并保存一个类的内部状态，这样就可以利用该保存的状态实施恢复操作。为了达到这个目标，可以在后面的实现中看到我们采取了一定语言支持的技术
      Memento模式的关键就是friend class Originator;我们可以看到，Memento的接口都声明为private，而将Originator声明为Memento的友元类。我们将Originator的状态保存在Memento类中，而将Memento接口private起来，也就达到了封装的功效。
 
     在Originator类中我们提供了方法让用户后悔：RestoreToMemento(Memento* mt)；我们可以通过这个接口让用户后悔。在示例程序中，我们演示了这一点：Originator的状态由old变为new最后又回到了old。


Observer模式来源：
         Observer模式应该可以说是应用最多、影响最广的模式之一。
         因为Observer的一个实例Model/View/Control（MVC）结构在系统开发架构设计中有着很重要的地位和意义，MVC实现了业务逻辑和表示层的解耦。在MFC中，Doc/View（文档视图结构）提供了实现MVC的框架结构（有一个从设计模式（Observer模式）的角度分析分析Doc/View的文章正在进一步的撰写当中，遗憾的是时间：））。在Java阵容中，Struts则提供和MFC中Doc/View结构类似的实现MVC的框架。另外Java语言本身就提供了Observer模式的实现接口。
        当然，MVC只是Observer模式的一个实例。Observer模式要解决的问题为：建立一个一（Subject）对多（Observer）的依赖关系，并且做到当“一”变化的时候，依赖这个“一”的多也能够同步改变。
        最常见的一个例子就是：对同一组数据进行统计分析时候，我们希望能够提供多种形式的表示（例如以表格进行统计显示、柱状图统计显示、百分比统计显示等）。这些表示都依赖于同一组数据，我们当然需要当数据改变的时候，所有的统计的显示都能够同时改变。Observer模式就是解决了这一个问题而产生的，如图1所示。
                                    
                                                                        图1  Observer模式示例图
Observer模式作用：

     观察者模式定义了一种一对多的依赖关系，让多个观察者对象同时监听某一个主题对象，这个主题对象在状态发生变化时，会通知所有观察者对象，使它们能够自动更新自己。
Observer模式UML结构图如图2所示：

            
Observer模式的构成：

      Subject类：可翻译为主题或抽象通知者，一般用一个抽象类或者一个借口实现。它把所有对观察者对象的引用保存在一个聚集里，每个主题都可以有任何数量的观察者。抽象主题提供一个借口，可以增加和删除观察者对象。
      Observer类：抽象观察者，为所有的具体观察者定义一个借口，在得到主题的通知时更新自己。这个借口叫做更新接口。抽象观察者一般用一个抽象类或者一个接口实现。更新接口通常包含一个Update()方法。
      ConcreteSubject类：叫做具体主题或具体通知者，将有关状态存入具体通知者对象；在具体主题的内部状态改变时，给所有等级过的观察者发出通知。通常用一个具体子类实现。
      ConcreteObserver类：具体观察者，实现抽象观察者角色所要求的更新接口，以便使本身的状态与主题的状态相协调。具体观察者角色可以保存一个指向一个具体主题对象的引用。
Observer模式的代码示例：

Observer.h

#ifndef _OBSERVER_H_
#define _OBSERVER_H_

#include &lt;string&gt;
#include &lt;list&gt;
using namespace std;

class Subject;

class Observer
{
public:
    ~Observer();
    virtual void Update(Subject*)=0;
protected:
    Observer();
private:
};

class ConcreteObserverA : public Observer
{
public:
    ConcreteObserverA();
    ~ConcreteObserverA();
    virtual void Update(Subject*);
protected:
private:
    string m_state;
};

class ConcreteObserverB : public Observer
{
public:
    ConcreteObserverB();
    ~ConcreteObserverB();
    virtual void Update(Subject*);
protected:
private:
    string m_state;
};

class Subject
{
public:
    ~Subject();
    virtual void Notify();
    virtual void Attach(Observer*);
    virtual void Detach(Observer*);
    virtual string GetState();
    virtual void SetState(string state);
protected:
    Subject();
private:
    string m_state;
    list&lt;Observer*&gt; m_lst;
};

class ConcreteSubjectA : public Subject
{
public:
    ConcreteSubjectA();
    ~ConcreteSubjectA();
protected:
private:
};

class ConcreteSubjectB : public Subject
{
public:
    ConcreteSubjectB();
    ~ConcreteSubjectB();
protected:
private:
};

#endif

Observer.cpp

#include "Observer.h"
#include &lt;iostream&gt;
#include &lt;algorithm&gt;

using namespace std;

Observer::Observer()
{}

Observer::~Observer()
{}

ConcreteObserverA::ConcreteObserverA()
{}

ConcreteObserverA::~ConcreteObserverA()
{}

void ConcreteObserverA::Update(Subject* pSubject)
{
    this-&gt;m_state = pSubject-&gt;GetState();
    cout &lt;&lt; "The ConcreteObserverA is " &lt;&lt; m_state &lt;&lt; std::endl;
}

ConcreteObserverB::ConcreteObserverB()
{}

ConcreteObserverB::~ConcreteObserverB()
{}

void ConcreteObserverB::Update(Subject* pSubject)
{
    this-&gt;m_state = pSubject-&gt;GetState();
    cout &lt;&lt; "The ConcreteObserverB is " &lt;&lt; m_state &lt;&lt; std::endl;
}

Subject::Subject()
{}

Subject::~Subject()
{}

void Subject::Attach(Observer* pObserver)
{
    this-&gt;m_lst.push_back(pObserver);
    cout &lt;&lt; "Attach an Observer\n";
}

void Subject::Detach(Observer* pObserver)
{
    list&lt;Observer*&gt;::iterator iter;
    iter = find(m_lst.begin(),m_lst.end(),pObserver);
    if(iter != m_lst.end())
    {
        m_lst.erase(iter);
    }
    cout &lt;&lt; "Detach an Observer\n";
}

void Subject::Notify()
{
    list&lt;Observer*&gt;::iterator iter = this-&gt;m_lst.begin();
    for(;iter != m_lst.end();iter++)
    {
        (*iter)-&gt;Update(this);
    }
}

string Subject::GetState()
{
    return this-&gt;m_state;
}

void Subject::SetState(string state)
{
    this-&gt;m_state = state;
}

ConcreteSubjectA::ConcreteSubjectA()
{}

ConcreteSubjectA::~ConcreteSubjectA()
{}

ConcreteSubjectB::ConcreteSubjectB()
{}

ConcreteSubjectB::~ConcreteSubjectB()
{}Main.cpp


#include "Observer.h"
#include &lt;iostream&gt;

using namespace std;

int main()
{
    Observer* p1 = new ConcreteObserverA();
    Observer* p2 = new ConcreteObserverB();
    Observer* p3 = new ConcreteObserverA();

    Subject* pSubject = new ConcreteSubjectA();
    pSubject-&gt;Attach(p1);
    pSubject-&gt;Attach(p2);
    pSubject-&gt;Attach(p3);

    pSubject-&gt;SetState("old");

    pSubject-&gt;Notify();

    cout &lt;&lt; "-------------------------------------" &lt;&lt; endl;
    pSubject-&gt;SetState("new");

    pSubject-&gt;Detach(p3);
    pSubject-&gt;Notify();

    return 0;
}


        在Observer模式的实现中，Subject维护一个list作为存储其所有观察者的容器。每当调用Notify操作就遍历list中的Observer对象，并广播通知改变状态（调用Observer的Update操作）。目标的状态state可以由Subject自己改变（示例），也可以由Observer的某个操作引起state的改变（可调用Subject的SetState操作）。Notify操作可以由Subject目标主动广播（示例），也可以由Observer观察者来调用（因为Observer维护一个指向Subject的指针）。运行示例程序，可以看到当Subject处于状态“old”时候，依赖于它的两个观察者都显示“old”，当目标状态改变为“new”的时候，依赖于它的两个观察者也都改变为“new”。
Observer模式使用场景：
（1）.当一个对象的改变需要同时改变其他对象的时候，而且它不知道具体有多少对象有待改变时，应该考虑使用观察者模式。
（2）.观察者模式所做的工作其实就是在解除耦合。让耦合的双方都依赖于抽象，而不是依赖于具体。从而使得各自的变化都不会影响另一边的变化。
Observer模式优缺点总结：

观察者模式的优点：
（1）. Subject和Observer之间是松耦合的，分别可以各自独立改变。
（2）. Subject在发送广播通知的时候，无须指定具体的Observer，Observer可以自己决定是否要订阅Subject的通知。
（3）. 遵守大部分GRASP原则和常用设计原则，高内聚、低耦合。
观察者模式的缺点：
（1）.松耦合导致代码关系不明显，有时可能难以理解。(废话)
（2）. 如果一个Subject被大量Observer订阅的话，在广播通知的时候可能会有效率问题。（毕竟只是简单的遍历）
Observer模式的使用总结：

        Observer是影响极为深远的模式之一，也是在大型系统开发过程中要用到的模式之一。除了MFC、Struts提供了MVC的实现框架，在Java语言中还提供了专门的接口实现Observer模式：通过专门的类Observable及Observer接口来实现MVC编程模式，其UML图可以表示为：Java中实现MVC的UML图。这里的Observer就是观察者，Observable则充当目标Subject的角色。
             
       Observer模式也称为发布－订阅（publish-subscribe），目标就是通知的发布者，观察者则是通知的订阅者（接受通知）。






State模式来源： 
       每个人、事物在不同的状态下会有不同表现（动作），而一个状态又会在不同的表现下转移到下一个不同的状态（State）。最简单的一个生活中的例子就是：地铁入口处，如果你放入正确的地铁票，门就会打开让你通过。在出口处也是验票，如果正确你就可以ok，否则就不让你通过（如果你动作野蛮，或许会有报警（Alarm））。
      有限状态自动机（FSM）也是一个典型的状态不同，对输入有不同的响应（状态转移）。通常我们在实现这类系统会使用到很多的Switch/Case语句，Case某种状态，发生什么动作，Case另外一种状态，则发生另外一种状态。但是这种实现方式至少有以下两个问题：
      (1).当状态数目不是很多的时候，Switch/Case可能可以搞定。但是当状态数目很多的时候（实际系统中也正是如此），维护一大组的Switch/Case语句将是一件异常困难并且容易出错的事情。
      (2).状态逻辑和动作实现没有分离。在很多的系统实现中，动作的实现代码直接写在状态的逻辑当中。这带来的后果就是系统的扩展性和维护得不到保证。

State模式作用：

      State模式就是被用来解决上面列出的两个问题的，在State模式中我们将状态逻辑和动作实现进行分离。当一个操作中要维护大量的case分支语句，并且这些分支依赖于对象的状态。State模式将每一个分支都封装到独立的类中。

State模式UML结构图如图1所示：

 
                    

State模式的构成：


State类：抽象状态类，定义一个接口以封装与Context的一个特定状态相关的行为。
ConcreteState类：具体状态，每一个子类实现一个与Context的一个状态相关的行为。
Context类：维护一个ConcreteState子类的实例，这个实例定义当前的状态。
State模式的代码示例：
State.h

#ifndef _STATE_H_
#define _STATE_H_

class Context;
class State
{
public:
    virtual void Handle(Context* pContext)=0;
    ~State();
protected:
    State();
private:
};

class ConcreteStateA : public State
{
public:
    ConcreteStateA();
    ~ConcreteStateA();
    virtual void Handle(Context* pContext);
protected:
private:
};

class ConcreteStateB : public State
{
public:
    ConcreteStateB();
    ~ConcreteStateB();
    virtual void Handle(Context* pContext);
protected:
private:
};

class ConcreteStateC : public State
{
public:
    ConcreteStateC();
    ~ConcreteStateC();
    virtual void Handle(Context* pContext);
protected:
private:
};

class Context
{
public:
    Context(State* pState);
    ~Context();
    void Request();
    void ChangeState(State* pState);
protected:
private:
    State* _state;
};

#endif
State.cpp

#include "State.h"
#include &lt;iostream&gt;
using namespace std;
State::State()
{}
State::~State()
{}
ConcreteStateA::ConcreteStateA()
{}
ConcreteStateA::~ConcreteStateA()
{}
//执行该状态的行为并改变状态
void ConcreteStateA::Handle(Context* pContext)
{
    cout &lt;&lt; "ConcreteStateA" &lt;&lt; endl;
    pContext-&gt;ChangeState(new ConcreteStateB());
}
ConcreteStateB::ConcreteStateB()
{}
ConcreteStateB::~ConcreteStateB()
{}
//执行该状态的行为并改变状态
void ConcreteStateB::Handle(Context* pContext)
{
    cout &lt;&lt; "ConcreteStateB" &lt;&lt; endl;
    pContext-&gt;ChangeState(new ConcreteStateC());
}
ConcreteStateC::ConcreteStateC()
{}
ConcreteStateC::~ConcreteStateC()
{}
//执行该状态的行为并改变状态
void ConcreteStateC::Handle(Context* pContext)
{
    cout &lt;&lt; "ConcreteStateC" &lt;&lt; endl;
    pContext-&gt;ChangeState(new ConcreteStateA());
}

//定义_state的初始状态
Context::Context(State* pState)
{
    this-&gt;_state = pState;
}
Context::~Context()
{}
//对请求做处理，并设置下一状态
void Context::Request()
{
    if(NULL != this-&gt;_state)
    {
        this-&gt;_state-&gt;Handle(this);
    }
}
//改变状态
void Context::ChangeState(State* pState)
{
    this-&gt;_state = pState;
}
Main.cpp


#include "State.h"

int main()
{
    State* pState = new ConcreteStateA();
    Context* pContext = new Context(pState);
    pContext-&gt;Request();
    pContext-&gt;Request();
    pContext-&gt;Request();
    pContext-&gt;Request();
    pContext-&gt;Request();
    return 0;
}

State模式使用场景：

 
      State模式的应用也非常广泛，从最高层逻辑用户接口GUI到最底层的通讯协议（例如利用State模式模拟实现一个TCP连接的类。）都有其用武之地。

 
       State模式和Strategy模式又很大程度上的相似：它们都有一个Context类，都是通过委托（组合）给一个具有多个派生类的多态基类实现Context的算法逻辑。两者最大的差别就是State模式中派生类持有指向Context对象的引用，并通过这个引用调用Context中的方法，但在Strategy模式中就没有这种情况。因此可以说一个State实例同样是Strategy模式的一个实例，反之却不成立。实际上State模式和Strategy模式的区别还在于它们所关注的点不尽相同：State模式主要是要适应对象对于状态改变时的不同处理策略的实现，而Strategy则主要是具体算法和实现接口的解耦（coupling），Strategy模式中并没有状态的概念（虽然很多时候有可以被看作是状态的概念），并且更加不关心状态的改变了。
       State模式很好地实现了对象的状态逻辑和动作实现的分离，状态逻辑分布在State的派生类中实现，而动作实现则可以放在Context类中实现（这也是为什么State派生类需要拥有一个指向Context的指针）。这使得两者的变化相互独立，改变State的状态逻辑可以很容易复用Context的动作，也可以在不影响State派生类的前提下创建Context的子类来更改或替换动作实现。
State模式经典示例： 
   

       允许一个对象在其内部状态改变时改变它的行为。对象看起来似乎修改了它的类。
1-&gt;main()，客户
2-&gt;CLiftState，电梯状态抽象类
3-&gt;CCloseingState，电梯门关闭
4-&gt;COpenningState，电梯门打开
5-&gt;CRunningState，电梯运行
6-&gt;CStoppingState，电梯停止
7-&gt;CContext，电梯的控制面板
       说明：CContext保持电梯的状态，并提供操作的接口函数。当函数被调用时，CContext直接调用当前状态的相应函数。由状态的接口函数来确定是否可以执行这个动作，以及修改状态为执行这个动作后的状态。
       看代码：第一块是不使用模式的做法，第二块是使用模式的做法，在main()函数里会有调用的方式。
 
&lt;span style="font-size:14px;"&gt;&lt;span style="font-size:14px;"&gt;//ILift.h
#pragma once
class ILift
{
public:

    ILift(void)
    {
    }

    virtual ~ILift(void)
    {
    }
    static const int OPENING_STATE = 1;
    static const int CLOSING_STATE = 2;
    static const int RUNNING_STATE = 3;
    static const int STOPPING_STATE = 4;
    virtual void SetState(int state) = 0;
    virtual void Open() = 0;
    virtual void Close() = 0;
    virtual void Run() = 0;
    virtual void Stop() = 0;
};

//Lift.h
#pragma once
#include "ilift.h"
class CLift :
    public ILift
{
public:
    CLift(void);
    ~CLift(void);
    void SetState(int state);
    void Open();
    void Close();
    void Run();
    void Stop();
private:
    int m_state;
    void OpenWithoutLogic();
    void CloseWithoutLogic();
    void RunWithoutLogic();
    void StopWithoutLogic();
};

//Lift.cpp
#include "StdAfx.h"
#include "Lift.h"
#include &lt;iostream&gt;
using std::cout;
using std::endl;

CLift::CLift(void)
{
    this-&gt;m_state = 0;
}

CLift::~CLift(void)
{
}

void CLift::SetState(int state)
{
    this-&gt;m_state = state;
}

void CLift::Open()
{
    switch(this-&gt;m_state)
    {
    case OPENING_STATE:
        break;
    case CLOSING_STATE:
        this-&gt;OpenWithoutLogic();
        this-&gt;SetState(OPENING_STATE);
        break;
    case RUNNING_STATE:
        break;
    case STOPPING_STATE:
        this-&gt;OpenWithoutLogic();
        this-&gt;SetState(OPENING_STATE);
        break;
    }
}

void CLift::Close()
{
    switch(this-&gt;m_state)
    {
    case OPENING_STATE:
        this-&gt;CloseWithoutLogic();
        this-&gt;SetState(CLOSING_STATE);
        break;
    case CLOSING_STATE:
        break;
    case RUNNING_STATE:
        break;
    case STOPPING_STATE:
        break;
    }
}

void CLift::Run()
{
    switch(this-&gt;m_state)
    {
    case OPENING_STATE:
        break;
    case CLOSING_STATE:
        this-&gt;RunWithoutLogic();
        this-&gt;SetState(RUNNING_STATE);
        break;
    case RUNNING_STATE:
        break;
    case STOPPING_STATE:
        this-&gt;RunWithoutLogic();
        this-&gt;SetState(RUNNING_STATE);
        break;
    }
}

void CLift::Stop()
{
    switch(this-&gt;m_state)
    {
    case OPENING_STATE:
        break;
    case CLOSING_STATE:
        this-&gt;StopWithoutLogic();
        this-&gt;SetState(CLOSING_STATE);
        break;
    case RUNNING_STATE:
        this-&gt;StopWithoutLogic();
        this-&gt;SetState(CLOSING_STATE);
        break;
    case STOPPING_STATE:
        break;
    }
}

void CLift::OpenWithoutLogic()
{
    cout &lt;&lt; "电梯门开启..." &lt;&lt; endl;
}

void CLift::CloseWithoutLogic()
{
    cout &lt;&lt; "电梯门关闭..." &lt;&lt; endl;
}

void CLift::RunWithoutLogic()
{
    cout &lt;&lt; "电梯上下跑起来..." &lt;&lt; endl;
}

void CLift::StopWithoutLogic()
{
    cout &lt;&lt; "电梯停止了..." &lt;&lt; endl;
}&lt;/span&gt;&lt;/span&gt;
&lt;span style="font-size:14px;"&gt;&lt;span style="font-size:14px;"&gt;//LiftState.h
#pragma once
class CContext;
class CLiftState
{
public:
    CLiftState(void);
    virtual ~CLiftState(void);
    void SetContext(CContext *pContext);
    virtual void Open() = 0;
    virtual void Close() = 0;
    virtual void Run() = 0;
    virtual void Stop() = 0;
protected:
    CContext *m_pContext;
};

//LiftState.cpp
#include "StdAfx.h"
#include "LiftState.h"

CLiftState::CLiftState(void)
{
}

CLiftState::~CLiftState(void)
{
}

void CLiftState::SetContext( CContext *pContext )
{
    m_pContext = pContext;
}

//CloseingState.h
#pragma once
#include "liftstate.h"
class CCloseingState :
    public CLiftState
{
public:
    CCloseingState(void);
    ~CCloseingState(void);
    void Open();
    void Close();
    void Run();
    void Stop();
};

//CloseingState.cpp
#include "StdAfx.h"
#include "CloseingState.h"
#include "Context.h"
#include &lt;iostream&gt;
using std::cout;
using std::endl;

CCloseingState::CCloseingState(void)
{
}

CCloseingState::~CCloseingState(void)
{
}

void CCloseingState::Open()
{
    this-&gt;CLiftState::m_pContext-&gt;SetLiftState(CContext::pOpenningState);
    this-&gt;CLiftState::m_pContext-&gt;GetLiftState()-&gt;Open();
}

void CCloseingState::Close()
{
    cout &lt;&lt; "电梯门关闭..." &lt;&lt; endl;
}

void CCloseingState::Run()
{
    this-&gt;CLiftState::m_pContext-&gt;SetLiftState(CContext::pRunningState);
    this-&gt;CLiftState::m_pContext-&gt;GetLiftState()-&gt;Run();
}

void CCloseingState::Stop()
{
    this-&gt;CLiftState::m_pContext-&gt;SetLiftState(CContext::pStoppingState);
    this-&gt;CLiftState::m_pContext-&gt;GetLiftState()-&gt;Stop();
}

//OpenningState.h
#pragma once
#include "liftstate.h"
class COpenningState :
    public CLiftState
{
public:
    COpenningState(void);
    ~COpenningState(void);
    void Open();
    void Close();
    void Run();
    void Stop();
};

//OpenningState.cpp
#include "StdAfx.h"
#include "OpenningState.h"
#include "Context.h"
#include &lt;iostream&gt;
using std::cout;
using std::endl;

COpenningState::COpenningState(void)
{
}

COpenningState::~COpenningState(void)
{
}

void COpenningState::Open()
{
    cout &lt;&lt; "电梯门开启..." &lt;&lt; endl;
}

void COpenningState::Close()
{
    this-&gt;CLiftState::m_pContext-&gt;SetLiftState(CContext::pCloseingState);
    this-&gt;CLiftState::m_pContext-&gt;GetLiftState()-&gt;Close();
}

void COpenningState::Run()
{
    //do nothing
}

void COpenningState::Stop()
{
    //do nothing
}

//RunningState.h
#pragma once
#include "liftstate.h"
class CRunningState :
    public CLiftState
{
public:
    CRunningState(void);
    ~CRunningState(void);
    void Open();
    void Close();
    void Run();
    void Stop();
};

//RunningState.cpp
#include "StdAfx.h"
#include "RunningState.h"
#include "Context.h"
#include &lt;iostream&gt;
using std::cout;
using std::endl;

CRunningState::CRunningState(void)
{
}

CRunningState::~CRunningState(void)
{
}

void CRunningState::Open()
{
    //do nothing
}

void CRunningState::Close()
{
    //do nothing
}

void CRunningState::Run()
{
    cout &lt;&lt; "电梯上下跑..." &lt;&lt; endl;
}

void CRunningState::Stop()
{
    this-&gt;CLiftState::m_pContext-&gt;SetLiftState(CContext::pStoppingState);
    this-&gt;CLiftState::m_pContext-&gt;GetLiftState()-&gt;Stop();
}

//StoppingState.h
#pragma once
#include "liftstate.h"
class CStoppingState :
    public CLiftState
{
public:
    CStoppingState(void);
    ~CStoppingState(void);
    void Open();
    void Close();
    void Run();
    void Stop();
};

//StoppingState.cpp
#include "StdAfx.h"
#include "StoppingState.h"
#include "Context.h"
#include &lt;iostream&gt;
using std::cout;
using std::endl;

CStoppingState::CStoppingState(void)
{
}

CStoppingState::~CStoppingState(void)
{
}

void CStoppingState::Open()
{
    this-&gt;CLiftState::m_pContext-&gt;SetLiftState(CContext::pOpenningState);
    this-&gt;CLiftState::m_pContext-&gt;GetLiftState()-&gt;Open();
}

void CStoppingState::Close()
{
    //do nothing
}

void CStoppingState::Run()
{
    this-&gt;CLiftState::m_pContext-&gt;SetLiftState(CContext::pRunningState);
    this-&gt;CLiftState::m_pContext-&gt;GetLiftState()-&gt;Run();
}

void CStoppingState::Stop()
{
    cout &lt;&lt; "电梯停止了..." &lt;&lt; endl;
}&lt;/span&gt;&lt;/span&gt;
&lt;span style="font-size:14px;"&gt;&lt;span style="font-size:14px;"&gt;//Contex.h
#pragma once
#include "LiftState.h"
#include "OpenningState.h"
#include "CloseingState.h"
#include "RunningState.h"
#include "StoppingState.h"
class CContext
{
public:
    CContext(void);
    ~CContext(void);
    static COpenningState *pOpenningState;
    static CCloseingState *pCloseingState;
    static CRunningState *pRunningState;
    static CStoppingState *pStoppingState;
    CLiftState * GetLiftState();
    void SetLiftState(CLiftState *pLiftState);
    void Open();
    void Close();
    void Run();
    void Stop();
private:
    CLiftState *m_pLiftState;
};

//Context.cpp
#include "StdAfx.h"
#include "Context.h"
COpenningState* CContext::pOpenningState = NULL;
CCloseingState* CContext::pCloseingState = NULL;
CRunningState* CContext::pRunningState = NULL;
CStoppingState* CContext::pStoppingState = NULL;

CContext::CContext(void)
{
    m_pLiftState = NULL;
    pOpenningState = new COpenningState();
    pCloseingState = new CCloseingState();
    pRunningState = new CRunningState();
    pStoppingState = new CStoppingState();
}

CContext::~CContext(void)
{
    delete pOpenningState;
    pOpenningState = NULL;
    delete pCloseingState;
    pCloseingState = NULL;
    delete pRunningState;
    pRunningState = NULL;
    delete pStoppingState;
    pStoppingState = NULL;
}

CLiftState * CContext::GetLiftState()
{
    return m_pLiftState;
}

void CContext::SetLiftState(CLiftState *pLiftState)
{
    this-&gt;m_pLiftState = pLiftState;
    this-&gt;m_pLiftState-&gt;SetContext(this);
}

void CContext::Open()
{
    this-&gt;m_pLiftState-&gt;Open();
}

void CContext::Close()
{
    this-&gt;m_pLiftState-&gt;Close();
}

void CContext::Run()
{
    this-&gt;m_pLiftState-&gt;Run();
}

void CContext::Stop()
{
    this-&gt;m_pLiftState-&gt;Stop();
}

// State.cpp : 定义控制台应用程序的入口点。
#include "stdafx.h"
#include "ILift.h"
#include "Lift.h"
#include "Context.h"
#include "OpenningState.h"
#include "CloseingState.h"
#include "RunningState.h"
#include "StoppingState.h"
#include &lt;iostream&gt;
using std::cout;
using std::endl;

void DoIt()
{
    //ILift.h, Lift.h, Lift.cpp
    ILift *pLift = new CLift();
    pLift-&gt;SetState(ILift::STOPPING_STATE);//电梯的初始条件是停止状态。
    pLift-&gt;Open();//首先是电梯门开启，人进去
    pLift-&gt;Close();//然后电梯门关闭
    pLift-&gt;Run();//再然后，电梯跑起来，向上或者向下
    pLift-&gt;Stop();//最后到达目的地，电梯停下来
    delete pLift;
}


void DoNew()
{
    //LiftState.h, LiftState.cpp, OpenningState.h, CloseingState.h, RunningState.h, StoppingState.h
    //Context.h, Context.cpp
    CContext context;
    CCloseingState closeingState;
    context.SetLiftState(&amp;closeingState);
    context.Close();
    context.Open();
    context.Run();
    context.Stop();
}

int _tmain(int argc, _TCHAR* argv[])
{
    cout &lt;&lt; "----------使用模式之前----------" &lt;&lt; endl;
    DoIt();
    cout &lt;&lt; "----------使用模式之后----------" &lt;&lt; endl;
    DoNew();

    _CrtSetDbgFlag(_CRTDBG_LEAK_CHECK_DF | _CRTDBG_ALLOC_MEM_DF);
    _CrtDumpMemoryLeaks();
    return 0;
}&lt;/span&gt;&lt;/span&gt;State状态模式的优点:

（1）.状态模式将与特定状态相关的行为局部化，并且将不同状态的行为分割开来。

（2）.所有状态相关的代码都存在于某个ConcereteState中，所以通过定义新的子类很容易地增加新的状态和转换。

（3）.状态模式通过把各种状态转移逻辑分不到State的子类之间，来减少相互间的依赖。
State状态模式的缺点:
（1）.在于使用状态模式会增加系统类和对象的个数，且状态模式的结构与实现都较为复杂，如果使用不当将导致程序结构和代码的混乱，对于可以切换状态的状态模式不满足“开闭原则”的要求。导致较多的ConcreteState子类

State状态模式使用总结：

      策略模式关注行为的变化,但归根结底只有一个行为,变化的只是行为的实现.客户不关注这些.当新增变化时对客户可以没有任何影响.状态模式同样关注行为的变化,但这个变化是由状态来驱动,一般来说每个状态和行为都不同.新增的状态或行为一般与已有的不同,客户需要关注这些变化.状态模式中State及其子类中的操作都将Context传入作为参数，以便State可以通过这个指针调用Context中的方法，修改状态。而策略模式没有。


Strategy模式来源： 

      在软件开发中也常常遇到类似的情况，实现某一个功能有多种算法或者策略，我们可以根据环境或者条件的不同选择不同的算法或者策略来完成该功能。如查找、排序等，一种常用的方法是硬编码(Hard Coding)在一个类中，如需要提供多种查找算法，可以将这些算法写到一个类中，在该类中提供多个方法，每一个方法对应一个具体的查找算法；当然也可以将这些查找算法封装在一个统一的方法中，通过if…else…或者case等条件判断语句来进行选择。这两种实现方法我们都可以称之为硬编码，如果需要增加一种新的查找算法，需要修改封装算法类的源代码；更换查找算法，也需要修改客户端调用代码。在这个算法类中封装了大量查找算法，该类代码将较复杂，维护较为困难。如果我们将这些策略包含在客户端，这种做法更不可取，将导致客户端程序庞大而且难以维护，如果存在大量可供选择的算法时问题将变得更加严重。
Strategy模式作用：

      Strategy模式和Template模式要解决的问题是相同（类似）的，都是为了给业务逻辑（算法）具体实现和抽象接口之间的解耦。Strategy模式将逻辑（算法）封装到一个类（Context）里面，通过组合的方式将具体算法的实现在组合对象中实现，再通过委托的方式将抽象接口的实现委托给组合对象实现。
Strategy模式UML结构图如图1所示：

                      

Strategy模式的构成：


      环境类(Context):用一个ConcreteStrategy对象来配置。维护一个对Strategy对象的引用。可定义一个接口来让Strategy访问它的数据。
      抽象策略类(Strategy):定义所有支持的算法的公共接口。 Context使用这个接口来调用某ConcreteStrategy定义的算法。
      具体策略类(ConcreteStrategy):以Strategy接口实现某具体算法。
Strategy模式的代码示例：


Strategy.h

#ifndef _STRATEGY_H_
#define _STRATEGY_H_

class Strategy
{
public:
    ~Strategy();
    virtual void AlgrithmInterface()=0;
protected:
    Strategy();
private:
};

class ConcreteStrategyA : public Strategy
{
public:
    ConcreteStrategyA();
    ~ConcreteStrategyA();
    virtual void AlgrithmInterface();
protected:
private:
};

class ConcreteStrategyB : public Strategy
{
public:
    ConcreteStrategyB();
    ~ConcreteStrategyB();
    virtual void AlgrithmInterface();
protected:
private:
};

/*这个类是Strategy模式的关键，
  也是Strategy模式和Template模式的根本区别所在。
  *Strategy通过“组合”（委托）方式实现算法（实现）的异构，
  而Template模式则采取的是继承的方式
  这两个模式的区别也是继承和组合两种实现接口重用的方式的区别
*/
class Context
{
public:
    Context(Strategy*);
    ~Context();
    void DoAction();
private:
    Strategy* _strategy;
};
#endif

Strategy.cpp

#include "Strategy.h"
#include "iostream"

using namespace std;

Strategy::Strategy()
{}

Strategy::~Strategy()
{}

ConcreteStrategyA::ConcreteStrategyA()
{}

ConcreteStrategyA::~ConcreteStrategyA()
{}

void ConcreteStrategyA::AlgrithmInterface()
{
    cout &lt;&lt; "ConcreteStrategyA::AlgrithmInterface" &lt;&lt; endl;
}

ConcreteStrategyB::ConcreteStrategyB()
{}

ConcreteStrategyB::~ConcreteStrategyB()
{}

void ConcreteStrategyB::AlgrithmInterface()
{
    cout &lt;&lt; "ConcreteStrategyB::AlgrithmInterface" &lt;&lt; endl;
}

Context::Context(Strategy* strategy)
{
    this-&gt;_strategy = strategy;
}

Context::~Context()
{
    delete this-&gt;_strategy;
}

void Context::DoAction()
{
    this-&gt;_strategy-&gt;AlgrithmInterface();
}


Main.cpp




#include "Strategy.h"

int main()
{
    /*
    Strategy模式和Template模式实际是实现一个抽象接口的两种方式：继承和组合之间的区别。
    要实现一个抽象接口，继承是一种方式：我们将抽象接口声明在基类中，将具体的实现放在具体子类中。
    组合（委托）是另外一种方式：我们将接口的实现放在被组合对象中，将抽象接口放在组合类中。
    这两种方式各有优缺点
    */
    //策略A与B可替换
    Strategy* pStr = new ConcreteStrategyA();
    Context* pcon = new Context(pStr);
    pcon-&gt;DoAction();

    pStr = new ConcreteStrategyB();
    pcon = new Context(pStr);
    pcon-&gt;DoAction();

    return 0;
}

Strategy模式使用场景：


(1).许多相关的类仅仅是行为有异。 “策略”提供了一种用多个行为中的一个行为来配置一个类的方法。即一个系统需要动态地在几种算法中选择一种。
(2).需要使用一个算法的不同变体。例如，你可能会定义一些反映不同的空间 /时间权衡的算法。当这些变体实现为一个算法的类层次时,可以使用策略模式。
(3).算法使用客户不应该知道的数据。可使用策略模式以避免暴露复杂的、与算法相关的数据结构。
(4).一个类定义了多种行为 , 并且这些行为在这个类的操作中以多个条件语句的形式出现。将相关的条件分支移入它们各自的Strategy类中以代替这些条件语句。
Strategy模式和Template模式优缺点对比分析：

       可以看到Strategy模式和Template模式解决了类似的问题，也正如在Template模式中分析的，Strategy模式和Template模式实际是实现一个抽象接口的两种方式：继承和组合之间的区别。要实现一个抽象接口，继承是一种方式：我们将抽象接口声明在基类中，将具体的实现放在具体子类中。组合（委托）是另外一种方式：我们将接口的实现放在被组合对象中，将抽象接口放在组合类中。这两种方式各有优缺点，先列出来：

1. 继承： 
优点： 
(1).易于修改和扩展那些被复用的实现。
缺点：
(1).破坏了封装性，继承中父类的实现细节暴露给子类了； 
(2). “白盒”复用，原因在(1)中； 
(3).当父类的实现更改时，其所有子类将不得不随之改变
(4).从父类继承而来的实现在运行期间不能改变（编译期间就已经确定了）。
2. 组合: 
优点： 
(1). “黑盒”复用，因为被包含对象的内部细节对外是不可见的； 
(2).封装性好，原因为(1)； 
(3).实现和抽象的依赖性很小（组合对象和被组合对象之间的依赖性小）； 
(4).可以在运行期间动态定义实现（通过一个指向相同类型的指针，典型的是抽象基类的指针）。
缺点： 
(1).系统中对象过多。
Strategy模式使用总结：

         从上面对比中我们可以看出，组合相比继承可以取得更好的效果，因此在面向对象的设计中的有一条很重要的原则就是：优先使用（对象）组合，而非（类）继承（FavorComposition OverInheritance）。
         实际上，继承是一种强制性很强的方式，因此也使得基类和具体子类之间的耦合性很强。例如在Template模式中在ConcreteClass1中定义的原语操作别的类是不能够直接复用（除非你继承自AbstractClass，具体分析请参看Template模式文档）。而组合（委托）的方式则有很小的耦合性，实现（具体实现）和接口（抽象接口）之间的依赖性很小，例如在本实现中，ConcreteStrategyA的具体实现操作很容易被别的类复用，例如我们要定义另一个Context类AnotherContext，只要组合一个指向Strategy的指针就可以很容易地复用ConcreteStrategyA的实现了。
         我们在Bridge模式的问题和Bridge模式的分析中，正是说明了继承和组合之间的区别。请参看相应模式解析。
         另外Strategy模式很State模式也有相似之处，但是State模式注重的对象在不同的状态下不同的操作。两者之间的区别就是State模式中具体实现类中有一个指向Context的引用，而Strategy模式则没有。
        此外，Strategy模式是一个比较容易理解和使用的设计模式，策略模式是对算法的封装，它把算法的责任和算法本身分割开，委派给不同的对象管理。策略模式通常把一个系列的算法封装到一系列的策略类里面，作为一个抽象策略类的子类。用一句话来说，就是“准备一组算法，并将每一个算法封装起来，使得它们可以互换”。在Strategy模式中，应当由客户端自己决定在什么情况下使用什么具体策略角色。
        Strategy模式仅仅封装算法，提供新算法插入到已有系统中，以及老算法从系统中“退休”的方便，策略模式并不决定在何时使用何种算法，算法的选择由客户端来决定。这在一定程度上提高了系统的灵活性，但是客户端需要理解所有具体策略类之间的区别，以便选择合适的算法，这也是策略模式的缺点之一，在一定程度上增加了客户端的使用难度。





Template模式来源：   
        在面向对象系统的分析与设计过程中经常会遇到这样一种情况：对于某一个业务逻辑（算法实现）在不同的对象中有不同的细节实现，但是逻辑（算法）的框架（或通用的应用算法）是相同的。Template提供了这种情况的一个实现框架。
Template模式作用：


Template模式又叫模板方法模式，在一个方法中定义一个算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以在不改变算法结构的情冴下，重新定义算法中的某些步骤。
 我们使用冲泡咖啡和冲泡茶的例子
加工流程：
咖啡冲泡法：1.把水煮沸、2.用沸水冲泡咖啡、3.把咖啡倒进杯子、4.加糖和牛奶
茶冲泡法：   1.把水煮沸、2.用沸水冲泡茶叶、3.把
  茶倒进杯子、4.加蜂蜜
Template模式UML结构图如图1所示：
                        

Template模式的构成：


抽象类（AbstractClass）：实现了模板方法，定义了算法的骨架。
具体类（ConcreteClass)：实现抽象类中的抽象方法，已完成完整的算法。
Template模式的代码示例：

TemplateMethod.h

#ifndef _TEMPLATEMETHOD_H_
#define _TEMPLATEMETHOD_H_

//抽象模板，并定义了一个模板方法。
class AbstractClass
{
public:
    ~AbstractClass();
    //具体的模板方法，给出了逻辑的骨架，而逻辑的组成是一些相应的抽象操作，它们都推迟到子类中实现
    void TemplateMethod();
    //一些抽象行为，放到子类中去实现
    virtual void PrimitiveOperation1()=0;
    virtual void PrimitiveOperation2()=0;
protected:
    AbstractClass();
private:
};

//实现基类所定义的抽象方法
class ConcreteClassA : public AbstractClass
{
public:
    ConcreteClassA();
    ~ConcreteClassA();
    //实现基类定义的抽象行为
    virtual void PrimitiveOperation1();
    virtual void PrimitiveOperation2();
private:
};

//实现基类所定义的抽象方法
class ConcreteClassB : public AbstractClass
{
public:
    ConcreteClassB();
    ~ConcreteClassB();
    //实现基类定义的抽象行为
    virtual void PrimitiveOperation1();
    virtual void PrimitiveOperation2();
private:
};
#endifTemplateMethod.cpp


#include "TemplateMethod.h"
#include &lt;iostream&gt;

using namespace std;

AbstractClass::AbstractClass()
{}

AbstractClass::~AbstractClass()
{}

void AbstractClass::TemplateMethod()
{
    this-&gt;PrimitiveOperation1();
    this-&gt;PrimitiveOperation2();
}

ConcreteClassA::ConcreteClassA()
{}

ConcreteClassA::~ConcreteClassA()
{}

void ConcreteClassA::PrimitiveOperation1()
{
    cout &lt;&lt; "ConcreteClassA::PrimitiveOperation1" &lt;&lt; endl;
}

void ConcreteClassA::PrimitiveOperation2()
{
    cout &lt;&lt; "ConcreteClassA::PrimitiveOperation2" &lt;&lt; endl;
}

ConcreteClassB::ConcreteClassB()
{}

ConcreteClassB::~ConcreteClassB()
{}

void ConcreteClassB::PrimitiveOperation1()
{
    cout &lt;&lt; "ConcreteClassB::PrimitiveOperation1" &lt;&lt; endl;
}

void ConcreteClassB::PrimitiveOperation2()
{
    cout &lt;&lt; "ConcreteClassB::PrimitiveOperation2" &lt;&lt; endl;
}Main.cpp

#include "TemplateMethod.h"

int main()
{
    //ConcreteClassA与ConcreteClassB可相互替换
    AbstractClass* pAbstract = new ConcreteClassA();
    pAbstract-&gt;TemplateMethod();

    pAbstract = new ConcreteClassB();
    pAbstract-&gt;TemplateMethod();
    
    return 0;
}

Template模式使用场景：

(1).在某些类的算法中，用了相同的方法，造成代码的重复。

(2).控制子类扩展，子类必须遵守算法规则。
Template模式优点:
(1).模板方法模式通过把不变的行为搬移到超类，去除了子类中的重复代码。
(2).子类实现算法的某些细节，有助于算法的扩展。
(3).通过一个父类调用子类实现的操作，通过子类扩展增加新的行为，符合“开放-封闭原则”。
Template模式缺点:
(1).每个不同的实现都需要定义一个子类，这会导致类的个数的增加，设计更加抽象。
Template模式使用总结：   
    Template模式是很简单模式，但是也应用很广的模式。如上面的分析和实现中阐明的Template是采用继承的方式实现算法的异构，其关键点就是将通用算法封装在抽象基类中，并将不同的算法细节放到子类中实现。
    Template模式获得一种反向控制结构效果，这也是面向对象系统的分析和设计中一个原则DIP（依赖倒置：Dependency Inversion Principles）。其含义就是父类调用子类的操作（高层模块调用低层模块的操作），低层模块实现高层模块声明的接口。这样控制权在父类（高层模块），低层模块反而要依赖高层模块。继承的强制性约束关系也让Template模式有不足的地方，我们可以看到对ConcreteClass类中的实现的原语方法Primitive1()，是不能被别的类复用。假设我们要创建一个AbstractClass的变体AnotherAbstractClass，并且两者只是通用算法不一样，其原语操作想复用AbstractClass的子类的实现。但是这是不可能实现的，因为ConcreteClass继承自AbstractClass，也就继承了AbstractClass的通用算法，AnotherAbstractClass是复用不了ConcreteClass的实现，因为后者不是继承自前者。
    Template模式暴露的问题也正是继承所固有的问题，Strategy模式则通过组合（委托）来达到和Template模式类似的效果，其代价就是空间和时间上的代价。


Proxy模式的产生原因：
       在某些情况下，一个对象不想或者不能直接引用另一个对象，而代理对象可以在客户端和目标对象之间起到中介的作用。
       当由于某些特定的需要调用的对象在另外一台机器上，需要跨越网络才能访问，在没有WebService的情况下我们需要直接coding去处理网络连接、处理打包、解包等等非常复杂的步骤，而WebService的出现帮我们解决了其中的一些问题简化客户端的处理，我们只需在客户端建立一个远程对象的代理，客户端就象调用本地对象一样调用该代理，再由代理去跟实际对象联系，对于客户端来说可能根本没有感觉到调用的东西在网络另外一端。
Proxy模式的作用：

       提供一个Subject角色，它定义了Proxy与ConcreteSubject的功用接口，使得任何使用ConcreteSubject的地方都能使用Proxy，Proxy对象维护了一个对ConcreteSubject对象的引用，Proxy接受来至客户端请求，通过这个引用，将请求转给具体的ConcreteSubject对象，Proxy在执行具体对象操作时可以附加其他的操作。这样通过增加代理来解耦客户端与调用对象之间的调用，封装了原来客户端调用具体对象的一些相关细节，客户端不在依赖具体的调用对象，具体对象的修改对客户端来说是透明的，不会影响客户端的修改。

Proxy模式的构成：
        抽象主题（Subject）角色：声明了代理主题和具体主题的公共接口,使任何需要具体主题的地方都能用代理主题代替。 
        代理主题（Proxy）角色：代理对象角色内部含有对具体对象的引用，从而可以操作具体对象，同时代理对象提供与具体对象相同的接口以便在任何时刻都能代替具体对象。同时，代理对象可以在执行具体对象操作时，附加其他的操作，相当于对具体对象进行封装。 
        具体具体对象（ConcreteSubject）角色：代理角色所代表的具体对象，是我们最终要引用的对象。
Proxy模式典型的结构图如图1所示：
               
Proxy模式的示例程序：


Proxy.h

#ifndef _PROXY_H_
#define _PROXY_H_

// 定义了Proxy和ConcreteSubject的公有接口,
// 这样就可以在任何需要使用到ConcreteSubject的地方都使用Proxy.
class Subject
{
public:
    virtual ~Subject();
    virtual void Request()=0;
protected:
    Subject();
};

class ConcreteSubject : public Subject
{
public:
    ConcreteSubject();
    ~ConcreteSubject();
    virtual void Request();
};

//定义代理类
class Proxy : public Subject
{
public:
    Proxy();
    ~Proxy();
    void DoSomething1();
    virtual void Request();
    void DoSomething2();
private:
    Subject* _subject;
};
#endif

Proxy.cpp

#include "Proxy.h"
#include "iostream"

using namespace std;

Subject::Subject()
{}

Subject::~Subject()
{}

ConcreteSubject::ConcreteSubject()
{}

ConcreteSubject::~ConcreteSubject()
{}

void ConcreteSubject::Request()
{
    cout &lt;&lt; "ConcreteSubject::Request" &lt;&lt; endl;
}

Proxy::Proxy() : _subject(NULL)
{}

Proxy::~Proxy()
{}

void Proxy::DoSomething1()
{
    cout &lt;&lt; "Proxy::DoSomething1" &lt;&lt; endl;
}

void Proxy::DoSomething2()
{
    cout &lt;&lt; "Proxy::DoSomething2" &lt;&lt; endl;
}

void Proxy::Request()
{
    if(NULL == this-&gt;_subject)
    {
        this-&gt;_subject = new ConcreteSubject();
    }

    this-&gt;DoSomething1();//表示额外附加的操作

    this-&gt;_subject-&gt;Request();//代理的实体类操作

    this-&gt;DoSomething2();//表示额外附加的操作
}

Main.cpp

#include "Proxy.h"

int main()
{
    Proxy* proxy = new Proxy();
    proxy-&gt;Request();

    return 0;
}



Proxy模式使用场景：
(1).创建开销大的对象时候，比如显示一幅大的图片，我们将这个创建的过程交给代理去完成，GoF称之为虚代理（Virtual Proxy）。
(2).为网络上的对象创建一个局部的本地代理，比如要操作一个网络上的一个对象（网络性能不好的时候，问题尤其突出），我们将这个操纵的过程交给一个代理去完成，GoF称之为远程代理（Remote Proxy）。
(3).对对象进行控制访问的时候，比如在Jive论坛中不同权限的用户（如管理员、普通用户等）将获得不同层次的操作权限，我们将这个工作交给一个代理去完成，GoF称之为保护代理（Protection Proxy）。
(4).当一个对象被引用时，提供一些额外的操作，比如将对此对象调用的次数记录下来等。是指当调用真实的对象时，代理处理另外一些事。如计算真实对象的引用次数，这样当该对象没有引用时，可以自动释放它；或当第一次引用一个持久对象时，将它装入内存；或在访问一个实际对象前，检查是否已经锁定它，以确保其他对象不能改变它。它们都是通过代理在访问一个对象时附加一些内务处理。又称智能引用（Smart Reference）代理。
在所有种类的代理模式中，虚拟（Virtual）代理、远程（Remote）代理、智能引用代理（Smart Reference Proxy）和保护（Protect or Access）代理是最为常见的代理模式。

Proxy代理模式与外观模式的区别： 
(1).外观模式也是屏蔽复杂性的，但是外观模式不会实现客户端调用的目标类型接口。
(2).一般客户端调用外观模式的方法都是直接调用。
(3).代理模式中对客户端目标对象类型抽象接口具体化了。
(4).外观模式是代理模式中一种特殊的子级模式（广泛的，非约束性）。
Proxy代理模式使用总结：
      Proxy代理模式非常常用，大致的思想就是通过为对象加一个代理来降低对象的使用复杂度、或是提升对象使用的友好度、或是提高对象使用的效率。 
      GoF《设计模式》中说的：为其他对象提供一种代理以控制这个对象的访问。理类型从某种角度上讲也可以起到控制被代理类型的访问的作用。




程序员必须进行的12项投资：
        第一项:你的健康
        显而易见，开发人员经常都是坐着不动的。每天坐8到16个小时，只休息很短的时间，你的肚子周围很容易会堆起赘肉。肥胖是其它疾病的催化剂，而其中心脏病又是最可怕的。多花点时间运动一下就能避免这样的情况，如果有可能的话，花点钱办一张健身房的会员卡。
        长时间以不符合人体工程学的姿势打字，也容易使开发人员选成重复性的肌肉损伤，比如说腕管综合征。每隔几个小时拉伸一下你的手腕或者买块鼠标和键盘都能用的手腕护垫可以避免这样的损伤。
        盯着屏幕也会给你的眼睛造成压迫，这就是为什么建议配一副防眩光眼镜而不是普通眼镜的原因（假设你是戴眼镜的）。
 
       第二项：提高你的数学能力
       数学可以提高你的逻辑思考能力，解决问题的方式，在多数情况下，它还能增加你的耐心。有些数学知识是可以直接在软件开发中使用的，比如离散数学，但有些知识忘了也没事，这要看你从事的是那个领域的开发了。比如说，游戏开发人员经常会使用到物理和微积分，而我作为一名企业应用的开发人员，很少发现有能用上它们的地方。不管怎么说，数学能让你变得更加优秀。
                                      
        第三项：提高你的英语水平
        编程、脚本 、标记性语言使用的都是英文，而开源项目比如Linux它们的注解通常也都是英文。来自世界各地的开发人员通过英语来协作开发。面向国际客户的开发人员都必须学习英语，因为这样才能够将业务需求转化成解决方案。
明白了吧？英语对程序员来说，就好比二进制对于计算机一样。
                                                

        第四项：一个自己的域名和网站
        有一个自己专属的邮箱地址而不是什么something@yahoo.com或者someone@gmail.com是不是感觉挺酷？每年只需花费一点点钱，就能让你看起来和别人与众不同。我自己买的这个chinadata.cn域名只花了115RMB而已。个人域名还能给你的个人品牌添加自信和专业，让潜在的用户或者同事了解到，你自己究竟是什么样的。当然了，这假设你的域名不能是什么乱七八糟.com。
                                               
    第五项：一个活跃的Github帐户或者专业技术博客账号，如CSDN博客，CNBLOG博客
            作品集之于艺术家就犹如Github之于开发人员。这点就不必多说了。专业的博客既是学习巩固旧知识的一个非常好的途径，又是快速获取最新知识新技术的权威平台！
                                                             
     第六项：一台好的电脑
                                               
         你听说过有程序员没有自己的电脑的么？我是没听说过。没有自己电脑的开发人员就好比没有激光剑的绝地武士一样。我这里指的好的电脑至少得有4GB内存（要想不那么快过时至少得有8GB）。我比较推荐的是7999RMB的ThinkPad，不过对于那些有特殊需求的极端场景，我持保留意见。                                      
      第七项：足够快的网速
                                            
         互联网就是码农们的氧气。断网对我来说就好比断了空气供给一样，简直无法忍受。有了一个稳定的网络连接，你可以看下视频教程来进行学习，或者泡泡论坛，甚至可以及时关注到Hacker News上的最新文章。
      第八项：读一些经典的计算机科学的书籍
                                              
有一些书我认为是软件开发领域的经典之作：
·        《计算机程序的构造和解释》
·        《代码大全2》
·        《程序员修炼之道》
·        《重构-改善既有代码的设计》
·        《算法导论》
·        《离散数学及其应用》
·        《人月神话》
·        《C++primer》
·        《编译原理》
·        《剑指offer》
·        《C++设计新思维》
·        《STL源码剖析简体中文完整版清晰扫描带目录》
·        《算法珠玑》
       这是我个人整理的部分电子版资料：http://download.csdn.net/detail/fanyun_01/9561220
       第九项：学士学位
                                                         
         一个本科学历能够增加获取工作的几率。不管你是刚毕业的学生还是刚辞职准备找下一份工作，这都是有用的。再想像一下这样的情况：如果很多研究生都挤破了头去抢一份工作，那么一个本科生的简历能被看到的概率又有多少？
        第十项：VPN代理，翻墙浏览器或者拥有一台自己的境外服务器
                                                         
        了解最新全球编程最前言最新技术的窗口，如各开发语言社区动态，人工智能，云计算开源项目等的最新应用，我个人买的是阿里云（华为云和腾讯云，也都不错）的美国西部节点的服务器，也不是很贵，根据个人情况吧！
 
        第十一项：资格认证
                                                          

        资格认证能检验你的某一项技术的能力。通过了某项认证意味着别人已经认可了，你对企业认为可能有价值的某个东西有了比较深入的理解。有些公司会给有相关认证的程序员提供更好的薪水和待遇，但有些公司压根儿就不在乎这些东西。比如说，成为一名认证的Java程序员意味着你对Java语言理解得比较透彻了，但这并不意味着你具备了解决问题的能力。一些公司更看重的是一个人的关键的思考能力而不是他在某项编程语言上的经验，因为编程语言可以很容易学会，而解决问题则不然。比如现在好多公司招项目经理，PMP或IPMP证书就是一项硬性规定，技术牛是一方面，管理也懂的话，哪就最好不过了！
        第十二项：一群高质量的技术好友
        孔圣人2000多年前就说了“有朋自远方来，不亦乐乎？”，在这个高速发展，宅男宅女大幅增加，又容易人情闭塞的社会，我们更应该敞开怀抱，多交几个朋友，不管是生活中，还是工作开发中，好朋友可以让你少走很多弯路。但不是什么人都可以成为你的朋友，擦亮你的眼睛，多交一些真正有益的朋友吧！
        欢迎大家加入我的IT技术交流群，集思广益，共同进步！ 62775887（非IT领域技术人员不要申请，谢谢合作，需要面试验证）！
 

Facade模式产生原因：

        老旧的code(尤其是将C的代码转成C++代码)或者即便不是老旧code，但涉及多个子系统时，除了重写全部代码(对于老旧code而言)，我们还可能采用这样一种策略:重新进行类的设计，将原来分散在源码中的类/结构及方法重新组合，形成新的、统一的接口，供上层应用使用。这在某种意义上与Adapter及Proxy有类似之处，但是，Proxy(代理)注重在为Client-Subject提供一个访问的中间层，如CORBA可为应用程序提供透明访问支持，使应用程序无需去考虑平台及网络造成的差异及其它诸多技术细节;Adapter(适配器)注重对接口的转换与调整;而Facade所面对的往往是多个类或其它程序单元，通过重新组合各类及程序单元，对外提供统一的接口/界面。
Facade模式作用和目的：

        为子系统中的一组接口提供一个一致的界面， 外观模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。将一个系统划分成为若干个子系统有利于降低系统的复杂性。一个常见的设计目标是使子系统间的通信和相互依赖关系达到最小。达到该目标的途径之一是就是引入一个外观（Facade）对象，它为子系统中较一般的设施提供了一个单一而简单的界面。将各个子系统整合起来作为Facade，提供给客户端使用。
Facade模式使用场景：

       （1）.当你要为一个复杂子系统提供一个简单接口时。子系统往往因为不断演化而变得越来越复杂。大多数模式使用时都会产生更多更小的类。这使得子系统更具可重用性，也更容易对子系统进行定制，但这也给那些不需要定制子系统的用户带来一些使用上的困难。
       （2）.客户程序与抽象类的实现部分之间存在着很大的依赖性。引入Facade将这个子系统与客户以及其他的子系统分离，可以提高子系统的独立性和可移植性。
       （3）.当你需要构建一个层次结构的子系统时，使用Facade模式定义子系统中每层的入口点，如果子系统之间是相互依赖的，你可以让它们仅通过Facade进行通讯，从而简化了它们之间的依赖关系。
Facade模式的UML结构图如图1所示：

                                   

                                                                     


Facade模式的构成：

     外观角色（Facade）：是模式的核心，他被客户client角色调用，知道各个子系统的功能。同时根据客户角色已有的需求预订了几种功能组合。
    
子系统角色（Subsystemclasses）：实现子系统的功能，并处理由Facade对象指派的任务。对子系统而言，facade和client角色是未知的，没有Facade的任何相关信息；即没有指向Facade的实例。
     客户角色（client）：调用facade角色获得完成相应的功能。
Facade模式的示例代码如下：
Facade.h

&lt;span style="font-size:14px;"&gt;#ifndef _FACADE_H_
#define _FACADE_H_

class Subsystem1
{
public:
    Subsystem1();
    ~Subsystem1();
    void Operation();
};

class Subsystem2
{
public:
    Subsystem2();
    ~Subsystem2();
    void Operation();
};

class Facade
{
public:
    Facade();
    ~Facade();
    void OperationWrapper();
private:
    Subsystem1* _subsys1;
    Subsystem2* _subsys2;
};

#endif&lt;/span&gt;Facade.cpp


&lt;span style="font-size:14px;"&gt;#include "Facade.h"
#include &lt;iostream&gt;

using namespace std;

Subsystem1::Subsystem1()
{}

Subsystem1::~Subsystem1()
{}

void Subsystem1::Operation()
{
    cout &lt;&lt; "Subsystem1::Operation" &lt;&lt; endl;
}

Subsystem2::Subsystem2()
{}

Subsystem2::~Subsystem2()
{}

void Subsystem2::Operation()
{
    cout &lt;&lt; "Subsystem2::Operation" &lt;&lt; endl;
}

Facade::Facade()
{
    this-&gt;_subsys1 = new Subsystem1();
    this-&gt;_subsys2 = new Subsystem2();
}

Facade::~Facade()
{
    delete this-&gt;_subsys1;
    delete this-&gt;_subsys2;

    this-&gt;_subsys1 = NULL;
    this-&gt;_subsys2 = NULL;
}

void Facade::OperationWrapper()
{
    this-&gt;_subsys1-&gt;Operation();
    this-&gt;_subsys2-&gt;Operation();
}&lt;/span&gt;

main.cpp

&lt;span style="font-size:14px;"&gt;#include "Facade.h"

int main()
{
    Facade* pFacade = new Facade();
    pFacade-&gt;OperationWrapper();
    return 0;
}&lt;/span&gt;Facade模式的优缺点总结：
Facade模式的优点：
1. 松散耦合
外观模式松散了客户端与子系统的耦合关系，让子系统内部的模块能更容易扩展和维护。即要点2.
2. 简单易用
外观模式让子系统更加易用，客户端不再需要了解子系统内部的实现，也不需要跟众多子系统内部的模块进行交互，只需要跟外观交互就可以了，相当于外观类为外部客户端使用子系统提供了一站式服务。
3. 更好的划分访问层次
通过合理使用Facade，可以帮助我们更好的划分访问的层次。有些方法是对系统外的，有些方法是系统内部使用的。把需要暴露给外部的功能集中到外观中，这样既方便客户端使用，也很好的隐藏了内部的细节。
Facade模式的缺点：
1.过多的或者是不太合理的Facade也容易让人迷惑，到底是调用Facade好呢，还是直接调用模块好。







Flyweight模式产生原因：
        在面向对象系统的设计何实现中，创建对象是最为常见的操作。这里面就有一个问题：如果一个应用程序使用了太多的对象，就会造成很大的存储开销。特别是对于大量轻量级（细粒度）的对象，比如在文档编辑器的设计过程中，我们如果为没有字母创建一个对象的话，系统可能会因为大量的对象而造成存储开销的浪费。例如一个字母“a”在文档中出现了100000次，而实际上我们可以让这一万个字母“a”共享一个对象，当然因为在不同的位置可能字母“a”有不同的显示效果（例如字体和大小等设置不同），在这种情况我们可以为将对象的状态分为“外部状态”和“内部状态”，将可以被共享（不会变化）的状态作为内部状态存储在对象中，而外部对象（例如上面提到的字体、大小等）我们可以在适当的时候将外部对象最为参数传递给对象（例如在显示的时候，将字体、大小等信息传递给对象）。
Flyweight享元模式作用：

      Flyweight模式是一个提高程序效率和性能的模式，会大大加快程序的运行速度。应用场合很多:比如你要从一个数据库中读取一系列字符串，这些字符串中有许多是重复的，那么我们可以将这些字符串储存在Flyweight池(pool)中。就是先创建一个的原始模型，然后随着不同场合和环境，再产生各具特征的具体模型，很显然，在这里需要产生不同的新对象，所以Flyweight模式中常出现Factory模式。Flyweight的内部状态是用来共享的，Flyweight
 factory负责维护一个Flyweightpool(模式池)来存放内部状态的对象。
Flyweight享元模式的使用场景：

        当以下所有的条件都满足时，可以考虑使用享元模式：
（1）.一个系统有大量的对象。
（2）.这些对象耗费大量的内存。
（3）.这些对象的状态中的大部分都可以外部化。
（4）.这些对象可以按照内蕴状态分成很多的组，当把外蕴对象从对象中剔除时，每一个组都可以仅用一个对象代替。
（5）.软件系统不依赖于这些对象的身份，换言之，这些对象可以是不可分辨的。
       满足以上的这些条件的系统可以使用享元对象。
       最后，使用享元模式需要维护一个记录了系统已有的所有享元的表，而这需要耗费资源。因此，应当在有足够多的享元实例可供共享时才值得使用享元模式。
单纯Flyweight享元模式模式典型的UML结构图如图1所示：

                
单纯Flyweight享元模式抽象基类及接口：

        抽象享元(Flyweight)角色：此角色是所有的具体享元类的超类，为这些类规定出需要实现的公共接口。那些需要外蕴状态(External State)的操作可以通过调用商业方法以参数形式传入。
       具体享元(ConcreteFlyweight)角色：实现抽象享元角色所规定的接口。如果有内蕴状态的话，必须负责为内蕴状态提供存储空间。享元对象的内蕴状态必须与对象所处的周围环境无关，从而使得享元对象可以在系统内共享的。
       享元工厂(FlyweightFactory)角色：本角色负责创建和管理享元角色。本角色必须保证享元对象可以被系统适当地共享。当一个客户端对象调用一个享元对象的时候，享元工厂角色会检查系统中是否已经有一个复合要求的享元对象。如果已经有了，享元工厂角色就应当提供这个已有的享元对象；如果系统中没有一个适当的享元对象的话，享元工厂角色就应 当创建一个合适的享元对象。
       客户端(Client)角色：本角色需要维护一个对所有享元对象的引用。本角色需要自行存储所有享元对象的外蕴状态。
单纯Flyweight享元模式典型的示例代码如下：

&lt;span style="font-size:14px;"&gt;using System;
using System.Collections;

// 享元工厂类
class FlyweightFactory
{
	// 域
	private Hashtable flyweights = new Hashtable();

	//构造函数
	public FlyweightFactory()
	{
		flyweights.Add("X", new ConcreteFlyweight());
		flyweights.Add("Y", new ConcreteFlyweight());
		flyweights.Add("Z", new ConcreteFlyweight());
	}

	// 方法
	public Flyweight GetFlyweight(string key)
	{
		return((Flyweight)flyweights[key]);
	}
}
// 轻量级选手
abstract class Flyweight
{
	// 方法
	abstract public void Operation(int extrinsicstate);
}
// 具体享元类
class ConcreteFlyweight : Flyweight
{
	private string intrinsicstate = "A";
	// 方法
	override public void Operation(int extrinsicstate)
	{
		Console.WriteLine("ConcreteFlyweight: intrinsicstate {0}, extrinsicstate {1}",
			intrinsicstate, extrinsicstate);
	}
}

public class Client
{
	public static void Main(string[] args)
	{
		// 任意状态
		int extrinsicstate = 22;

		FlyweightFactory f = new FlyweightFactory();

		//使用不同的轻量级选手实例
		Flyweight fx = f.GetFlyweight("X");
		fx.Operation(--extrinsicstate);

		Flyweight fy = f.GetFlyweight("Y");
		fy.Operation(--extrinsicstate);

		Flyweight fz = f.GetFlyweight("Z");
		fz.Operation(--extrinsicstate);
	}
}
&lt;/span&gt;复合Flyweight享元模式典型的UML结构图如图2所示：
             


复合Flyweight享元模式抽象基类及接口：

       抽象享元角色：此角色是所有的具体享元类的超类，为这些类规定出需要实现的公共接口。那些需要外蕴状态(External State)的操作可以通过方法的参数传入。抽象享元的接口使得享元变得可能，但是并不强制子类实行共享，因此并非所有的享元对象都是可以共享的。
       具体享元(ConcreteFlyweight)角色：实现抽象享元角色所规定的接口。如果有内蕴状态的话，必须负责为内蕴状态提供存储空间。享元对象的内蕴状态必须与对象所处的周围环境无关，从而使得享元对象可以在系统内共享。有时候具体享元角色又叫做单纯具体享元角色，因为复合享元角色是由单纯具体享元角色通过复合而成的。
       复合享元(UnsharableFlyweight)角色：复合享元角色所代表的对象是不可以共享的，但是一个复合享元对象可以分解成为多个本身是单纯享元对象的组合。复合享元角色又称做不可共享的享元对象。
        享元工厂(FlyweightFactoiy)角色：本角色负责创建和管理享元角色。本角色必须保证享元对象可以被系统适当地共享。当一个客户端对象请求一个享元对象的时候，享元工厂角色需要检查系统中是否已经有一个符合要求的享元对象，如果已经有了，享元工厂角色就应当提供这个已有的享元对象；如果系统中没有一个适当的享元对象的话，享元工厂角色就应当创建一个新的合适的享元对象。
客户端(Client)角色：本角色还需要自行存储所有享元对象的外蕴状态。
示例1：一个咖啡的例子

       在这个咖啡摊(CoffeeStall)所使用的系统里，有一系列的咖啡"风味(Flavor)"。客人到摊位上购买咖啡，所有的咖啡均放在台子上，客人自己拿到咖啡后就离开摊位。咖啡有内蕴状态，也就是咖啡的风味；咖啡没有环境因素，也就是说没有外蕴状态。如果系统为每一杯咖啡都创建一个独立的对象的话，那么就需要创建出很多的细小对象来。这样就不如把咖啡按照种类(即"风味")划分，每一种风味的咖啡只创建一个对象，并实行共享。
       使用咖啡摊主的语言来讲，所有的咖啡都可按"风味"划分成如Capucino、Espresso等，每一种风味的咖啡不论卖出多少杯，都是全同、不可分辨的。所谓共享，就是咖啡风味的共享，制造方法的共享等。因此，享元模式对咖啡摊来说，就意味着不需要为每一份单独调制。摊主可以在需要时，一次性地调制出足够一天出售的某一种风味的咖啡。
      很显然，这里适合使用单纯享元模式。系统的设计如下：  

&lt;span style="font-size:14px;"&gt;using System;
using System.Collections;

public abstract class Order
{
  // 将咖啡卖给客人
  public abstract void Serve();
  // 返回咖啡的名字
  public abstract string GetFlavor();
}

public class Flavor : Order
{
  private string flavor;

  // 构造函数，内蕴状态以参数方式传入
  public Flavor(string flavor)
  {
    this.flavor = flavor;
  }

  // 返回咖啡的名字
  public override string GetFlavor()
  {
    return this.flavor;
  }

  // 将咖啡卖给客人
  public override void Serve()
  {
    Console.WriteLine("Serving flavor " + flavor);
  }
}

public class FlavorFactory
{
  private Hashtable flavors = new Hashtable();

  public Order GetOrder(string key)
  {
    if(! flavors.ContainsKey(key))
      flavors.Add(key, new Flavor(key));

        return ((Order)flavors[key]);
  }

  public int GetTotalFlavorsMade()
  {
    return flavors.Count;
  }
}

public class Client
{
  private static FlavorFactory flavorFactory;
  private static int ordersMade = 0;

  public static void Main( string[] args )
  {
    flavorFactory = new FlavorFactory();

    TakeOrder("Black Coffee");
    TakeOrder("Capucino");
    TakeOrder("Espresso");
    TakeOrder("Capucino");
    TakeOrder("Espresso");
    TakeOrder("Black Coffee");
    TakeOrder("Espresso");
    TakeOrder("Espresso");
    TakeOrder("Black Coffee");
    TakeOrder("Capucino");
    TakeOrder("Capucino");
    TakeOrder("Black Coffee");

    Console.WriteLine("\nTotal Orders made: " + ordersMade);

    Console.WriteLine("\nTotal Flavor objects made: " + 
      flavorFactory.GetTotalFlavorsMade());
  }

  private static void TakeOrder(string aFlavor)
  {
    Order o = flavorFactory.GetOrder(aFlavor);
    // 将咖啡卖给客人
    o.Serve();

    ordersMade++;
  }
}&lt;/span&gt;

示例2：咖啡店的例子

      在前面的咖啡摊项目里，由于没有供客人坐的桌子，所有的咖啡均没有环境的影响。换言之，咖啡仅有内蕴状态，也就是咖啡的种类，而没有外蕴状态。下面考虑一个规模稍稍大一点的咖啡屋(Coffee Shop)项目。屋子里有很多的桌子供客人坐，系统除了需要提供咖啡的"风味"之外，还需要跟踪咖啡被送到哪一个桌位上，因此，咖啡就有了桌子作为外蕴状态。
       由于外蕴状态的存在，没有外蕴状态的单纯享元模式不再符合要求。系统的设计可以利用有外蕴状态的单纯享元模式。系统的代码如下：
&lt;span style="font-size:14px;"&gt;using System;
using System.Collections;

public abstract class Order
{
  // 将咖啡卖给客人
  public abstract void Serve(Table table);
  // 返回咖啡的名字
  public abstract string GetFlavor();
}

public class Flavor : Order
{
  private string flavor;

  // 构造函数，内蕴状态以参数方式传入
  public Flavor(string flavor)
  {
    this.flavor = flavor;
  }

  // 返回咖啡的名字
  public override string GetFlavor()
  {
    return this.flavor;
  }

  // 将咖啡卖给客人
  public override void Serve(Table table)
  {
    Console.WriteLine("Serving table {0} with flavor {1}", table.Number, flavor);
  }
}

public class FlavorFactory
{
  private Hashtable flavors = new Hashtable();

  public Order GetOrder(string key)
  {
    if(! flavors.ContainsKey(key))
      flavors.Add(key, new Flavor(key));

        return ((Order)flavors[key]);
  }

  public int GetTotalFlavorsMade()
  {
    return flavors.Count;
  }
}

public class Table
{
  private int number;

  public Table(int number)
  {
    this.number = number;
  }

  public int Number
  {
    get { return number; }
  }
}

public class Client
{
  private static FlavorFactory flavorFactory;
  private static int ordersMade = 0;

  public static void Main( string[] args )
  {
    flavorFactory = new FlavorFactory();

    TakeOrder("Black Coffee");
    TakeOrder("Capucino");
    TakeOrder("Espresso");
    TakeOrder("Capucino");
    TakeOrder("Espresso");
    TakeOrder("Black Coffee");
    TakeOrder("Espresso");
    TakeOrder("Espresso");
    TakeOrder("Black Coffee");
    TakeOrder("Capucino");
    TakeOrder("Capucino");
    TakeOrder("Black Coffee");

    Console.WriteLine("\nTotal Orders made: " + ordersMade);

    Console.WriteLine("\nTotal Flavor objects made: " + 
      flavorFactory.GetTotalFlavorsMade());
  }

  private static void TakeOrder(string aFlavor)
  {
    Order o = flavorFactory.GetOrder(aFlavor);
    
    // 将咖啡卖给客人
    o.Serve(new Table(++ordersMade));
  }
}&lt;/span&gt;
Flyweight享元模式使用总结：


（1）.Flyweight享元模式的优点在于它大幅度地降低内存中对象的数量。但是，它做到这一点所付出的代价也是很高的：

（2）.Flyweight享元模式使得系统更加复杂。为了使对象可以共享，需要将一些状态外部化，这使得程序的逻辑复杂化。

（3）.Flyweight享元模式将享元对象的状态外部化，而读取外部状态使得运行时间稍微变长。




Composite模式产生原因：
        在开发中，我们经常可能要递归构建树状的组合结构，Composite模式则提供了很好的解决方案。
Composite模式作用：

        将对象组合成树形结构以表示“部分-整体”的层次结构。Composite使得用户对单个对象和组合对象的使用具有一致性。
Composite模式典型的UML结构图如图1所示：

           

        在Component中声明所有用来管理子对象的方法，其中包括Add、Remove等，这样实现Component接口的所有子类都具备了Add和Remove。这样做的好处就是叶节点和枝节点对于外界没有区别，它们具备完全一致的行为 接口。但问题也很明显，因为Leaf类本身不具备Add()、Remove()方法的功能，所以实现它是没有意义的。
Composite模式适用场景：
当你发现需求中是体现部分与整体层次的结构时，以及你希望用户可以忽略组合对象与单个对象的不同，统一地使用组合结构中的所有对象时，就应该考虑用组合模式了。
基本对象可以被组合成更复杂的组合对象，而这个组合对象又可以被组合，这样不断地递归下去，客户代码中，任何用到基本对象的地方都可以使用组合对象了。
用户不用关心到底是处理一个叶节点还是处理一个组合组件，也就用不着为定义组合二写一些选择判断语句了。
组合模式让客户可以一致地使用组合结构和单个对象。
Composite模式构成:
1)Component:为组合中的对象声明接口，声明了类共有接口的缺省行为(如这里的Add,Remove,GetChild函数)，声明一个接口函数可以访问Component的子组件.
接口函数:
1)Component::Operatation:定义了各个组件共有的行为接口，由各个组件的具体实现.
2)Component::Add添加一个子组件
3)Component::Remove::删除一个子组件.
4)Component::GetChild:获得子组件的指针.
说明:
Component模式是为解决组件之间的递归组合提供了解决的办法，它主要分为两个派生类：
1)、Leaf是叶子结点,也就是不含有子组件的结点
2)、Composite是含有子组件的类.
       举一个例子来说明这个模式，在UI的设计中,最基本的控件是诸如Button、Edit这样的控件，相当于是这里的Leaf组件，而比较复杂的控件比如Panel则可也看做是由这些基本的组件组合起来的控件，相当于这里的Composite，它们之间有一些行为含义是相同的，比如在控件上作一个点击,移动操作等等的，这些都可以定义为抽象基类中的接口虚函数，由各个派生类去实现之，这些都会有的行为就是这里的Operation函数，而添加、删除等进行组件组合的操作只有非叶子结点才可能有，所以虚拟基类中只是提供接口而且默认的实现是什么都不做。
Composite模式典型的示例代码如下：


Composite.h

&lt;span style="font-size:14px;"&gt;#ifndef _COMPOSITE_H_
#define _COMPOSITE_H_

#include &lt;vector&gt;

using namespace std;

/*
Component抽象基类，为组合中的对象声明接口,声明了类共有接口的缺省行为(如这里的Add,Remove,GetChild函数),
声明一个接口函数可以访问Component的子组件.
*/
class Component
{
public:
    //纯虚函数，只提供接口，没有默认的实现
    virtual void Operation()=0;    

    // 虚函数,提供接口,有默认的实现就是什么都不做
    virtual void Add(Component*);
    virtual void Remove(Component*);
    virtual Component* GetChild(int index);
    virtual ~Component();
protected:
    Component();
};

//Leaf是叶子结点,也就是不含有子组件的结点类，所以不用实现Add、Remove、GetChild等方法
class Leaf:public Component
{
public:
    //只实现Operation接口
    virtual void Operation();            
    Leaf();
    ~Leaf();
};

//Composite：含有子组件的类
class Composite:public Component
{
public:
    Composite();
    ~Composite();
    //实现所有接口
    void Operation();
    void Add(Component*);
    void Remove(Component*);
    Component* GetChild(int index);
private:
    //这里采用vector来保存子组件
    vector&lt;Component*&gt; m_ComVec;        
};
#endif&lt;/span&gt;

Compostie.cpp

&lt;span style="font-size:14px;"&gt;#include "Composite.h"
#include &lt;iostream&gt;

using namespace std;

Component::Component()
{}

Component::~Component()
{}

void Component::Add(Component* com)
{
    cout &lt;&lt; "add" &lt;&lt; endl;
}

void Component::Remove(Component* com)
{
}

void Component::Operation()
{
    cout &lt;&lt; "Component::Operation" &lt;&lt; endl;
}

Component* Component::GetChild(int index)
{
    return NULL;
}


Leaf::Leaf()
{}

Leaf::~Leaf()
{}

void Leaf::Operation()
{
    cout&lt;&lt; "Leaf::Operation" &lt;&lt;endl;
}

Composite::Composite()
{
}

Composite::~Composite()
{}

void Composite::Add(Component* com)
{
    this-&gt;m_ComVec.push_back(com);
}

void Composite::Remove(Component* com)
{
    this-&gt;m_ComVec.erase(&amp;com);
}

void Composite::Operation()
{
    cout &lt;&lt; "Composite::Operation" &lt;&lt; endl;
    vector&lt;Component*&gt;::iterator iter = this-&gt;m_ComVec.begin();
    for(;iter!= this-&gt;m_ComVec.end();iter++)
    {
        (*iter)-&gt;Operation();
    }
}

Component* Composite::GetChild(int index)
{
    if(index &lt; 0 || index &gt; this-&gt;m_ComVec.size())
    {
        return NULL;
    }
    return this-&gt;m_ComVec[index];
}&lt;/span&gt;

main.cpp
&lt;span style="font-size:14px;"&gt;#include "Composite.h"
#include &lt;iostream&gt;

using namespace std;

int main()
{
    /*
      不管是叶子Leaf还是Composite对象pRoot、pCom都实现了Operation接口，所以可以一致对待，直接调用Operation()
      体现了“使得用户对单个对象和组合对象的使用具有一致性。”
    */
    Composite* pRoot = new Composite();

    //组合对象添加叶子节点
    pRoot-&gt;Add(new Leaf());

    Leaf* pLeaf1 = new Leaf();
    Leaf* pLeaf2 = new Leaf();

    //这里的叶子再添加叶子是没有意义的。
    //由于叶子与组合对象继承了相同的接口，所以语法上是对的，实际上什么也没做(继承自基类Component的Add方法)。
    //叶子节点只实现了Operation方法，其他Add、Remove、GetChild都继承自基类，没有实际意义。
    pLeaf1-&gt;Add(pLeaf2);
    pLeaf1-&gt;Remove(pLeaf2);
    //执行叶子Operation操作
    pLeaf1-&gt;Operation();

    //组合对象实现了基类Component的所有接口，所以可以做各种操作(Add、Remove、GetChild、Operation)。
    Composite* pCom = new Composite();
    //组合对象添加叶子节点
    pCom-&gt;Add(pLeaf1);
    //组合对象添加叶子节点
    pCom-&gt;Add(pLeaf2);
    //执行组合对象Operation操作
    pCom-&gt;Operation();

    //组合对象添加组合对象
    pRoot-&gt;Add(pCom);

    //执行组合对象Operation操作
    pRoot-&gt;Operation();

    //Component* cp = pCom-&gt;GetChild(0);
    //cp-&gt;Operation();

    //pCom-&gt;Remove(pLeaf1);

    return 0;
}&lt;/span&gt;代码说明：
     Composite模式在实现中有一个问题就是要提供对于子节点（Leaf）的管理策略，这里使用的是STL 中的vector，可以提供其他的实现方式，如数组、链表、Hash表等。
     Composite模式通过和Decorator模式有着类似的结构图，但是Composite模式旨在构造类，而Decorator模式重在不生成子类即可给对象添加职责。Decorator模式重在修饰，而Composite模式重在表示。

Decorator装饰模式产生原因：        
        在OO设计和开发过程，可能会经常遇到以下的情况：我们需要为一个已经定义好的类添加新的职责（操作），通常的情况我们会给定义一个新类继承自定义好的类，这样会带来一个问题（将在本模式的讨论中给出）。通过继承的方式解决这样的情况还带来了系统的复杂性，因为继承的深度会变得很深。
Decorator装饰模式作用：

        Decorator提供了一种给类增加职责的方法，不是通过继承实现的，而是通过组合。这样就大大降低了基类的复杂性！
Decorator装饰模式典型的UML结构图如图1所示：

                      
         在结构图中，ConcreteComponent和Decorator需要有同样的接口，因此ConcreteComponent和Decorator有着一个共同的父类。这里有人会问，让Decorator直接维护一个指向ConcreteComponent引用（指针）不就可以达到同样的效果，答案是肯定并且是否定的。肯定的是你可以通过这种方式实现，否定的是你不要用这种方式实现，因为通过这种方式你就只能为这个特定的ConcreteComponent提供修饰操作了，当有了一个新的ConcreteComponent你又要去新建一个Decorator来实现。但是通过结构图中的ConcreteComponent和Decorator有一个公共基类，就可以利用OO中多态的思想来实现只要是Component型别的对象都可以提供修饰操作的类，这种情况下你就算新建了100个Component型别的类ConcreteComponent，也都可以由Decorator一个类搞定。这也正是Decorator模式的关键和威力所在了。
        当然如果你只用给Component型别类添加一种修饰，则Decorator这个基类就不是很必要了。
Decorator装饰模式典型的示例代码如下：


Decorator.h

#ifndef _DECORATOR_H_
#define _DECORATOR_H_

//Component抽象类，定义该类对象的接口
class Component
{
public:
    virtual ~Component();
    virtual void Operation()=0;
protected:
    Component();
};

//ConcreteDecorator：具体的Component对象，可以给该对象动态 添加职责
class ConcreteComponent:public Component
{
public:
    ConcreteComponent();
    ~ConcreteComponent();
    virtual void Operation();
};

//Decorator：装饰抽象类，继承自Component
class Decorator:public Component
{
public:
    Decorator(Component* com);
    void SetComponent(Component* com);
    virtual ~Decorator();
    virtual void Operation();
protected:
    Component* _com;
};

//ConcreteDecorator就是具体的装饰对象之一，起到给Component添加职责的功能
class ConcreteDecoratorA:public Decorator
{
public:
    ConcreteDecoratorA(Component* com);
    ~ConcreteDecoratorA();
    virtual void Operation();
    void AddBehavorA();
};

//ConcreteDecorator就是具体的装饰对象之二，起到给Component添加职责的功能
class ConcreteDecoratorB:public Decorator
{
public:
    ConcreteDecoratorB(Component* com);
    ~ConcreteDecoratorB();
    virtual void Operation();
    void AddBehavorB();
};

//ConcreteDecorator就是具体的装饰对象之三，起到给Component添加职责的功能
class ConcreteDecoratorC:public Decorator
{
public:
    ConcreteDecoratorC(Component* com);
    ~ConcreteDecoratorC();
    virtual void Operation();
    void AddBehavorC();
};

//只添加一种装饰，则不用抽象出装饰基类
class DecoratorOnlyOne:public Component
{
public:
    DecoratorOnlyOne(Component* com);
    ~DecoratorOnlyOne();
    virtual void Operation();
    void AddBehavor();
private:
    Component* _com;
};

//如果只有一个ConcreteComponent类而没有抽象的Component类，那么Decorator类可以是ConcreteComponent的一个子类。
//略
#endif

Decorator.cpp

#include "Decorator.h"
#include &lt;iostream&gt;

using namespace std;

Component::Component()
{}

Component::~Component()
{
    cout &lt;&lt; "~Component" &lt;&lt; endl;
}

ConcreteComponent::ConcreteComponent()
{}

ConcreteComponent::~ConcreteComponent()
{
    cout &lt;&lt; "~ConcreteComponent" &lt;&lt; endl;
}

void ConcreteComponent::Operation()
{
    cout &lt;&lt; "原职责：ConcreteComponent::Operation" &lt;&lt; endl;
}

Decorator::Decorator(Component* com)
{
    this-&gt;_com = com;
}

void Decorator::SetComponent(Component* com)
{
    this-&gt;_com = com;
}

Decorator::~Decorator()
{
    cout &lt;&lt; "~Decorator" &lt;&lt; endl;
    delete this-&gt;_com;
    this-&gt;_com = NULL;
}

void Decorator::Operation()
{}

ConcreteDecoratorA::ConcreteDecoratorA(Component* com):Decorator(com)
{}

ConcreteDecoratorA::~ConcreteDecoratorA()
{
    cout &lt;&lt; "~ConcreteDecoratorA" &lt;&lt; endl;
}

void ConcreteDecoratorA::Operation()
{
    this-&gt;_com-&gt;Operation();
    //附加职责A
    this-&gt;AddBehavorA();
}

void ConcreteDecoratorA::AddBehavorA()
{
    cout &lt;&lt; "附加职责A：ConcreteDecoratorA::AddBehavorA" &lt;&lt; endl;
}

ConcreteDecoratorB::ConcreteDecoratorB(Component* com):Decorator(com)
{}

ConcreteDecoratorB::~ConcreteDecoratorB()
{
    cout &lt;&lt; "~ConcreteDecoratorB" &lt;&lt; endl;
}

void ConcreteDecoratorB::Operation()
{
    this-&gt;_com-&gt;Operation();
    //附加职责B
    this-&gt;AddBehavorB();
}

void ConcreteDecoratorB::AddBehavorB()
{
    cout &lt;&lt; "附加职责B：ConcreteDecoratorB::AddBehavorB" &lt;&lt; endl;
}

ConcreteDecoratorC::ConcreteDecoratorC(Component* com):Decorator(com)
{}

ConcreteDecoratorC::~ConcreteDecoratorC()
{
    cout &lt;&lt; "~ConcreteDecoratorC" &lt;&lt; endl;
}

void ConcreteDecoratorC::Operation()
{
    this-&gt;_com-&gt;Operation();
    //附加职责C
    this-&gt;AddBehavorC();
}

void ConcreteDecoratorC::AddBehavorC()
{
    cout &lt;&lt; "附加职责C：ConcreteDecoratorC::AddBehavorC" &lt;&lt; endl;
}


//**************只添加一种修饰******************
DecoratorOnlyOne::DecoratorOnlyOne(Component* com):_com(com)
{
}

DecoratorOnlyOne::~DecoratorOnlyOne()
{
    cout &lt;&lt; "~DecoratorOnlyOne" &lt;&lt; endl;
    delete this-&gt;_com;
    this-&gt;_com = NULL;
}

void DecoratorOnlyOne::Operation()
{
    this-&gt;_com-&gt;Operation();
    this-&gt;AddBehavor();
}

void DecoratorOnlyOne::AddBehavor()
{
    cout &lt;&lt; "附加唯一职责：DecoratorOnlyOne::AddBehavor" &lt;&lt; endl;
}

main.cpp
#include "Decorator.h"
#include &lt;iostream&gt;

using namespace std;
int main()
{
    Component* pCom = new ConcreteComponent();        //要装饰的对象
    Decorator* pDec = NULL;
    pDec = new ConcreteDecoratorA(pCom);            //给装饰对象附加职责A
    pDec = new ConcreteDecoratorB(pDec);            //给装饰对象附加职责B
    pDec = new ConcreteDecoratorC(pDec);            //给装饰对象附加职责C
    pDec-&gt;Operation();

    cout &lt;&lt; "-------------------------------" &lt;&lt; endl;

    //只添加一种修饰
    Component* pCom1 = new ConcreteComponent();
    DecoratorOnlyOne* pDec1 = new DecoratorOnlyOne(pCom1);
    pDec1-&gt;Operation();

    cout &lt;&lt; "-------------------------------" &lt;&lt; endl;

    delete pDec;
    cout &lt;&lt; "-------------------------------" &lt;&lt; endl;

    delete pDec1;

    return 0;
}
Decorator模式使用总结：
       Decorator模式和Proxy模式的相似的地方在于它们都拥有一个指向其他对象的引用（指针），即通过组合的方式来为对象提供更多操作（或者Decorator模式）间接性（Proxy模式）。但是他们的区别是，Proxy模式会提供使用其作为代理的对象一样接口，使用代理类将其操作都委托给Proxy直接进行。这里可以简单理解为组合和委托之间的微妙的区别了。
       Decorator模式除了采用组合的方式取得了比采用继承方式更好的效果，Decorator模式还给设计带来一种“即用即付”的方式来添加职责。在OO设计和分析经常有这样一种情况：为了多态，通过父类指针指向其具体子类，但是这就带来另外一个问题，当具体子类要添加新的职责，就必须向其父类添加一个这个职责的抽象接口，否则是通过父类指针是调用不到这个方法了。这样处于高层的父类就承载了太多的特征（方法），并且继承自这个父类的所有子类都不可避免继承了父类的这些接口，但是可能这并不是这个具体子类所需要的。而在Decorator模式提供了一种较好的解决方法，当需要添加一个操作的时候就可以通过Decorator模式来解决，你可以一步步添加新的职责。



1、errorC2859 vc90.idb is not the idb file that was used when this precompiled headerwas created。
     解决方法：选中工程--&gt;点击右键属性--&gt;ConfigurationProperties--&gt;C/C++--&gt;Output
 Files--&gt;Program Database File Name中的$(IntDir)/vc90.pdb改为$(IntDir)/(工程名).pdb即可。
2、Failedto return new code element.
    解决方法：关闭工程，删除工程中的.ncb文件，再重新打开工程即可。
3、makesure that the file is not open by another process and is not write-protected
    解决方法：关闭工程，再重新打开即可。
    网上有的说：Tools--&gt;Options--&gt;Projectsand Solutions--&gt;Builde and Run--&gt;将maximum
 number of parallelproject builds该为1即可，可是试试了并不起作用。
4、add/removeoperation is impossible,because the code element 'Cxxx' is read only
     解决方法：关闭工程，删除工程中的.ncb和.suo两个文件，再重新打开工程即可。
5、errorC2471 cannot update program database …..debug\vc90.pdb
   解决方法：在属性配置里(1)、C\C++--&gt;General--&gt;Debug
 Information format：改为C7 Compatible (/Z7)；(2)、C\C++--&gt;Code
 Generation--&gt;Enable String Pooling：改为Yes (/GF)；(3)、Linker--&gt;Debuging--&gt;GeneralDebug
 Info：改为Yes(/DEBUG)即可。
6、Errorspawning 'cmd.exe'
  解决方法：Tools--&gt;Options--&gt;Projectsand Solutions--&gt;VC++ Directories：点击New
 Line(类似文件夹图标)添加一行(cmd.exe所在目录)：C:\WINDOWS\system32\，OK即可。
7、断点调试时，出现thereis no source code available for the current location
   解决方法：Tools--&gt;Options--&gt;Debugging--&gt;General--&gt;把Requiresource
 files to exactly match the original version前面的勾去掉即可。
 



Bridge模式的产生原因：
         总结面向对象实际上就两句话：一是松耦合（Coupling），二是高内聚（Cohesion）。面向对象系统追求的目标就是尽可能地提高系统模块内部的内聚（Cohesion）、尽可能降低模块间的耦合（Coupling）。然而这也是面向对象设计过程中最为难把握的部分，大家肯定在OO系统的开发过程中遇到这样的问题：
（1）.客户给了你一个需求，于是使用一个类来实现（A）； 
（2）.客户需求变化，有两个算法实现功能，于是改变设计，我们通过一个抽象的基类，再定义两个具体类实现两个不同的算法（A1和A2）； 
（3）.客户又告诉我们说对于不同的操作系统，于是再抽象一个层次，作为一个抽象基类A0，在分别为每个操作系统派生具体类（A00和A01，其中A00表示原来的类A）实现不同操作系统上的客户需求，这样我们就有了一共4个类。
（4）.可能用户的需求又有变化，比如说又有了一种新的算法…….. 
（5）.我们陷入了一个需求变化的郁闷当中，也因此带来了类的迅速膨胀。
Bridge模式则正是解决了这类问题。
Bridge模式的作用：
        作用：将抽象部份与它的实现部份分离，使它们都可以独立地变化。将抽象(Abstraction)与实现(Implementation)分离，使得二者可以独立地变化。桥接模式号称设计模式中最难理解的模式之一，关键就是这个抽象和实现的分离非常让人奇怪，大部分人刚看到这个定义的时候都会认为实现就是继承自抽象，那怎么可能将他们分离呢。
       《大话设计模式》中就Bridge模式的解释：
        手机品牌和软件是两个概念，不同的软件可以在不同的手机上，不同的手机可以有相同的软件，两者都具有很大的变动性。如果我们单独以手机品牌或手机软件为基类来进行继承扩展的话，无疑会使类的数目剧增并且耦合性很高，将两者抽象出来两个基类分别是PhoneBrand和PhoneSoft，那么在品牌类中聚合一个软件对象的基类将解决软件和手机扩展混乱的问题，这样两者的扩展就相对灵活，剪短了两者的必要联系，结构图如下：
                                 
Bridge模式的UML结构图如图1所示：
                

Bridge模式构成：
1、Abstraction::Operation()：定义要实现的操作接口
2、AbstractionImplement::Operation()：实现抽象类Abstaction所定义操作的接口，由其具体派生类ConcreteImplemenA、ConcreteImplemenA或者其他派生类实现。
3、在Abstraction::Operation()中根据不同的指针多态调用AbstractionImplement::Operation()函数。
理解:
Bridge用于将表示和实现解耦,两者可以独立的变化.在Abstraction类中维护一个AbstractionImplement类指针,需要采用不同的实现方式的时候只需要传入不同的AbstractionImplement派生类就可以了.
Bridge的实现方式其实和Builde十分的相近,可以这么说:本质上是一样的,只是封装的东西不一样罢了.两者的实现都有如下的共同点:
抽象出来一个基类,这个基类里面定义了共有的一些行为,形成接口函数(对接口编程而不是对实现编程),这个接口函数在Buildier中是BuildePart函数在Bridge中是Operation函数;
其次,聚合一个基类的指针,如Builder模式中Director类聚合了一个Builder基类的指针,而Brige模式中Abstraction类聚合了一个AbstractionImplement基类的指针(优先采用聚合而不是继承);
而在使用的时候,都把对这个类的使用封装在一个函数中,在Bridge中是封装在Director::Construct函数中,因为装配不同部分的过程是一致的,而在Bridge模式中则是封装在Abstraction::Operation函数中,在这个函数中调用对应的AbstractionImplement::Operation函数.就两个模式而言,Builder封装了不同的生成组成部分的方式,而Bridge封装了不同的实现方式.
桥接模式就将实现与抽象分离开来，使得RefinedAbstraction依赖于抽象的实现，这样实现了依赖倒转原则，而不管左边的抽象如何变化，只要实现方法不变，右边的具体实现就不需要修改，而右边的具体实现方法发生变化，只要接口不变，左边的抽象也不需要修改。
Bridge模式典型的示例代码如下：

#include &lt;iostream&gt;
#include &lt;string&gt;
using namespace std;

class HandsetSoft
{
public:
	virtual void run()=0;
};

class HandsetGame:public HandsetSoft
{
public:
	void run()
	{
		cout&lt;&lt;"运行手机游戏"&lt;&lt;endl;
	}
};

class HandsetAddressList:public HandsetSoft
{
public:
	void run()
	{
		cout&lt;&lt;"运行手机通讯录"&lt;&lt;endl;
	}
};

class HandsetBrand
{
protected:
	HandsetSoft *soft;
public:
	void setHandsetSoft(HandsetSoft *soft)
	{
		this-&gt;soft=soft;
	}
	virtual void run()=0;
};

class HandsetBrandN:public HandsetBrand
{
public:
	void run()
	{
		soft-&gt;run();
	}
};

class HandsetBrandM:public HandsetBrand
{
public:
	void run()
	{
		soft-&gt;run();
	}
};

int main()
{
	HandsetBrand *hb;
	hb=new HandsetBrandM();
	
	hb-&gt;setHandsetSoft(new HandsetGame());
	hb-&gt;run();
	hb-&gt;setHandsetSoft(new HandsetAddressList());
	hb-&gt;run();


	return 0;
}

 Bridge模式适用场景：
1.当一个对象有多个变化因素的时候，考虑依赖于抽象的实现，而不是具体的实现。如上面例子中手机品牌有2种变化因素，一个是品牌，一个是功能。
2.当多个变化因素在多个对象间共享时，考虑将这部分变化的部分抽象出来再聚合/合成进来，如上面例子中的通讯录和游戏，其实是可以共享的。
3.当我们考虑一个对象的多个变化因素可以动态变化的时候，考虑使用桥接模式，如上面例子中的手机品牌是变化的，手机的功能也是变化的，所以将他们分离出来，独立的变化。
总结：
1.设计中有超过一维的变化我们就可以用桥模式。如果只有一维在变化，那么我们用继承就可以圆满的解决问题。
Abstraction.h

#ifndef _ABSTRACTION_H_
#define _ABSTRACTION_H_

class AbstractionImplement;

class Abstraction
{
public:
    virtual void Operation()=0;//定义接口，表示该类所支持的操作
    virtual ~Abstraction();
protected:
    Abstraction();
};

class RefinedAbstractionA:public Abstraction
{
public:
    RefinedAbstractionA(AbstractionImplement* imp);//构造函数
    virtual void Operation();//实现接口
    virtual ~RefinedAbstractionA();//析构函数
private:
    AbstractionImplement* _imp;//私有成员
};

class RefinedAbstractionB:public Abstraction
{
public:
    RefinedAbstractionB(AbstractionImplement* imp);//构造函数
    virtual void Operation();//实现接口
    virtual ~RefinedAbstractionB();//析构函数
private:
    AbstractionImplement* _imp;//私有成员
};
#endif
Abstraction.cpp

#include "Abstraction.h"
#include "AbstractionImplement.h"
#include &lt;iostream&gt;

using namespace std;

Abstraction::Abstraction()
{}

Abstraction::~Abstraction()
{}

RefinedAbstractionA::RefinedAbstractionA(AbstractionImplement* imp)
{
    this-&gt;_imp = imp;
}

RefinedAbstractionA::~RefinedAbstractionA()
{
    delete this-&gt;_imp;
    this-&gt;_imp = NULL;
}

void RefinedAbstractionA::Operation()
{
    cout &lt;&lt; "RefinedAbstractionA::Operation" &lt;&lt; endl;
    this-&gt;_imp-&gt;Operation();
}

RefinedAbstractionB::RefinedAbstractionB(AbstractionImplement* imp)
{
    this-&gt;_imp = imp;
}

RefinedAbstractionB::~RefinedAbstractionB()
{
    delete this-&gt;_imp;
    this-&gt;_imp = NULL;
}

void RefinedAbstractionB::Operation()
{
    cout &lt;&lt; "RefinedAbstractionB::Operation" &lt;&lt; endl;
    this-&gt;_imp-&gt;Operation();
}AbstractImplement.h


#ifndef _ABSTRACTIONIMPLEMENT_H_
#define _ABSTRACTIONIMPLEMENT_H_

//抽象基类，定义了实现的接口
class AbstractionImplement
{
public:
    virtual void Operation()=0;//定义操作接口
    virtual ~AbstractionImplement();
protected:
    AbstractionImplement();
};

// 继承自AbstractionImplement,是AbstractionImplement的不同实现之一
class ConcreteAbstractionImplementA:public AbstractionImplement
{
public:
    ConcreteAbstractionImplementA();
    void Operation();//实现操作
    ~ConcreteAbstractionImplementA();
protected:
};

// 继承自AbstractionImplement,是AbstractionImplement的不同实现之一
class ConcreteAbstractionImplementB:public AbstractionImplement
{
public:
    ConcreteAbstractionImplementB();
    void Operation();//实现操作
    ~ConcreteAbstractionImplementB();
protected:
};
#endif
AbstractImplement.cpp

#include "AbstractionImplement.h"
#include &lt;iostream&gt;

using namespace std;

AbstractionImplement::AbstractionImplement()
{}

AbstractionImplement::~AbstractionImplement()
{}

ConcreteAbstractionImplementA::ConcreteAbstractionImplementA()
{}

ConcreteAbstractionImplementA::~ConcreteAbstractionImplementA()
{}

void ConcreteAbstractionImplementA::Operation()
{
    cout &lt;&lt; "ConcreteAbstractionImplementA Operation" &lt;&lt; endl;
}

ConcreteAbstractionImplementB::ConcreteAbstractionImplementB()
{}

ConcreteAbstractionImplementB::~ConcreteAbstractionImplementB()
{}

void ConcreteAbstractionImplementB::Operation()
{
    cout &lt;&lt; "ConcreteAbstractionImplementB Operation" &lt;&lt; endl;
}

main.cpp
#include "Abstraction.h"
#include "AbstractionImplement.h"
#include &lt;iostream&gt;

using namespace std;

int main()
{
    /* 将抽象部分与它的实现部分分离，使得它们可以独立地变化

    1、抽象Abstraction与实现AbstractionImplement分离;

    2、抽象部分Abstraction可以变化，如new RefinedAbstractionA(imp)、new RefinedAbstractionB(imp2);

    3、实现部分AbstractionImplement也可以变化，如new ConcreteAbstractionImplementA()、new ConcreteAbstractionImplementB();

    */

    AbstractionImplement* imp = new ConcreteAbstractionImplementA();        //实现部分ConcreteAbstractionImplementA
    Abstraction* abs = new RefinedAbstractionA(imp);                        //抽象部分RefinedAbstractionA
    abs-&gt;Operation();

    cout &lt;&lt; "-----------------------------------------" &lt;&lt; endl;

    AbstractionImplement* imp1 = new ConcreteAbstractionImplementB();        //实现部分ConcreteAbstractionImplementB
    Abstraction* abs1 = new RefinedAbstractionA(imp1);                        //抽象部分RefinedAbstractionA
    abs1-&gt;Operation();

    cout &lt;&lt; "-----------------------------------------" &lt;&lt; endl;

    AbstractionImplement* imp2 = new ConcreteAbstractionImplementA();        //实现部分ConcreteAbstractionImplementA
    Abstraction* abs2 = new RefinedAbstractionB(imp2);                        //抽象部分RefinedAbstractionB
    abs2-&gt;Operation();

    cout &lt;&lt; "-----------------------------------------" &lt;&lt; endl;

    AbstractionImplement* imp3 = new ConcreteAbstractionImplementB();        //实现部分ConcreteAbstractionImplementB
    Abstraction* abs3 = new RefinedAbstractionB(imp3);                        //抽象部分RefinedAbstractionB
    abs3-&gt;Operation();

    cout &lt;&lt; endl;
    return 0;
}

Bridge模式优缺点总结：

优点：
1.将实现抽离出来，再实现抽象，使得对象的具体实现依赖于抽象，满足了依赖倒转原则。
2.将可以共享的变化部分，抽离出来，减少了代码的重复信息。
3.对象的具体实现可以更加灵活，可以满足多个因素变化的要求。
缺点：
1.客户必须知道选择哪一种类型的实现。
Bridge模式使用总结：

        Bridge模式将抽象和实现分别独立实现，在代码中就是Abstraction类和AbstractionImplement类。
使用组合（委托）的方式将抽象和实现彻底地解耦，这样的好处是抽象和实现可以分别独立地变化，系统的耦合性也得到了很好的降低。
       GoF的那句话中的“实现”该怎么去理解：“实现”特别是和“抽象”放在一起的时候我们“默认”的理解是“实现”就是“抽象”的具体子类的实现，但是这里GoF所谓的“实现”的含义不是指抽象基类的具体子类对抽象基类中虚函数（接口）的实现，是和继承结合在一起的。而这里的“实现”的含义指的是怎么去实现用户的需求，并且指的是通过组合（委托）的方式实现的，因此这里的实现不是指的继承基类、实现基类接口，而是指的是通过对象组合实现用户的需求。
        实际上上面使用Bridge模式和使用带来问题方式的解决方案的根本区别在于是通过继承还是通过组合的方式去实现一个功能需求。
备注：
        由于实现的方式有多种，桥接模式的核心就是把这些实现独立出来，让他们各自变化。将抽象部分与它的实现部分分离：实现系统可能有多角度（维度）分类，每一种分类都可能变化，那么就把这种多角度分离出来让它们独立变化，减少它们之间的耦合。在发现需要多角度去分类实现对象，而只用继承会造成大量的类增加，不能满足开放-封闭原则时，就要考虑用Bridge桥接模式了。合成/聚合复用原则：尽量使用合成/聚合，精良不要使用类继承。
        优先使用对象的合成/聚合将有助于保持每个类被封装，并被集中在单个任务上。这样类和类继承层次会保持较小规模，并且不太可能增长为不可控制的庞然大物。



Prototype模式来源：
           关于这个模式，突然想到了小时候看的《西游记》，齐天大圣孙悟空再发飙的时候可以通过自己头上的3根毛立马复制出来成千上万的孙悟空，对付小妖怪很管用（数量最重要）。Prototype模式也正是提供了自我复制的功能，就是说新对象的创建可以通过已有对象进行创建。在C++中拷贝构造函数（CopyConstructor）曾经是很多程序员的噩梦，浅层拷贝和深层拷贝的魔魇也是很多程序员在面试时候的快餐和系统崩溃时候的根源之一。
Prototype模式作用：
         用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。Prototype模式提供了一个通过已存在对象进行新对象创建的接口（Clone）， Clone()实现和具体的语言相关，在C++中通过拷贝构造函数实现。
Prototype模式UML图如图1所示：
                                              
Prototype模式代码示例：

Prototype.h

#ifndef _PROTOTYPE_H_
#define _PROTOTYPE_H_

/*Prototype模式提供了一个通过已存在对象进行新对象创建的接口（Clone）
  Clone()实现和具体的语言相关，在C++中通过拷贝构造函数实现

作用:
用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。

*/

/*Prototype原型基类，定义Clone接口函数
*/
class Prototype
{
protected:
    Prototype();
public:
    virtual Prototype* Clone() const=0;//定义Clone接口，根据不同的派生类来实例化对象
    virtual ~Prototype();
};

//派生自Prototype，实现其接口函数
class ConcretePrototype1:public Prototype
{
public:
    ConcretePrototype1();//构造函数
    ~ConcretePrototype1();//析构函数
    ConcretePrototype1(const ConcretePrototype1&amp;);//拷贝构造函数
    virtual Prototype* Clone() const;//实现基类定义的Clone接口，内部调用拷贝构造函数实现复制功能
};

//派生自Prototype，实现其接口函数
class ConcretePrototype2:public Prototype
{
public:
    ConcretePrototype2();//构造函数
    ~ConcretePrototype2();//析构函数
    ConcretePrototype2(const ConcretePrototype2&amp;);//拷贝构造函数
    virtual Prototype* Clone() const;//实现基类定义的Clone接口，内部调用拷贝构造函数实现复制功能
};

#endifPrototype.cpp

#include "Prototype.h"
#include "iostream"

using namespace std;

//原型模式
Prototype::Prototype()
{
    cout&lt;&lt;"Prototype"&lt;&lt;endl;
}

Prototype::~Prototype()
{
    cout&lt;&lt;"~Prototype"&lt;&lt;endl;
}

//具体原型类1
ConcretePrototype1::ConcretePrototype1()
{
    cout&lt;&lt;"ConcretePrototype1"&lt;&lt;endl;
}

ConcretePrototype1::~ConcretePrototype1()
{
    cout&lt;&lt;"~ConcretePrototype1"&lt;&lt;endl;
}

ConcretePrototype1::ConcretePrototype1(const ConcretePrototype1&amp; cp)
{
    cout&lt;&lt;"ConcretePrototype1 copy"&lt;&lt;endl;
}

Prototype* ConcretePrototype1::Clone() const
{
    return new ConcretePrototype1(*this);
}

//具体原型类2
ConcretePrototype2::ConcretePrototype2()
{
    cout&lt;&lt;"ConcretePrototype2"&lt;&lt;endl;
}

ConcretePrototype2::~ConcretePrototype2()
{
    cout&lt;&lt;"~ConcretePrototype2"&lt;&lt;endl;
}

ConcretePrototype2::ConcretePrototype2(const ConcretePrototype2&amp; cp)
{
    cout&lt;&lt;"ConcretePrototype2 copy"&lt;&lt;endl;
}

Prototype* ConcretePrototype2::Clone() const
{
    return new ConcretePrototype2(*this);
}main.cpp#include "Prototype.h"
#include &lt;iostream&gt;
using namespace std;

int main()
{
    /*原型模式作用：用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。
      Prototype模式重在从自身复制自己创建新类，隐藏（不需知道）对象创建的细节
    */

    /*1、用原型实例p1指定创建对象的种类ConcretePrototype1 */
    Prototype* p1 = new ConcretePrototype1();

    /*2、通过拷贝这些原型创建新的对象 */
    Prototype* p2 = p1-&gt;Clone();

    cout&lt;&lt; "------------------------" &lt;&lt; endl;

    Prototype* p3 = new ConcretePrototype2();
    Prototype* p4 = p3-&gt;Clone();

    cout&lt;&lt; "------------------------" &lt;&lt; endl;

    delete p1;
    delete p2;
    cout&lt;&lt; "------------------------" &lt;&lt; endl;

    delete p3;
    delete p4;

    return 0;
}Prototype模式使用总结：

       Prototype模式和Builder模式、AbstractFactory模式都是通过一个类（对象实例）来专门负责对象的创建工作（工厂对象），它们之间的区别是：Builder模式重在复杂对象的一步步创建（并不直接返回对象），AbstractFactory模式重在产生多个相互依赖类的对象，而Prototype模式重在从自身复制自己创建新类。



Builder模式来源：         
         生活中有着很多的Builder的例子，个人觉得大学生活就是一个Builder模式的最好体验：要完成大学教育，一般将大学教育过程分成4个学期进行，因此没有学习可以看作是构建完整大学教育的一个部分构建过程，每个人经过这4年的（4个阶段）构建过程得到的最后的结果不一样，因为可能在四个阶段的构建中引入了很多的参数（每个人的机会和际遇不完全相同）。
Builder模式作用：

         当我们要创建的对象很复杂的时候（通常是由很多其他的对象组合而成），我们要要复杂对象的创建过程和这个对象的表示（展示）分离开来，这样做的好处就是通过一步步的进行复杂对象的构建，由于在每一步的构造过程中可以引入参数，使得经过相同的步骤创建最后得到的对象的展示不一样。
Builder模式的UML结构图如图1所示：
                          

         Builder模式的关键是其中的Director对象并不直接返回对象，而是通过一步步（BuildPartA，BuildPartB，BuildPartC）来一步步进行对象的创建。当然这里Director可以提供一个默认的返回对象的接口（即返回通用的复杂对象的创建，即不指定或者特定唯一指定BuildPart中的参数）。
Builder模式的实现基于以下几个面向对象的设计原则:
1)把变化的部分提取出来形成一个基类和对应的接口函数,在这里不会变化的是都会创建PartA和PartB,变化的则是不同的创建方法,于是就抽取出这里的Builder基类和BuildPartA,BuildPartB接口函数
2)采用聚合的方式聚合了会发生变化的基类,就是这里Director聚合了Builder类的指针.
以上，通过两个派生类ConcreteBuilder1、ConcreteBuilder2定义了两种不同的建造细节（建造步骤是一样的，由Construct函数确定），通过两个派生类所建造出来的对象，对外部所展现出来的属性或者功能是不一样的，由各自Builder派生类中的建造方法（BuildPartA、BuildPartB、BuildPartC）决定。
Builder.h


#ifndef _BUILDER_H_
#define _BUILDER_H_

#include &lt;string&gt;
#include &lt;vector&gt;

using namespace std;

//产品类
class Product
{
private:
    string m_partA;
    string m_partB;
    string m_partC;
public:
    void setPartA(const string&amp; s);
    void setPartB(const string&amp; s);
    void setPartC(const string&amp; s);
    Product();
    ~Product();
};

//抽象Builder基类，定义不同部分的创建接口
class Builder
{
public:
    virtual void BuildPartA()=0;
    virtual void BuildPartB()=0;
    virtual void BuildPartC()=0;
    virtual Product* GetProduct()=0;
    Builder();
    virtual ~Builder();
};

//  Builder的派生类,实现BuilderPartA和BuilderPartB和BuildPartC接口函数 
class ConcreteBuilder1:public Builder
{
public:
    ConcreteBuilder1();
    ~ConcreteBuilder1();
    virtual void BuildPartA();
    virtual void BuildPartB();
    virtual void BuildPartC();
    virtual Product* GetProduct();
private:
    Product* m_pProduct;
};

//  Builder的派生类,实现BuilderPartA和BuilderPartB和BuildPartC接口函数 
class ConcreteBuilder2:public Builder
{
public:
    ConcreteBuilder2();
    ~ConcreteBuilder2();
    virtual void BuildPartA();
    virtual void BuildPartB();
    virtual void BuildPartC();
    virtual Product* GetProduct();
private:
    Product* m_pProduct;
};

//ConcreteBuilder1与ConcreteBuilder2是Builder的两个派生类，用于实现两种不同的建造细节

 // 使用Builder构建产品,构建产品的过程都一致,但是不同的builder有不同的实现
 // 这个不同的实现通过不同的Builder派生类来实现,存有一个Builder的指针,通过这个来实现多态调用 
class Director
{
public:
    Director(Builder* pBuilder);
    ~Director();

    //Construct函数定义一个对象的整个构建过程,不同的部分之间的装配方式都是一致的,
    //首先构建PartA其次是PartB,只是根据不同的构建者会有不同的表示 
    void Construct();
    //void Construct(const string&amp; buildPara);
private:
    Builder* m_pBuilder;
};

#endif

Director.cpp


#include "Builder.h"
#include &lt;iostream&gt;
#include &lt;vector&gt;

using namespace std;

Product::~Product()
{
}

Product::Product()
{}

void Product::setPartA(const string&amp; s)
{
    this-&gt;m_partA = s;
}

void Product::setPartB(const string&amp; s)
{
    this-&gt;m_partB = s;
}

void Product::setPartC(const string&amp; s)
{
    this-&gt;m_partC = s;
}

Builder::Builder()
{}

Builder::~Builder()
{}

ConcreteBuilder1::ConcreteBuilder1()
{
    this-&gt;m_pProduct = new Product();
    cout&lt;&lt;"Create empty product!"&lt;&lt;endl;
}

void ConcreteBuilder1::BuildPartA()
{
    this-&gt;m_pProduct-&gt;setPartA("A");
    cout&lt;&lt;"BuildPartA"&lt;&lt;endl;
}

void ConcreteBuilder1::BuildPartB()
{
    this-&gt;m_pProduct-&gt;setPartB("B");
    cout&lt;&lt;"BuildPartB"&lt;&lt;endl;
}

void ConcreteBuilder1::BuildPartC()
{
    this-&gt;m_pProduct-&gt;setPartC("C");
    cout&lt;&lt;"BuildPartC"&lt;&lt;endl;
}

Product* ConcreteBuilder1::GetProduct()
{
    return this-&gt;m_pProduct;
}

ConcreteBuilder1::~ConcreteBuilder1()
{
    delete this-&gt;m_pProduct;
    this-&gt;m_pProduct = NULL;
}

ConcreteBuilder2::ConcreteBuilder2()
{
    this-&gt;m_pProduct = new Product();
    cout&lt;&lt;"Create empty product!"&lt;&lt;endl;
}

void ConcreteBuilder2::BuildPartA()
{
    this-&gt;m_pProduct-&gt;setPartA("A");
    cout&lt;&lt;"BuildPartA"&lt;&lt;endl;
}

void ConcreteBuilder2::BuildPartB()
{
    this-&gt;m_pProduct-&gt;setPartB("B");
    cout&lt;&lt;"BuildPartB"&lt;&lt;endl;
}

void ConcreteBuilder2::BuildPartC()
{
    this-&gt;m_pProduct-&gt;setPartC("C");
    cout&lt;&lt;"BuildPartC"&lt;&lt;endl;
}

Product* ConcreteBuilder2::GetProduct()
{
    return this-&gt;m_pProduct;
}

ConcreteBuilder2::~ConcreteBuilder2()
{
    delete this-&gt;m_pProduct;
    this-&gt;m_pProduct = NULL;
}

Director::Director(Builder* pBuilder)
{
    this-&gt;m_pBuilder = pBuilder;
}

void Director::Construct()
{
    this-&gt;m_pBuilder-&gt;BuildPartA();
    this-&gt;m_pBuilder-&gt;BuildPartB();
    this-&gt;m_pBuilder-&gt;BuildPartC();
}

Director::~Director()
{
    delete this-&gt;m_pBuilder;
    this-&gt;m_pBuilder = NULL;
}main.cpp


#include "Builder.h"
#include &lt;iostream&gt;

using namespace std;

int main()
{
    Director* pDirector = new Director(new ConcreteBuilder1());
    pDirector-&gt;Construct();

    Director* pDirector1 = new Director(new ConcreteBuilder2());
    pDirector1-&gt;Construct();

    return 0;
}
Builder模式的使用总结：


        建造者模式和工厂模式使用很相似，但也有区别，建造者模式最主要功能是基本方法的调用顺序安排，也就是这些基本方法已经实现了；而工厂方法则重点是创建，你要什么对象我创造一个对象出来，组装顺序则不是他关心的。
        建造者模式使用的场景，一是产品类非常的复杂，或者产品类中的调用顺序不同产生了不同的效能，这个时候使用建造者模式是非常合适。




Adapter模式来源：
     在实际应用中，将一个类的接口转换成客户希望的另外一个接口。Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。生活中笔记本电脑和手机等数码产品的充电器就是一个适配器，将家用220V的交流电转换为笔记本或手机正常工作所需的目标电压和电流。适配器起到一种转换和包装的作用。
Adapter模式作用：


     Adapter设计模式主要目的组合两个不相干类，常用有两种方法：第一种解决方案是修改各自类的接口。但是如果没有源码，或者不愿意为了一个应用而修改各自的接口，则需要使用Adapter适配器，在两种接口之间创建一个混合接口。

Adapter适配器设计模式中有3个重要角色：被适配者Adaptee，适配器Adapter和目标对象Target。其中两个现存的想要组合到一起的类分别是被适配者Adaptee和目标对象Target角色，我们需要创建一个适配器Adapter将其组合在一起。
Adapter模式中的角色：

　　1. 目标接口（Target）：客户所期待的接口。目标可以是具体的或抽象的类，也可以是接口。

　　2. 需要适配的类（Adaptee）：需要适配的类或适配者类。

　　3. 适配器（Adapter）：通过包装一个需要适配的对象，把原接口转换成目标接口。　


共有两类适配器模式：
（1）.对象适配器模式：
      在这种适配器模式中，适配器容纳一个它包裹的类的实例。在这种情况下，适配器调用被包裹对象的物理实体。是一种组合(compositon, has-a关系)。对象适配器模式使用组合，UML图如图1所示： 
                                     
                                     图1  对象适配器UML结构图
（2）.类适配器模式：
      这种适配器模式下，适配器继承自已实现的类（一般多重继承），是一种继承(inheritance,is-a关系)。UML图如如图2所示：
                     　

                                     图2  类适配器UML结构图

代码示例如下：


Adapter.h
#ifndef _ADAPTER_H_
#define _ADAPTER_H_

//目标接口类，客户需要的接口
class Target
{
public:
	Target();
	virtual ~Target();
	virtual void Request();//定义标准接口
};

//需要适配的类
class Adaptee
{
public:
	Adaptee();
	~Adaptee();
	void SpecificRequest();
};

//类模式，适配器类，通过public继承获得接口继承的效果，通过private继承获得实现继承的效果
class Adapter :public Target, private Adaptee
{
public:
	Adapter();
	~Adapter();
	virtual void Request();//实现Target定义的Request接口
};

//对象模式，适配器类，继承Target类，采用组合的方式实现Adaptee的复用
class Adapter1 :public Target
{
public:
	Adapter1(Adaptee* adaptee);
	Adapter1();
	~Adapter1();
	virtual void Request();//实现Target定义的Request接口
private:
	Adaptee* _adaptee;
};
#endifAdapter.cpp
#include "Adapter.h"
#include &lt;iostream&gt;

using namespace std;

Target::Target()
{}

Target::~Target()
{}

void Target::Request()
{
	cout &lt;&lt; "Target::Request()" &lt;&lt; endl;
}

Adaptee::Adaptee()
{
}

Adaptee::~Adaptee()
{
}

void Adaptee::SpecificRequest()
{
	cout &lt;&lt; "Adaptee::SpecificRequest()" &lt;&lt; endl;
}

//类模式的Adapter
Adapter::Adapter()
{
}

Adapter::~Adapter()
{
}

void Adapter::Request()
{
	cout &lt;&lt; "Adapter::Request()" &lt;&lt; endl;
	this-&gt;SpecificRequest();
	cout &lt;&lt; "----------------------------" &lt;&lt; endl;
}

//对象模式的Adapter
Adapter1::Adapter1() :_adaptee(new Adaptee)
{
}

Adapter1::Adapter1(Adaptee* _adaptee)
{
	this-&gt;_adaptee = _adaptee;
}

Adapter1::~Adapter1()
{
}

void Adapter1::Request()
{
	cout &lt;&lt; "Adapter1::Request()" &lt;&lt; endl;
	this-&gt;_adaptee-&gt;SpecificRequest();
	cout &lt;&lt; "----------------------------" &lt;&lt; endl;
}&lt;/span&gt;main.cpp
#include "Adapter.h"

int main()
{
	//类模式Adapter
	Target* pTarget = new Adapter();
	pTarget-&gt;Request();

	//对象模式Adapter1
	Adaptee* ade = new Adaptee();
	Target* pTarget1 = new Adapter1(ade);
	pTarget1-&gt;Request();

	//对象模式Adapter2
	Target* pTarget2 = new Adapter1();
	pTarget2-&gt;Request();

	return 0;
}

Adapter模式使用总结：
        在Adapter模式的两种模式中，有一个很重要的概念就是接口继承和实现继承的区别和联系。接口继承和实现继承是面向对象领域的两个重要的概念，接口继承指的是通过继承，子类获得了父类的接口，而实现继承指的是通过继承子类获得了父类的实现（并不提供接口）。在C++中的public继承既是接口继承又是实现继承，因为子类在继承了父类后既可以对外提供父类中的接口操作，又可以获得父类的接口实现。当然我们可以通过一定的方式和技术模拟单独的接口继承和实现继承，例如我们可以通过private继承获得实现继承的效果（private继承后，父类中的接口都变为private，当然只能是实现继承了），通过纯抽象基类模拟接口继承的效果，但是在C++中pure
 virtual function也可以提供默认实现，因此这是不纯正的接口继承，但是在Java中我们可以interface来获得真正的接口继承了。





           这是一个快速发展的时代，随着互联网的普及，数据成指数倍增长，相同类型的企业也如雨后春笋般越来越多！那么如何在这个快速发展的时代，脱颖而出，把握时代的脉搏呢？答案就是：建立自己企业的大数据！提高企业的生存和竞争力，大数据无疑是一把利剑，通过数据分析，不仅可以让你知己知彼，更可以让自己的企业决胜千里之外，使企业在与同行竞争中，更具竞争力的一大利器，用的好，甚至能碾压竞争对手。大数据近年的崛起和发展已经初现其巨大的作用，据分析拥有优秀大数据能力的企业，做出正确决策的可能性高出竞争对手3倍、决策速度比竞争对手快5倍。
                                    
           当某在线视频网站准备推出自制剧的时候，评论家纷纷嘲笑他们把握观众品味的能力。很难有谁会想到，该公司通过分析其积累的多年用户观影偏好的大数据，来指导制片人、主演选择和编剧内容并一炮走红，帮助其在一个季度内获取数百万新增用户，并在接下来的一两年内里获得数倍的股价提升。
 
打造大数据战略的四大挑战：


                                                
 
         大数据之所以“大”，在于数量（Volume）、种类（Variety）和产生速度（Velocity）的特征。Volume：数据体量大，许多企业大数据规模可达数百TB、甚至EB级别（1EB=1024PB、1PB=1024TB、1TB=1024GB）。Variety：数据类型多，包括各种格式与形态。Velocity：大量数据每一秒都在产生，对数据实时收集、处理与分析应用的速度要求很高。


         我们正在迎来一个数据爆炸的时代：各类设备和互动产生的数据量正以年均大于50%的速度增长，预计在2020年可能会达到44ZB（44万亿GB）。全球企业越来越关注大数据给自己带来的机会或者冲击，贝恩公司的大数据行业调研显示，北美和欧洲400多家大型企业（年营业额高于5亿美元）中，大约60%的企业积极在大数据方面进行投资，希望能够带来显著的收益。（见图1《全球数据的数量、种类和产生速度正在爆发性增长》）
                                                    


         据调查显示，拥有优秀大数据能力的企业，它的财务表现排在行业前25分位的可能性是竞争对手的2倍、做出正确决策的可能性高出竞争对手3倍、决策速度比竞争对手快5倍。可见，大数据对于企业乃至整个社会的重要性不言而喻。
无论数据如何变化，它们是“金矿”还是“垃圾”，取决于企业是否了解自身拥有（或能够获得）的数据资产，并以此建立清晰的大数据战略，从而在战略、运营和一线层面产生价值。无法持续地产生价值的数据是没有意义的。
         基于贝恩公司的大数据行业调研，企业今天在运用大数据时还面临不少困难。主要包括战略、人才、数据资产和工具等四大类挑战。
战略：仅有约23%的企业拥有明确的大数据相关战略，决定并知道如何将大数据分析有效地应用于企业运营，并建立相应的组织能力、流程和激励机制来赋能数据分析以支持决策。
人才：仅有约36%的企业拥有专门的数据洞察团队，并拥有同时具备数据科学专业能力和商业敏感度的人才。
数据资产：仅有约19%的企业拥有高质量、一致性较好、易于获取和应用的大数据。
工具：仅有38%的企业正在使用先进的大数据工具，如Hadoop、NoSQL、HPCC和自动数据清洗算法等。

建立大数据战略与能力的关键因素：
企业如何建立清晰的大数据战略和关键的大数据能力？贝恩根据与全球客户合作的大数据相关项目经验，总结出企业建立大数据战略与能力的6大关键因素。
 关键成功因素1：发现独有的“数据资产”：
       你拖动某部电影视频滚动条的时候，视频网站正在分析整体观众的偏好数据并指导下一步剧集的剧情走向；当你在店里购物的时候，零售商正在分析整体客流轨迹数据并指导店面的布局和商品的上下架；当工厂每天使用机器设备的时候，厂商正在分析整体设备的使用习惯并指导下一代产品的设计、维修保养的主动性变动以提升效率……这样的例子非常多。
作为建立大数据能力的基础，企业应像对待其他重要资产一样，发现、评估和管理好并不断扩充数据资产。
首先，应对企业数据资产现状开展深入评估，明确目前数据资产的来源、类型与数据准备情况，评估数据是否足够完整、是否与业务发展直接相关；
其次，根据评估结果以及企业经营战略目标，应明确目前还有哪些数据资产与目标存在显著差距，弥补差距的优先级是什么；
然后，对所有可进一步获取的内外部数据资产进行识别与评估，在深入考虑数据质量、重要性与相关度、获取成本与时间要求等相关因素之后，选择获取数据资产的最佳方式，诸如自行采集整理、对外采购数据、与外部合作伙伴进行交换等；

在获取新的数据资产后，企业还需建立数据治理机制，对数据进行妥善清洗与存储，确保数据的可用性与一致性，并明确数据授权和更新制度。
 关键成功因素2：明确数据资产如何“创造价值”：
       在评估企业的数据资产后，需确定如何运用其对企业战略进行支撑与引领。具体而言，大数据可为企业带来五方面的价值：
优化企业内部运营流程：例如，某饮料公司运用复杂算法分析社交媒体大数据，识别对于重要议题具有影响力的品牌意见领袖，并对其进行针对性的引领，以提升营销效果；某连锁零售公司通过分析大量门店销售数据，寻找产品之间的未知联系，以提升捆绑销售。
优化现有产品与服务：例如，某娱乐公司运用电子公园通行证来采集游客在其主题公园中的活动数据信息，以此来优化游客在公园中的体验；某汽车安全信息系统服务商使用传感器来收集车辆驾驶数据，以改进其产品的设计、生产与维修流程。
开发新产品新服务：例如，某保险公司使用插入式设备来收集驾驶行为数据、通过分析司机的驾驶习惯对其保险提供相应折扣，以主动保留驾驶行为较安全的客户；某在线影片租赁提供商通过分析观影档案数据来针对性提升用户观影体验，并提供分析结果给影片投资方以优化影片制作内容。
建立新业务模式：例如，某医疗保险公司通过对病人信息数据的预测性分析，向易患病的人群提供预防性关怀服务，以提高服务此类客户的利润率；某理财服务公司免费赠送个人财务软件给用户，在用户使用时分析其消费数据，再向其精准推送相关广告。
获取生态系统控制力：例如，某企业级软件公司通过对渠道伙伴的运营数据开展智能化管理与分析，鉴别渠道商的资质与能力，并对业绩进行预测和预警；某电子商务公司数据产品团队基于其电商平台沉淀的大量交易数据，为平台上的卖家开发各类大数据产品，帮助它实现数据化运营和增收，提升电商生态系统对卖家的吸引力。
 关键成功因素3：识别优先应用场景：
                                                 
        对于公司业务部门（营销、销售和服务等部门），大数据可以帮助其创造以上五种战略价值；对于公司职能部门（研发、供应链和人力资源等部门），大数据也可以帮助其优化内部运营流程。为识别公司业务与职能部门具体可能的大数据应用场景，可对标业界大数据应用实践、基于数据资产现状评估、剖析业务与职能流程中可能进一步采集的数据与应用方式，运用头脑风暴和内部研讨会列出所有可能的大数据应用。需注意的是，大数据应用场景必须契合业务与职能部门的现实需求，切忌闭门造车、脱离实际。
表：各领域常见大数据应用场景示例
        在确定可能的大数据应用之后，可通过价值创造与业务成熟度两个维度对大数据应用进行评估和优先级排序，以按顺序推动相关大数据应用的落地实施。对于价值创造维度，可以用创造价值的多少（如提升运营效率、提升投资回报等）作为评估标准；对于业务成熟度维度，可以将所需数据资产的可获得性以及所需资源投入和大数据能力支撑（如资金、人才和跨部门合作等）等标准用于评估。

图2：大数据应用优先级评估与排序框架：
                                                

关键成功因素4：数据-分析-洞察-决策支撑的产品化、常态化：

        为将大数据高效应用于企业的日常运营，需要不断将数据分析能力转化为内部应用产品，并将数据分析工作常态化。对于数据分析产品化，可通过大数据应用战略规划、大数据应用场景设计、分析大数据以获取洞察这一过程的牵引，不断推动大数据应用产品的设计、开发与应用，最终实现数据分析产品的可持续运营。而对于分析工作常态化，需要持续维护数据分析产品并监测实际使用效果，为业务与职能部门提供数据分析支持，并对其日常使用中的问题及时进行解答。
        以某家电公司为例，其借助于收集存储了上亿用户数据的大数据平台，建立了需求预测和用户活跃度等数据模型。以此为基础，该公司为营销及销售人员开发了具有精准营销功能的应用软件，可辅助其面向区域、社区和用户个体开展精准营销；此外，还为研发人员开发了具有用户交互功能的应用软件，可帮助研发人员更全面地了解用户痛点、受欢迎的产品特征、用户兴趣分布与可参与交互的活跃用户。这些大数据产品在日常应用中取得了巨大的成效，在系统运营的近一年里，该公司开展了数百场基于数据挖掘和需求预测的精准营销活动，转化的销售额达60亿元。
关键成功因素5:通过组织、人才与IT，为大数据提供强力保障与支撑：
        大数据战略的落地离不开组织、人才与IT能力的支持，而这些关键要素与能力的建设，需要既能贴近业务一线、又能与战略保持一致。
         对于大数据组织运作方式，由于大数据核心分析能力、工具投资等在各业务部门之间协同效应突出，企业（尤其是大型企业）一般采用集中化运营中心模式。同时，与业务决策、应用相关的权利被授予前线部门，以确保数据分析与业务决策的无缝衔接。无论如何设计大数据组织运作模式，核心原则是根据公司自身情况与需要，确保大数据分析能力能够最有效地支持一线决策。
        此外，大数据组织需要多种具备关键能力的人才队伍，来共同支持大数据组织架构的运营。其需要的人才团队包括大数据应用业务经理团队、大数据分析团队、数据资产管理团队、技术开发与维护团队以及风险管理团队等等。
        除了组织与人才之外，大数据的落地还需要强大的IT系统架构作为支撑。企业需要建立强大的大数据分析平台系统，从不同数据源调取并分析数据，拉通数据基础分析，以统一服务各部门的大数据应用场景。同时，该数据平台还需具备对跨数据源数据进行统一清洗和存储，以确保数据可用性与一致性的能力。除此之外，企业可以建立或优化主数据管理系统，为大数据分析平台以及各业务大数据应用提供统一、便捷的数据联机交易服务，以支撑企业大数据运营。
 关键成功因素6：通过大数据隐私和安全管理，消除法律及消费者认知风险：
        大数据带来机遇与价值的同时，也带来了一定的商业风险，特别是涉及法律（例如某社交网络平台由于违反其隐私政策，遭到美国贸易委员会起诉）与消费者认知的风险（例如某互联网公司街景项目由于拍摄的很多照片涉及当地居民隐私，遭到后者大规模抗议）。为此，可按数据类型及从数据收集到分析使用各环节来识别不同类型、地域的法规与认知风险，并予以及时应对。
         以隐私风险程度较高的数据收集环节为例：对于个人可识别数据（如身份号等），由于法律规定最高级别保护，故若无明确用途不建议采集；对于敏感数据（如交易和信用信息等），数据采集需明确告知用户并获得其同意；对于非敏感数据（如产品数据等）可按需采集。
        此外，企业应建立统一的国际政策法规团队，通过基于全球标准的数据流程来管理数据隐私，并在此基础上根据各国不同法规进行合法的数据隐私本土化管理。同时，还可通过主动披露客户隐私政策以获取数据使用分析授权、向用户提供自身隐私信息控制与删除权限或将个人隐私数据整合为群体匿名数据进行分析以及获取第三方隐私风险管理认证等方式，来降低用户的担忧。
       企业在建立大数据能力过程中，需要专业公司的帮助和支持。贝恩公司的完整大数据方法论可助力企业建立制胜的大数据战略和能力。
图3：贝恩可提供的大数据战略制定、能力建设及决策支持与分析外包服务：

                                  

大数据的快速发展对于企业既是挑战，更是机会。企业必须及时抓住大数据带来的战略机遇，制定明确的大数据战略、建立强大的大数据决策支持体系与各方面能力，以充分挖掘大数据时代蕴含的巨大商业价值。
 大数据的六种典型应用：
                                         

1、个性化营销
        《一对一的未来》一书的作者罗杰斯和派柏斯在该书中曾这样表示，“我们正经历从工业时代到信息时代史诗般宏伟的转型。我们也随之目睹了大众营业员营销的衰亡，一对一营销的兴起。”事实上，在这场营销革命的背后，大数据的应用恰恰是始作俑者。这也是大数据在当前商业方面最典型的一项应用。可以说，由数据驱动的个性化营销正成为任何企业不容回避的重要趋势。
        伴随信息过载与消费者异质化，一方面是海量数据和海量信息导致用户信息饥饿感，用户对非关联信息的容忍度与日俱减。同时，用户兴趣数据与日俱增，但用户甄别信息能力占比与日俱减，消费者呈现长尾化趋势，这一切，导致了个性化成为大数据的应用方向。
         由此个性化的技术被关注和应用，并进而推动企业在生产领域由单纯追求成本最优的规模化生产向客户化定制方向转变。同时，个性化推荐、移动跨屏推荐成为典型应用。而这些应用的背后，是计算机学、统计学、营销学的集成。
2、对客户价值的识别和挖掘
         按照科特勒在1995年对客户终身价值的定义，客户终身价值是“从一个客户身上所得到的其生命周期中全部销售额减少公司用来获取该客户和销售与服务于该客户所花费的总成本的净值。”就是公司将从该客户身上所得到的未来所有现金流的净现值。
         这意味着，以数据为支撑的客户终身价值的评价和分析将有助于公司建立市场细分的策略，确认哪一类客户才是值得花费成本来建立客户关系的，并最终找到自己真正的目标客户群。同时，它将帮助企业更好地推进客户关系管理，比如通过数据的挖掘和分析，可以知道究竟百分之多少的销售额分别来自于现有客户和新客户。当然，它还会影响到企业的定价行为，比如降价可以迅速提升老客户保留比率和新客户获取比率，但同时也会降低利润率。涨价将提高利润率，但同时也会降低老客户保留比率和新客户获取比率，这意味着企业需要用数据支持决策，最终获得一个最优化的平衡。
3、客户流失预警
         在用户即资产的时代，客户流失预警对企业的战略制定有着重要意义，流失的是否是目标客户，这些客户流失是否代表进攻者的强势进攻，还是自然选择的一个过程，哪一类型的客户，或者满足哪些条件的用户更容易流失，而满足哪些条件的用户则不易流失等等，通过不同的算法，可以发现最终客户流失的特殊及其原因，最终帮助企业决定是否需要挽留这些用户。
4、数据驱动的精准广告
        值得注意的是，在大数据时代，营销理论正在经历重要演变，在历史上，包括产品、价格、渠道、促销在内的4P理论，由于科特勒的创新变为了6P，加上了权力和公共关系。1990年，美国营销专家罗伯特·劳特朋教授提出了4G理论，以消费者需求为导向，重新设定了市场营销组合的四个基本要素，即消费者、成本、便利和沟通。但是大数据出现，尤其是由因果关系向相关关系的转变，4R理论正变得越来越适时，即关联、反应、关系和回报。
        营销理论的变迁背后，恰恰是数据驱动的精准广告时代的来临，它要求广告主在投放广告前要识别目标消费者，在投放中要实现精准定位，投放后要用一系列的数据工具进行广告效果监控。
5、企业商业决策
        如前所述，企业的诸多重要的商业决策已经变得与数据密不可分，在很多企业里，用数据说话，拿数据指路已经成为企业运营过程的必备准则。同样以苏宁为例子，其数据部门需要为业务部门提供多重服务。首先就是报表服务，即通过Adhoc的技术，为运营部门提供实时的、丰富的、准确的数据支持，帮助所有的运营部门拿数据说话的问题。比如说你今天做一个活动，你需要直接把流量的数据拿出来。其次，就是引擎服务，引擎的意思就是说，它能够应用大数据的技术去驱动前台的业务，它与报表报务的差别在于它已经直接嵌入到企业的生产经营活动中，出来的数据会直接影响到企业的整个业务。
6、库存管理和物流配送
      对于很多电商公司，或者是O2O公司来说，库存管理和物流配送正成为企业重要的竞争力，它不仅直接关系到企业的成本、利润，同时还直接关系到用户体验。由此，通过数据的分析和挖掘，可以精准测算出不同品类不同规格段商品的库存水平，同时获取物流配送的时间效率、最佳用户体验与物流整体配送效益的平衡。
 

Abstract Factory模式来源：

        抽象工厂模式是所有形态的工厂模式中最为抽象和最具一般性的一种形态。抽象工厂模式是指当有多个抽象角色时，使用的一种工厂模式。抽象工厂模式可以向客户端提供一个接口，使客户端在不必指定产品的具体的情况下，创建多个产品族中的产品对象。根据里氏替换原则，任何接受父类型的地方，都应当能够接受子类型。因此，实际上系统所需要的，仅仅是类型与这些抽象产品角色相同的一些实例，而不是这些抽象产品的实例。换言之，也就是这些抽象产品的具体子类的实例。工厂类负责创建抽象产品的具体子类的实例。
Abstract Factory模式作用：



通过引进抽象工厂模式，可以处理具有相同（或者相似）等级结构的多个产品族中的产品对象的创建问题。

由于每个具体工厂角色都需要负责两个不同等级结构的产品对象的创建，因此每个工厂角色都需要提供两个工厂方法，分别用于创建两个等级结构的产品。既然每个具体工厂角色都需要实现这两个工厂方法，所以具有一般性，不妨抽象出来，移动到抽象工厂角色中加以声明。为创建一组相关或相互依赖的对象提供一个接口，而且无需指定他们的具体类。

Abstract Factory模式UML结构图：
        以股票行情沪深板块行情数据访问为例，行情源访问程序设计，不同的行情访问方式可能不一样，为了抽象对对不同行情的访问，可以将行情源隐藏起来，提供统一的访问方式，用多态进行实现。
                    
Abstract Factory模式构成：


     工厂类角色：这是本模式的核心，含有一定的商业逻辑和判断逻辑，根据逻辑不同，产生具体的工厂产品。如例子中的Driver类。
     抽象产品角色：它一般是具体产品继承的父类或者实现的接口。由接口或者抽象类来实现。
     具体产品角色：工厂类所创建的对象就是此角色的实例。由一个具体类实现。
Abstract Factory模式代码示例：
&lt;span style="font-size:14px;"&gt;#include &lt;iostream&gt;
#include &lt;string&gt;
using namespace std;

class Icode
{
public:
	virtual void Getcode()=0;
	virtual void Setcode()=0;
};

class ShenAcode:public Icode
{
public:
	void Getcode()
	{
		cout&lt;&lt;"在ShenA中返回code"&lt;&lt;endl;
	}
	void Setcode()
	{
		cout&lt;&lt;"在ShenA中设置code"&lt;&lt;endl;
	}
};

class ShangAcode:public Icode
{
public:
	void Getcode()
	{
		cout&lt;&lt;"在ShangA中返回code"&lt;&lt;endl;
	}
	void Setcode()
	{
		cout&lt;&lt;"在ShangA中设置code"&lt;&lt;endl;
	}
};


class Iindex
{
public:
	virtual void Getindex()=0;
	virtual void Setindex()=0;
};

class ShenAindex:public Iindex
{
public:
	void Getindex()
	{
		cout&lt;&lt;"在ShenA中返回index"&lt;&lt;endl;
	}
	void Setindex()
	{
		cout&lt;&lt;"在ShenA中设置index"&lt;&lt;endl;
	}
};

class ShangAindex:public Iindex
{
public:
	void Getindex()
	{
		cout&lt;&lt;"在ShangA中返回index"&lt;&lt;endl;
	}
	void Setindex()
	{
		cout&lt;&lt;"在ShangA中设置index"&lt;&lt;endl;
	}
};

class IFactory
{
public:
	virtual Icode *CreateCode()=0;
	virtual Iindex *CreateIndex()=0;
};

class ShenAFactory:public IFactory
{
public:
	Icode *CreateCode() 
	{
		return new ShenAcode();
	}
	Iindex *CreateIndex() 
	{
		return new ShenAindex();
	}
};

class ShangAFactory:public IFactory
{
public:
	Icode *CreateCode()
	{
		return new ShangAcode();
	}
	Iindex *CreateIndex() 
	{
		return new ShangAindex();
	}
};

/*************************************************************/

class DataShangA
{
private:
	static string stock;
	//string stock="ShangA";
public:
	static Icode *CreateCode()
	{
		if(stock=="ShangA")
		{
			return new ShangAcode();
		}
		else if(stock=="ShenA")
		{
			return new ShenAcode();
		}
	}
	static Iindex *CreateIndex()
	{
		if(stock=="ShangA")
		{
			return new ShangAindex();
		}
		else if(stock=="ShenA")
		{
			return new ShenAindex();
		}	
	}
};
string DataShangA::stock="ShenA";

/*************************************************************/


int main()
{
	//IFactory *factory=new ShenAFactory();
	IFactory *factory;
	Icode *code;
	Iindex *index;

	factory=new ShangAFactory();
	code=factory-&gt;CreateCode();
	index=factory-&gt;CreateIndex();
	
	code-&gt;Getcode();
	code-&gt;Setcode();
	index-&gt;Getindex();
	index-&gt;Setindex();


	code=DataShangA::CreateCode();
	index=DataShangA::CreateIndex();

	code-&gt;Getcode();
	code-&gt;Setcode();
	index-&gt;Getindex();
	index-&gt;Setindex();

		return 0;
}&lt;/span&gt;
Abstract Factory模式优缺点总结：

优点：
1.它分离了具体的类
2.它使得易于交换产品系列
3.它有利于产品的一致性
缺点：
1.难以支持新种类的产品










一、简单工厂模式：
        简单工厂模式是工厂模式中最简单的一种，他可以用比较简单的方式隐藏创建对象的细节，一般只需要告诉工厂类所需要的类型，工厂类就会返回需要的产品类，但客户端看到的只是产品的抽象对象，无需关心到底是返回了哪个子类。客户端唯一需要知道的具体子类就是工厂子类。除了这点，基本是达到了依赖倒转原则的要求。
        假如，我们不用工厂类，只用CreateOperate和它的子类，那客户端每次使用不同的子类的时候都需要知道到底是用哪一个子类，当类比较少的时候还没什么问题，但是当类比较多的时候，管理起来就非常的麻烦了，就必须要做大量的替换，一个不小心就会发生错误。而使用了工厂类之后，就不会有这样的问题，不管里面多少个类，我只需要知道类型号即可。不过，这里还有一个疑问，那就是如果我每次用工厂类创建的类型都不相同，这样修改起来的时候还是会出现问题，还是需要大量的替换。所以简单工厂模式一般应该于程序中大部分地方都只使用其中一种“产品”，工厂类也不用频繁创建产品类的情况。这样修改的时候只需要修改有限的几个地方即可。
       看了许多工厂模式的例子，觉得加减乘除运算工厂模式最能直观易懂的讲解工厂模式，当需要加法类的时候，调用工厂类的CreateOperate()，要指定生产相应的“产品”，见类图：
                                 



&lt;span style="font-size:14px;"&gt;#include &lt;iostream&gt;
#include &lt;string&gt;
using namespace std;

class Operation
{
public:
	double numberA, numberB;
	virtual double  getResult()
	{
		return 0;
	}
};

class addOperation :public Operation
{
	double getResult()
	{
		return numberA + numberB;
	}
};


class subOperation :public Operation
{
	double getResult()
	{
		return numberA - numberB;
	}
};

class mulOperation :public Operation
{
	double getResult()
	{
		return numberA*numberB;
	}
};

class divOperation :public Operation
{
	double getResult()
	{
		return numberA / numberB;
	}
};

class operFactory
{
public:
	static Operation *createOperation(char c)
	{
		switch (c)
		{
		case '+':
			return new addOperation;
			break;

		case '-':
			return new subOperation;
			break;

		case '*':
			return new mulOperation;
			break;

		case '/':
			return new divOperation;
			break;
		}
	}
};

int main()
{
	Operation *oper = operFactory::createOperation('+');
	//Operation *oper = operFactory::createOperation('-');
	//Operation *oper = operFactory::createOperation('*');
	//Operation *oper = operFactory::createOperation('/');
	oper-&gt;numberA = 9;
	oper-&gt;numberB = 99;
	cout &lt;&lt; oper-&gt;getResult() &lt;&lt; endl;
	return 0;
}&lt;/span&gt;







调用工厂，需要createOperator("/")，就能返回除法运算符。
　　优点：客户端不需要修改代码。
　　缺点： 当需要增加新的运算类的时候，不仅需新加运算类，还要修改工厂类，违反了开闭原则。

二、工厂方法模式：
  UML类图如下：
                                   

&lt;span style="font-size:14px;"&gt;#include &lt;iostream&gt;
#include &lt;string&gt;
using namespace std;

class Operation
{
public:
	double numberA,numberB;
	virtual double  getResult()
	{
		return 0;
	}
};

class addOperation:public Operation
{
	double getResult()
	{
		return numberA+numberB;
	}
};

 
class subOperation:public Operation
{
	double getResult()
	{
		return numberA-numberB;
	}
};

class mulOperation:public Operation
{
	double getResult()
	{
		return numberA*numberB;
	}
};

class divOperation:public Operation
{
	double getResult()
	{
		return numberA/numberB;
	}
};

class IFactory
{
public:
	virtual Operation *createOperation()=0;
};

class AddFactory:public IFactory
{
public:
	static Operation *createOperation()
	{
		return new addOperation();
	}
};


class SubFactory:public IFactory
{
public:
	static Operation *createOperation()
	{
		return new subOperation();
	}
};

class MulFactory:public IFactory
{
public:
	static Operation *createOperation()
	{
		return new mulOperation();
	}
};

class DivFactory:public IFactory
{
public:
	static Operation *createOperation()
	{
		return new divOperation();
	}
};

int main()
{
	Operation *oper=MulFactory::createOperation();
	oper-&gt;numberA=9;
	oper-&gt;numberB=99;
	cout&lt;&lt;oper-&gt;getResult()&lt;&lt;endl;
	return 0;
}&lt;/span&gt;Simple Factory模式优缺点总结：


这个和简单工厂有区别，简单工厂模式只有一个工厂，工厂方法模式对每一个产品都有相应的工厂
　 
Simple Factory模式优点：增加一个运算类（例如N次方类），只需要增加运算类和相对应的工厂，两个类，不需要修改工厂类。
　　Simple Factory模式缺点：增加运算类，会修改客户端代码，工厂方法只是把简单工厂的内部逻辑判断移到了客户端进行。




          文章作者Juan Pablo Sarmiento收集了60个较为实用、高效的工具资源库，可以帮助开发者快速创建各种Web
 App和移动App。这些资源的特点是：简单、便捷、免费、高效、功能多。当你独自一人需要在短期内构建一个产品的时候，这些起关键作用的工具定会给你留下更深刻的印象。（以下是编译内容）

下面所列举的就是近期比较受欢迎的资源集合：

1、Mmenu：这个jQuery插件能够为移动网站创造出灵活的、类似App的滑动菜单。



2、Fabric
 textures：这5个布料纹理的背景可以以JPG的格式免费下载。



3、Fitgrd：这是一个轻量级的、看起来很酷的响应式网格资源。



 

 

4、Horizontal
 menu：这是一个水平的滑出式菜单，还带有一个基于网格的子菜单。



5、Upload form：这是一个微型的Ajax上传表单。



6、Minimalistic（免费供个人使用）：Minimalistic是一个小型的WordPress主题，带有很好用的绘画技巧模板。



7、Mobile application
 UI kit

 

：这个移动App UI工具包可供PSD格式下载。

 



8、Flat icon set：几乎没有人会嫌平面图标集太多用不完吧，这里有很多，随你选。



9、Flatilicious
 icons：毫不掩饰的说，这里的icon数量是最多的，最多的，最多的。



10、Fries：在这里，可以使用HTML、CSS和JavaScript来创建Android类型的UI。

 

 



 

11、Linecons：这里包含48个完全可伸缩矢量图标。



12、Windows：这是一个jQuery插件，用在全屏滚动窗口上。



13、Slider：这是一个很有用的、免费的滑块，带有流畅的滑动背景。



14、Transit

 

：超级顺当的CSS转换效果，还可以随意的切换为jQuery。

 



15、FastCache：很明显，这曾经是最快的PHP对象缓存系统。



16、Beag：这是一套高效实用的移动App
 UI组件。



17、Mixitup：这是一个CSS3和jQuery过滤器，另外也是一个分类插件。



 

 

18、Pricing table：它是一个为设计师和开发者准备的定价表。



19、Cesium：这是一款JavaScript类库，用于在Web浏览器上创建3D地球。



20、Bootstrap magic：使用这一资源就可以创建你自己的Twitter
 Bootstrap主题。



21、Map icons designer：这些都是Google地图上的图标，适用于各种地图App。

 

 



 

22、Transparent UI
 kit：这是一个在PSD里的分层UI工具包。



23、Topcoat：这是一个CSS工具包用于创建轻便的Web
 Apps。



24、Controls settings：一个较为实用的PSD平面控制面板。



 

25、CSS modal：这是一个用纯CSS语言创建的模拟动态下的建设者。

 



26、Sassaparilla：这是利用一个较快的方式来启动响应式设计项目的资源。



27、Beach UI：这是一种PSD形式的微型UI工具包。



28、Retina-ready
 menu：这是一个响应式的触摸屏菜单，带有供不同浏览器使用的三种布局。

 

 



 

29、Flexisel：这是一个响应式的旋转木马jQuery插件。



30、W2UI library：作为连接jQuery插件的组件，它的主要设计目的是用在前段技术上的。



31、Dark
 Accordion：这是一个以PSD文件形式具有现代风格的导航菜单。



32、Skel.js

 

：一个用于创建响应式站点的前端框架。

 



33、Magnific Popup：它是一个响应式的jQuery光箱特效插件。



34、Nice Less buttons：这是一个用CSS语言制作的按钮组集。



35、Multipurpose website
 template：多用途的、设计精美的PSD。



 

 

36、Two.js：这是一个2D绘图API。



37、Flat UI login form：这是一个流行的登录和注册表单格式。



38、Ecommerce
 flat：这是一个PSD格式的、扁平的商业电子UI工具包。



39、Brand
 identity：它是一个分层的PSD，用于呈现出商标和标识设计。

 

 



 

40、Animation fill code：这是一个为CSS关键帧动画添加必要代码的App。



41、Countable.js：这个脚本可以添加实时段落，文字和字符数到一个html元素。



42、Github Archiveroom：可以在3D形式下探索Github档案文件。



 

 

43、iPhone
 5 flat：这是一个扁平结构的iPone 5实物模型。

 



44、HoverMenu：这是个较为简单的可访问菜单，在你网页的任何地方都可以使用。



45、SLY：这是一个JavaScript类库，只可以向一个方向滚动。



46、Flatstrap：这是一个引导程序的外表形式，使用了扁平设计技术。

 

 



 

47、Packery：一个用于解决装箱布局的JavaScript插件。



48、Snaps theme：一个用于WordPress的免费组合主题。



49、Easy responsive
 tabs：这是一个用来优化水平或垂直标签的jQuery插件。



50、CSS social share button

 

：这是一个扁平式的社交工具提示按钮。

 



51、Knockout：使用Knockout可以简化动态的JavaScript
 UI运行MVVM图案的过程。



52、Magazine mockup：这是一个PSD模板，是展示工程项目的理想选择。



53、Weather icons：PSD格式的气象图标组合，相当有实用性。



 

 

54、Flat icons set：这是另外一个图标组集，以.png文件的格式传输。



55、Social media
 icon set：免费的社交媒体图标组集。



56、Simple icons：这也是一个免费的图标组集。



57、Icon hover
 effects：圆形图标悬浮效果，可以和CSS转换。

 



 

58、Percentage bars：使用CSS技术构建的扁平化风格的百分比滚动条。



59、Twitter UI：这是一个用CSS和HTML构建的Twitter用户UI。 



60、Music widget：这是一个带有一些简单动画的扁平窗口音乐播放器。






Singleton模式来源：         
       单例模式是设计模式中最简单的形式之一。这一模式的目的是使得类的一个对象成为系统中的唯一实例。要实现这一点，可以从客户端对其进行实例化开始。因此需要用一种只允许生成对象类的唯一实例的机制，"阻止"所有想要生成对象的访问。使用工厂方法来限制实例化过程。这个方法应该是静态方法(类方法)，因为让类的实例去生成另一个唯一实例毫无意义。
Singleton模式作用：
       对于系统中的某些类来说，只有一个实例很重要，例如，一个系统中可以存在多个打印任务，但是只能有一个正在工作的任务;一个系统只能有一个窗口管理器或文件系统;一个系统只能有一个计时工具或ID(序号)生成器。如在Windows中就只能打开一个任务管理器。如果不使用机制对窗口对象进行唯一化，将弹出多个窗口，如果这些窗口显示的内容完全一致，则是重复对象，浪费内存资源;如果这些窗口显示的内容不一致，则意味着在某一瞬间系统有多个状态，与实际不符，也会给用户带来误解，不知道哪一个才是真实的状态。因此有时确保系统中某个对象的唯一性即一个类只能有一个实例非常重要。
      显然单例模式的要点有三个;一是某个类只能有一个实例;二是它必须自行创建这个实例;三是它必须自行向整个系统提供这个实例。
 从具体实现角度来说，就是以下三点:一是单例模式的类只提供私有的构造函数，二是类定义中含有一个该类的静态私有对象，三是该类提供了一个静态的公有的函数用于创建或获取它本身的静态私有对象。
      单例模式会阻止其他对象实例化其自己的单例对象的副本，从而确保所有对象都访问唯一实例
Singleton模式示例代码如下：

class CSingleton

{
private:
CSingleton() //构造函数是私有的
{

}
public:
static CSingleton * GetInstance()
{
static CSingleton *m_pInstance;
if(m_pInstance == NULL) //判断是否第一次调用
m_pInstance = new CSingleton();
return m_pInstance;
}
         用户访问唯一实例的方法只有GetInstance()成员函数。如果不通过这个函数，任何创建实例的尝试都将失败，因为类的构造函数是私有的。GetInstance()使用懒惰初始化，也就是说它的返回值是当这个函数首次被访问时被创建的。

         一般Singleton模式通常有三种形式:
第一种形式:懒汉式，也是常用的形式，什么时候用，再创建，不会出现空耗内存的情况。

/*testcode1*/
#include &lt;iostream&gt;
using namestd std;
class Singleton{
public:
    static  Singleton&amp; getInstance(void){
         if(! s_instance)
            s_instance = new Singleton;
          return *s_instance;//不用Singleton,就不创建。
}
private:
    Singleton (void){}
    Singleton(const Singleton&amp;);
    Singleton&amp; operator=(const Singleton&amp;);
    static Singleton* s_instance;
};
Singleton* Singleton::s_instance=NULL;
int main (void){
     Singleton&amp; s1 = Singleton::getInstance ();
     Singleton&amp; s2 = Singleton::getInstance ();
     cout &lt;&lt; &amp;s1 &lt;&lt; ' '&lt;&lt; &amp;s2 &lt;&lt; ' '&lt;&lt; endl;
     return  0;
}


第二种形式:饿汉式,进程一起来，s_instance就存在了，先于main函数构造。
#include &lt;iostream&gt;
using namestd std;
class Singleton{
public:
    static  Singleton&amp; getInstance(void){
          return s_instance;
}
private:
    Singleton (void){}
    Singleton(const Singleton&amp;);
    Singleton&amp; operator=(const Singleton&amp;);
    static Singleton s_instance;
};
Singleton Singleton::s_instance;
int main (void){
     Singleton&amp; s1 = Singleton::getInstance ();
     Singleton&amp; s2 = Singleton::getInstance ();
     cout &lt;&lt; &amp;s1 &lt;&lt; ' '&lt;&lt; &amp;s2 &lt;&lt; ' '&lt;&lt; endl;
     return  0;
}

Singleton模式适用场景：

        单例模式常常与工厂模式结合使用，因为工厂只需要创建产品实例就可以了，在多线程的环境下也不会造成任何的冲突，因此只需要一个工厂实例就可以了。

Singleton模式优缺点总结：

Singleton模式优点：
1.减少了时间和空间的开销（new实例的开销）。
2.提高了封装性，使得外部不易改动实例。
 Singleton模式缺点：
1.懒汉式是以时间换空间的方式。
2.饿汉式是以空间换时间的方式。
Singleton模式的使用总结：

         从上可见，单例模式是最简单的一种设计模式，在实际应用中，有一些对象其实只需要一个，比如：线程池，缓存，对话框，处理偏好设置和注册表的对象，日志对象，充当打印机，显卡等设备的驱动程序对象。这些对象只能够拥有一个实例，如果创建出了多个实例，就会导致一些程序的问题。程序的行为异常，资源使用的过量，或者导致不一致的结果。常用来管理共享的资源，比如数据库的连接或者线程池。




 


环境准备：

硬件要求：50G硬盘 8G内存 4核CPU

软件要求：Linux操作系统：CentOS6.5_X64
  mongodb-linux-x86_64-2.6.10.tgz

目的：

安装配置MongoDB数据库
具体操作：
一、关闭SElinux、配置防火墙
1.vi /etc/selinux/config
#SELINUX=enforcing #注释掉
#SELINUXTYPE=targeted #注释掉
SELINUX=disabled #增加
:wq!  #保存退出
setenforce 0 #使配置立即生效

2.vi /etc/sysconfig/iptables  #编辑
-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 27017 -j ACCEPT  #允许27017端口通过防火墙
:wq! #保存退出
/etc/init.d/iptables restart #重启防火墙使配置生效

二、安装MongoDB
（1）.下载MongoDB安装包：mongodb-linux-x86_64-2.6.10.tgz
（按照以下官网链接中的流程安装MongoDB，需要安装的版本是 v2.6.9：
http://docs.mongodb.org/manual/tutorial/install-mongodb-on-red-hat/）

# tar -zxvf mongodb-linux-x86_64-2.6.10.tgz
在/root下建立mongodb运行时的文件夹并放入mongodb组件
# mkdir -p mongodb
# cp -R -n mongodb-linux-x86_64-2.6.10/ mongodb
mv mongodb-linux-x86_64-2.6.10 /usr/local/mongodb  #移动解压文件夹到MongoDB安装目录
mkdir  -p  /data/mongodb/mongodb_data/                  #创建MongoDB数据库存放路径
mkdir  -p  /data/mongodb/mongodb_log/                        #创建MongoDB数据库日志存放路径

设置环境变量


# vi ~/.bashrc
在该文件最后一行添加：
export PATH=/usr/local/mongodb/mongodb-linux-x86_64-2.6.10/bin:$PATH
保存并退出文件
输入命令使环境变量生效：
# source ~/.bashrc
建立默认数据存储位置：
mkdir-p /data/db
建立mongodb日志存储位置：
mkdir 
/usr/local/mongodb/logs
启动数据库命令：
# mongod--logpath=/usr/local/mongodb/logs/mongodb.log --fork
#mongod --logpath=/usr/local/mongodb/logs/mongodb.log --fork --nojournal(第二种启动方法：有，之前起mongodb的时候journal也一起启动，这是mongodb在宕机时回复写操作用的，但是要多占一些硬盘，内存资源。
（2）.启动MongoDB
/usr/local/mongodb/bin/mongod --port 27017 --fork --dbpath=/data/mongodb/mongodb_data/ --logpath=/data/mongodb/mongodb_log/mongodb.log --logappend

about to fork child process, waiting until server is ready for connections.
forked process: 2102
child process started successfully, parent exiting


安装完毕后数据库默认监听端口为27017，数据库默认存储路径为/var/lib/mongo
安装完毕后使用如下命令启动MongoDB：
#service mongod start

（3）.查看你启动状态

netstat -ntpl    #查看MongoDB是否启动

用这个命令查看启动状态：ps aux | grep mongod





（4）.添加系统服务和守护进程


配置MongoDB开机自启动：chkconfig
 mongod on    


# echo“/usr/local/mongodb/mongodb-linux-x86_64-2.6.10/bin/mongod--logpath=/root/mongodb/logs/mongodb.log
 --fork” &gt;&gt; /etc/rc.local
1）.修改mongodb系统文件设置开机自启动
       i. 先停止mongodb服务：service mongod stop 
       ii. 重新启动mongodb服务，用以下命令：
#mongod --logpath=/usr/local/mongodb/logs/mongodb.log --fork –nojournal
      iii. 最后修改下开机启动文件里mongodb的部分：vi /etc/rc.local，在这行添加：/usr/local/mongodb/mongodb-linux-x86_64-2.6.10/bin/mongod--logpath=/usr/local/mongodb/logs/mongodb.log--fork，后面加上--nojournal，保存。

（5）.测试MongoDB数据库



a)  执行导入命令：
    # 
/usr/local/mongodb/mongodb-linux-x86_64-2.6.10/bin/mongorestore  --db project /root/newDump/project
b)  导入完以后，进行如下操作验证数据数否有了：
# mongo登入数据库
# &gt;useproject
# &gt;db.stock.getIndexes()  (备注：如果有乱码，设置shell终端为utf8)，若如下图所示，表明导入数据成功。








MongoDB数据导出、导入及索引建立
（1）    数据导出——每个表一个文件：
# mongoexport --db project --collection 表名 --out 输出文件名，如：
# mongoexport --db project --collectionstock --out stock.json，导出所有F10数据到文件/root/stock.json中
（2）    数据导入——每个表一个文件：
# mongoimport --db users --collection 表名 --file输入文件名
（3）    数据备份：
一个表：
# mongodump --db project --collection 表名 --out 目的路径
整个库：
# mongodump --db project--out 目的路径
在目的路径生成以数据库名命名的备份文件夹，默认备份路径在用户主文件夹的dump目录下
（4）    数据恢复：
一个表:
# mongorestore  --db project --collection 表名备份路径
备份路径需要指向一个表的.bson文件，如：#mongorestore  --db project --collection news_cjyw/…/dump/project/news_cjyw.bson
整个库：
# mongorestore  --db project备份路径
备份路径需要指向以数据库名命的文件夹，如：
# mongorestore  --db project /…/…/dump/project
备份路径中对应的.bson文件与.metadata文件需要同时存在
（5）    建立索引：（用dump/restore方式恢复数据不用重新建立索引，export/import方式需要重新建立索引）
进入数据库：
# mongo
&gt;use project
&gt;db.stock.ensureIndex({“股票代码”:1，“lastChange”:-1}) //这个索引必须建，两个参量的顺序不能错
&gt;exit

      在我们的日常开发工作中，经常用到，如互联网金融行业的定时批处理业务，核心系统定时状态检测及状态日志生成，数据库文件的定时入库，定时生成批量付息，交易系统文件定时升级更新，日终交易文件上传备份至文件服务器等等，都会用到这个命令，从而大大减少工作量，提高工作效率，现在总结一下它的常用方法和参数配置，与大家分享。(也不是最全的，如有补充和建议，欢迎留言,也可以加入我们的IT技术交流群
 62775887).
一.  Crontab介绍
      crontab命令的功能就是让系统在指定的时间，去执行某个指定的工作，我们可以使用crontab指令来管理cron机制。
1.1 /etc/crontab 文件
      在/etc目录下有一个crontab文件，这里存放有系统运行的一些调度程序。每个用户可以建立自己的调度crontab。
如：
[root@dave ~]# cat /etc/crontab
SHELL=/bin/bash
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
HOME=/
# run-parts
01 * * * * root run-parts /etc/cron.hourly
02 4 * * * root run-parts /etc/cron.daily
22 4 * * 0 root run-parts /etc/cron.weekly
42 4 1 * * root run-parts /etc/cron.monthly
通过crontab  -e命令给当前系统添加定时任务，如下图所示，增加一个定时文件采集任务：
    
通过crontab  -l命令查看当前系统的定时任务，如下图所示，系统有一个定时文件采集任务：
    
如果crontab  -l出现如上命令行，则表示添加任务成功。  
1.2/etc/cron.deny 和/etc/cron.allow文件
/etc/cron.deny 表示不能使用crontab命令的用户
/etc/cron.allow 表示能使用crontab的用户。
如果两个文件同时存在，那么/etc/cron.allow优先。
如果两个文件都不存在，那么只有超级用户可以安排作业。
每个用户都会生成一个自己的crontab文件。这些文件在/var/spool/cron目录下：
如：
[root@dave ~]# cd /var/spool/cron
[root@dave cron]# ls
oracle  root 
我们直接查看这个文件，里面的内容和对应用户显示的crontab -l一致。
[root@dave cron]# cat oracle
00 6 * * * /u02/scripts/del_st_archive.sh&gt;/u02/scripts/del_st_arch.log 2&gt;&amp;1
[root@dave cron]# cat root
0 12 * * * /root/bin/sync-clock.sh
[root@dave cron]#  
二.  Crontab使用说明
2.1  Crontab语法
usage:  crontab[-u user] file
        crontab[-u user] [ -e | -l | -r ]
               (default operation is replace, per 1003.2)
        -e      (edit user's crontab)
        -l      (list user's crontab)
        -r      (delete user's crontab)
        -i      (prompt before deleting user's crontab)
        -s     (selinux context)
      其中，file是命令文件的名字。如果在命令行中指定了这个文件，那么执行crontab命令，则将这个文件拷贝到crontabs目录下；如果在命令行中没有制定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将他们也存放在crontab目录下。
     “内事不决问百度，外事不决问谷歌，在Linux/Uinux系统里，我们就要问man了”.
帮助：
[root@dave ~]# man crontab
CRONTAB(1)                                                          CRONTAB(1)
NAME
       crontab -maintain crontab files for individual users (ISC Cron V4.1)
SYNOPSIS
       crontab[-u user] file
       crontab[-u user] [-l | -r | -e] [-i] [-s] 

DESCRIPTION
      Crontab  is the program used toinstall, deinstall or list the tables used to drive the cron(8) daemon in ISCCron.  Each user can have their owncrontab,  and  though these are  files in /var/spool/ , theyare not intended
 to be edited directly. For SELinux in mls mode can be evenmore crontabs  -  for each  range.  For more  see selinux(8).
       If  the cron.allow  file  exists, then  you must be listed thereinin order to be allowed to use this command. If  the  cron.allow file  does  not exist  but  the cron.deny file  does  exist, then you must not be listed
 in the cron.deny file in order to use thiscommand.  If neither of these filesexists, only the  super  user will be allowed to use this command.
  
OPTIONS
       -u     It specifies  the name of the userwhose crontab is to be tweaked.  Ifthis  option is not given, crontabexamines "your" crontab, i.e., the crontab  of the  person executing the command.  Note thatsu(8) can confuse
 crontab and               that if you are running inside of su(8)you should always use the -u  option               for  safety?ˉs sake.  The first form of this command is used toinstall a new               crontab fromsome named file or standard input if
 the pseudo-filename "-" is               given.
       -l     The current crontab will be displayed onstandard output.
       -r     The current crontab will be be removed.
       -e     This option  is used to edit thecurrent crontab using the editor specified by the VISUAL or EDITOR environmentvariables.  After you exit from theedi-tor, the modified crontab will be installed automatically.
       -i     This option  modifies the -r option toprompt the user for a ?ˉy/Y?ˉ response before actually removing the crontab.
       -s     It will append the current SELinuxsecurity context string as  an  MLS_LEVEL setting  to the  crontab file before editing /replacement occurs - see the documentation of MLS_LEVEL in crontab(5).
  
SEE ALSO
      crontab(5), cron(8)
FILES
      /etc/cron.allow
      /etc/cron.deny
STANDARDS
       Thecrontab command conforms to IEEE Std1003.2-1992 (????POSIX?ˉ?ˉ).  This new command syntax  differs  from previous versions of Vixie Cron, as well as from the classic
       SVR3syntax.
DIAGNOSTICS
       A fairlyinformative usage message appears if you run it with a bad command line.
AUTHOR
       Paul Vixie&lt;vixie@isc.org&gt;
4th Berkeley Distribution       16 Januar 2007                      CRONTAB(1)
2.2  Crontab 格式说明
      我们可以用crontab -e添加要执行的命令。命令执行的结果，无论是标准输出还是错误输出，都将以邮件形式发给用户。
    添加的命令必须以如下格式：
   * * * * */command path
     前五个字段可以取整数值，指定何时开始工作，第六个域是字符串，即命令字段，其中包括了crontab调度执行的命令。各个字段之间用spaces和tabs分割。
前5个字段分别表示：
       分钟：0-59
       小时：1-23
       日期：1-31
       月份：1-12
       星期：0-6（0表示周日）
还可以用一些特殊符号：
       *：表示任何时刻
       ,：　表示分割
　　－：表示一个段，如第二端里： 1-5，就表示1到5点
       /n : 表示每个n的单位执行一次，如第二段里，*/1,就表示每隔1个小时执行一次命令。也可以写成1-23/1.
一些示例：
00 8,12,16 * * * /data/app/scripts/monitor/df.sh
30 2 * * */data/app/scripts/hotbackup/hot_database_backup.sh
10 8,12,16 * * */data/app/scripts/monitor/check_ind_unusable.sh
10 8,12,16 * * */data/app/scripts/monitor/check_maxfilesize.sh
10 8,12,16 * * * /data/app/scripts/monitor/check_objectsize.sh
43 21 * * * 21:43 执行
15 05 * * * 05:15执行
0 17 * * *       17:00 执行
0 17 * * 1       每周一的 17:00执行
0,10 17 * * 0,2,3 每周日,周二,周三的
 17:00和 17:10执行
0-10 17 1 * *    毎月1日从 17:00到7:10毎隔1分钟执行
0 0 1,15 * 1     毎月1日和 15日和一日的
 0:00 执行
42 4 1 * * 毎月1日的 4:42分执行
0 21 * * 1-6周一到周六 21:00执行
0,10,20,30,40,50 * * * *　每隔10分执行
*/10 * * * *     每隔10分执行
* 1 * * *        从1:0到1:59每隔1分钟执行
0 1 * * *        1:00 执行
0 */1 * * *      毎时0分每隔1小时执行
0 * * * *        毎时0分每隔1小时执行
2 8-20/3 * * *    8:02,11:02,14:02,17:02,20:02执行
30 5 1,15 * *     1日和 15日的 5:30执行
2.3  &amp; 后台执行命令
      当在前台运行某个作业时，终端被该作业占据；而在后台运行作业时，它不会占据终端。可以使用&amp;命令把作业放到后台执行。
    如：
    30 2 * * */data/app/scripts/hotbackup/hot_database_backup.sh &amp;
     在后台运行作业时要当心：需要用户交互的命令不要放在后台执行，因为这样你的机器就会在那里傻等。
     不过，作业在后台运行一样会将结果输出到屏幕上，干扰你的工作。如果放在后台运行的作业会产生大量的输出，最好使用下面的方法把它的输出重定向到某个文件中：
    如：
              command &gt;out.file 2&gt;&amp;1 &amp;
    在这个例子中，2&gt;&amp;1表示所有的标准输出和错误输出都将被重定向到一个叫做out.file的文件中。
2.4  2&gt;&amp;1 含义
 先看一个例子：
0 2 * * * /u01/test.sh &gt;/dev/null 2&gt;&amp;1 &amp;
  
      这句话的意思就是在后台执行这条命令，并将错误输出2重定向到标准输出1，然后将标准输出1全部放到/dev/null文件，也就是清空。
在这里有有几个数字的意思：
       0表示键盘输入
       1表示标准输出
       2表示错误输出.
我们也可以这样写：
0 2 * * * /u01/test.sh &gt;/u01/out.file &amp;  --这里没写，默认是1
0 2 * * * /u01/test.sh 1&gt;/u01/out.file &amp;
0 2 * * * /u01/test.sh 2&gt;/u01/out.file &amp;
0 2 * * * /u01/test.sh 2&gt;/u01/out.file  2&gt;&amp;1&amp;
     将tesh.sh命令输出重定向到out.file,即输出内容不打印到屏幕上，而是输出到out.file文件中。
     2&gt;&amp;1 是将错误输出重定向到标准输出。然后将标准输入重定向到文件out.file。
&amp;1 表示的是文件描述1，表示标准输出，如果这里少了&amp;就成了数字1，就表示重定向到文件1。
  &amp; ：后台执行
  测试：
ls 2&gt;1 ：不会报没有2文件的错误，但会输出一个空的文件1；
ls xxx 2&gt;1：没有xxx这个文件的错误输出到了1中；
ls xxx 2&gt;&amp;1：不会生成1这个文件了，不过错误跑到标准输出了；
ls xxx &gt;out.txt 2&gt;&amp;1 == ls xxx 1&gt;out.txt2&gt;&amp;1； 因为重定向符号&gt;默认是1，这句就把错误输出和标准输出都传到out.txt文件中。
2.5  2&gt;&amp;1写在后面的原因
      格式：command &gt;file 2&gt;&amp;1   == command  1&gt; file 2&gt;&amp;1
      首先是command &gt;file将标准输出重定向到file中，
 2&gt;&amp;1 是标准错误拷贝了标准输出，也就是同样被重定向到file中，最终结果就是标准输出和错误都被重定向到file中。
      如果改成： command2&gt;&amp;1 &gt;file
     2&gt;&amp;1 标准错误拷贝了标准输出的行为，但此时标准输出还是在终端。&gt;file后输出才被重定向到file，但标准错误仍然保持在终端。

        在我们日常跨系统开发和服务部署上，Linux find文件查找命令与grep文件内容查找命令是经常用的，现整理了一下这两种命令的常用方法和参数设置，与大家共享！(也不是最全的，如有补充和建议，欢迎留言,也可以加入我们的IT技术交流群 62775887).
        在使用linux时，经常需要进行文件查找。其中查找的命令主要有find和grep。两个命令是有区别：
(1)find命令是根据文件的属性进行查找，如文件名，文件大小，所有者，所属组，是否为空，访问时间，修改时间等。
(2)grep是根据文件的内容进行查找，会对文件的每一行按照给定的模式(patter)进行匹配查找。
一.find命令
基本格式：find  pathexpression
1.按照文件名查找
(1)find/ -name httpd.conf　　#在根目录下查找文件httpd.conf，表示在整个硬盘查找
(2)find /etc -name httpd.conf　　#在/etc目录下文件httpd.conf
(3)find /etc -name '*srm*'　　#使用通配符*(0或者任意多个)。表示在/etc目录下查找文件名中含有字符串‘srm’的文件
(4)find . -name 'srm*' 　　#表示当前目录下查找文件名开头是字符串‘srm’的文件
2.按照文件特征查找
(1)find/ -amin -10 　　# 查找在系统中最后10分钟访问的文件(access time)
(2)find / -atime -2　　 # 查找在系统中最后48小时访问的文件
(3)find / -empty 　　# 查找在系统中为空的文件或者文件夹
(4)find / -group cat 　　# 查找在系统中属于 group为cat的文件
(5)find / -mmin -5 　　# 查找在系统中最后5分钟里修改过的文件(modify time)
(6)find / -mtime -1 　　#查找在系统中最后24小时里修改过的文件
(7)find / -user fred 　　#查找在系统中属于fred这个用户的文件
(8)find / -size +10000c　　#查找出大于10000000字节的文件(c:字节，w:双字，k:KB，M:MB，G:GB)
(9)find / -size -1000k 　　#查找出小于1000KB的文件
3.使用混合查找方式查找文件
参数有： ！，-and(-a)，-or(-o)。
(1)find/tmp -size +10000c -and -mtime +2 　　#在/tmp目录下查找大于10000字节并在最后2分钟内修改的文件
(2)find / -user fred -or -user george 　　#在/目录下查找用户是fred或者george的文件文件
(3)find /tmp ! -user panda　　#在/tmp目录中查找所有不属于panda用户的文件


二、grep命令
基本格式：find expression
1.主要参数
[options]主要参数：
－c：只输出匹配行的计数。
－i：不区分大小写
－h：查询多文件时不显示文件名。
－l：查询多文件时只输出包含匹配字符的文件名。
－n：显示匹配行及行号。
－s：不显示不存在或无匹配文本的错误信息。
－v：显示不包含匹配文本的所有行。
pattern正则表达式主要参数：
　　　　\：忽略正则表达式中特殊字符的原有含义。
　　　　^：匹配正则表达式的开始行。
　　　　$: 匹配正则表达式的结束行。
　　　　\&lt;：从匹配正则表达式的行开始。
　　　　\&gt;：到匹配正则表达式的行结束。
　　　　[ ]：单个字符，如[A]即A符合要求。
　　　　[ - ]：范围，如[A-Z]，即A、B、C一直到Z都符合要求。
　　　　.：所有的单个字符。
　　　　* ：有字符，长度可以为0。
2.实例
(1)grep'test' d*　　#显示所有以d开头的文件中包含 test的行
(2)grep ‘test’ aa bb cc 　　 #显示在aa，bb，cc文件中包含test的行
(3)grep ‘[a-z]\{5\}’ aa 　　#显示所有包含每行字符串至少有5个连续小写字符的字符串的行
(4)grep magic /usr/src　　#显示/usr/src目录下的文件(不含子目录)包含magic的行
(5)grep -r magic /usr/src　　#显示/usr/src目录下的文件(包含子目录)包含magic的行
(6)grep-w pattern files ：只匹配整个单词，而不是字符串的一部分(如匹配’magic’，而不是’magical’)。

         在我们实际开发中，快捷键的使用可以大大提高开发调试效率，以下是VC环境开发的一些常用快捷键总结，与大家共享：
VS2008快捷键大全：
Ctrl+m+Crtr+o折叠所有大纲
Ctrl+M+Crtr+P: 停止大纲显示
Ctrl+K+Crtr+C: 注释选定内容
Ctrl+K+Crtr+U: 取消选定注释内容
Ctrl+J : 列出成员智能感知
Shift+Alt+Enter: 切换全屏编辑
Ctrl+B,T / Ctrl+K,K: 切换书签开关
Ctrl+B,N / Ctrl+K,N: 移动到下一书签
Ctrl+B,P: 移动到上一书签
Ctrl+B,C: 清除全部标签
Ctrl+I: 渐进式搜索
Ctrl+Shift+I: 反向渐进式搜索
Ctrl+F: 查找
Ctrl+Shift+F: 在文件中查找
F3: 查找下一个
Shift+F3: 查找上一个
Ctrl+H: 替换
Ctrl+Shift+H: 在文件中替换
Alt+F12: 查找符号(列出所有查找结果)
Ctrl+Shift+V: 剪贴板循环
Ctrl+左右箭头键:一次可以移动一个单词
Ctrl+上下箭头键:滚动代码屏幕，但不移动光标位置。
Ctrl+Shift+L: 删除当前行
Ctrl+M,M: 隐藏或展开当前嵌套的折叠状态
Ctrl+M,L: 将所有过程设置为相同的隐藏或展开状态
Ctrl+E,S: 查看空白
Ctrl+E,W: 自动换行
Ctrl+G: 转到指定行
Shift+Alt+箭头键:选择矩形文本
Alt+鼠标左按钮:选择矩形文本
Ctrl+Shift+U: 全部变为大写
Ctrl+U: 全部变为小写
代码快捷键
Ctrl+Shift+空格键/ Ctrl+K,P: 参数信息
Ctrl+K,I: 快速信息
Ctrl+E,U / Ctrl+K,U: 取消选定注释内容
Ctrl+K,M: 生成方法存根
Ctrl+K,X: 插入代码段
Ctrl+K,S: 插入外侧代码
F12: 转到所调用过程或变量的定义
窗口快捷键
Ctrl+W,W: 浏览器窗口
Ctrl+W,S: 解决方案管理器
Ctrl+W,C: 类视图
Ctrl+W,E: 错误列表
Ctrl+W,O: 输出视图
trl+W,P: 属性窗口
Ctrl+W,T: 任务列表
Ctrl+W,X: 工具箱
Ctrl+W,B: 书签窗口
Ctrl+W,U: 文档大纲
Ctrl+D,B: 断点窗口
Ctrl+D,I: 即时窗口
Ctrl+Tab: 活动窗体切换
Ctrl+Shift+N: 新建项目
Ctrl+Shift+O: 打开项目
Ctrl+Shift+S: 全部保存
Shift+Alt+C: 新建类
Ctrl+Shift+A: 新建项
Shift+Alt+Enter: 切换全屏编辑
Ctrl+B,T / Ctrl+K,K: 切换书签开关
Ctrl+B,N / Ctrl+K,N: 移动到下一书签
Ctrl+B,P: 移动到上一书签
Ctrl+B,C: 清除全部标签
Ctrl+I: 渐进式搜索
Ctrl+Shift+I: 反向渐进式搜索
Ctrl+F: 查找
Ctrl+Shift+F: 在文件中查找
F3: 查找下一个
Shift+F3: 查找上一个
Ctrl+H: 替换
Ctrl+Shift+H: 在文件中替换
Alt+F12: 查找符号(列出所有查找结果)
Ctrl+Shift+V: 剪贴板循环
Ctrl+左右箭头键:一次可以移动一个单词
Ctrl+上下箭头键:滚动代码屏幕，但不移动光标位置。
Ctrl+Shift+L: 删除当前行
Ctrl+M,M: 隐藏或展开当前嵌套的折叠状态
Ctrl+M,L: 将所有过程设置为相同的隐藏或展开状态
Ctrl+M,P: 停止大纲显示
Ctrl+E,S: 查看空白
Ctrl+E,W: 自动换行
Ctrl+G: 转到指定行
Shift+Alt+箭头键:选择矩形文本
Alt+鼠标左按钮:选择矩形文本
Ctrl+Shift+U: 全部变为大写
Ctrl+U: 全部变为小写
 
VS2013常用快捷键总结：
Ctrl+E,D ----格式化全部代码
Ctrl+E,F ----格式化选中的代码
CTRL + SHIFT + B生成解决方案
CTRL + F7 生成编译
CTRL + O 打开文件
CTRL + SHIFT + O打开项目
CTRL + SHIFT + C显示类视图窗口
F4 显示属性窗口
SHIFT + F4显示项目属性窗口
CTRL + SHIFT + E显示资源视图
F12 转到定义
CTRL + F12转到声明
CTRL + ALT + J对象浏览
CTRL + ALT + F1帮助目录
CTRL + F1 动态帮助
F1 帮助
SHIFT + F1当前窗口帮助
CTRL + ALT + F3帮助-搜索
SHIFT + ALT + ENTER全屏显示
CTRL + -向后定位
CTRL + SHIFT + -向前定位
CTRL + F4关闭文档窗口
CTRL + PAGE DOWN光标定位到窗口上方
CTRL + PAGE UP光标定位到窗口下方
CTRL + F6
CTRL + TAB下一个文档窗口
CTRL + SHIFT + F6
CTRL + SHIFT + TAB上一个文档窗口
ALT + F6下一个面板窗口
CTRL + K, CTRL + L取消remark
CTRL + K, CTRL + C注释选择的代码
CTRL + K, CTRL + U取消对选择代码的注释
CTRL + M, CTRL + O折叠代码定义
CTRL + M, CTRL + L展开代码定义
CTRL + DELETE删除至词尾
CTRL + BACKSPACE删除至词头
SHIFT + TAB取消制表符
CTRL + U转小写
CTRL + SHIFT + U转大写
CTRL + SHIFT + END选择至文档末尾
CTRL + SHIFT + HOME选择至文档末尾开始
SHIFT + END选择至行尾
SHIFT + HOME选择至行开始处
SHIFT + ALT + END垂直选择到最后尾
SHIFT + ALT + HOME垂直选择到最前面
CTRL + SHIFT + PAGE UP选择至本页前面
CTRL + SHIFT + PAGE DOWN选择至本页后面
CTRL + END文档定位到最后
CTRL + HOME文档定位到最前
CTRL + A全选
CTRL + W选择当前单词
 
CTRL + G转到…
CTRL + K, CTRL + P上一个标签
CTRL + K, CTRL + N下一个标签
ALT + F10调试-ApplyCodeChanges
CTRL + ALT+ Break停止调试
CTRL + SHIFT + F9 取消所有断点
CTRL + F9允许中断
CTRL + SHIFT + F5调试-重新开始
F5运行调试
CTRL + F5运行不调试
F10跨过程序执行
F11单步逐句执行
CTRL + J列出成员
CTRL + PAGE DOWN下一个视图
CTRL + B格式-粗体
CTRL + SHIFT + T格式-文字缩进
调试快捷键
F6: 生成解决方案
Ctrl+F6: 生成当前项目
F7: 查看代码
Shift+F7: 查看窗体设计器
F5: 启动调试
Ctrl+F5: 开始执行(不调试)
Shift+F5: 停止调试
Ctrl+Shift+F5: 重启调试
F9: 切换断点
Ctrl+F9: 启用/停止断点
Ctrl+Shift+F9: 删除全部断点
F10: 逐过程
Ctrl+F10: 运行到光标处
F11: 逐语句
编辑快捷键
Shift+Alt+Enter: 切换全屏编辑
Ctrl+B,T / Ctrl+K,K: 切换书签开关
Ctrl+B,N / Ctrl+K,N: 移动到下一书签
Ctrl+B,P: 移动到上一书签
Ctrl+B,C: 清除全部标签
Ctrl+I: 渐进式搜索
Ctrl+Shift+I: 反向渐进式搜索
Ctrl+F: 查找
Ctrl+Shift+F: 在文件中查找
F3: 查找下一个
Shift+F3: 查找上一个
Ctrl+H: 替换
Ctrl+Shift+H: 在文件中替换
Alt+F12: 查找符号(列出所有查找结果)
Ctrl+Shift+V: 剪贴板循环
Ctrl+左右箭头键:一次可以移动一个单词
Ctrl+上下箭头键:滚动代码屏幕，但不移动光标位置。
Ctrl+Shift+L: 删除当前行
Ctrl+M,M: 隐藏或展开当前嵌套的折叠状态
Ctrl+M,L: 将所有过程设置为相同的隐藏或展开状态
Ctrl+M,P: 停止大纲显示
Ctrl+E,S: 查看空白
Ctrl+E,W: 自动换行
Ctrl+G: 转到指定行
Shift+Alt+箭头键:选择矩形文本
Alt+鼠标左按钮:选择矩形文本
Ctrl+Shift+U: 全部变为大写
Ctrl+U: 全部变为小写
代码快捷键
Ctrl+J / Ctrl+K,L: 列出成员
Ctrl+Shift+空格键/ Ctrl+K,P: 参数信息
Ctrl+K,I: 快速信息
Ctrl+E,C / Ctrl+K,C: 注释选定内容
Ctrl+E,U / Ctrl+K,U: 取消选定注释内容
Ctrl+K,M: 生成方法存根
Ctrl+K,X: 插入代码段
Ctrl+K,S: 插入外侧代码
F12: 转到所调用过程或变量的定义
窗口快捷键
Ctrl+W,W: 浏览器窗口
Ctrl+W,S: 解决方案管理器
Ctrl+W,C: 类视图
Ctrl+W,E: 错误列表
Ctrl+W,O: 输出视图
Ctrl+W,P: 属性窗口
Ctrl+W,T: 任务列表
Ctrl+W,X: 工具箱
Ctrl+W,B: 书签窗口
Ctrl+W,U: 文档大纲
Ctrl+D,B: 断点窗口
Ctrl+D,I: 即时窗口
Ctrl+Tab: 活动窗体切换
Ctrl+Shift+N: 新建项目
Ctrl+Shift+O: 打开项目
Ctrl+Shift+S: 全部保存
Shift+Alt+C: 新建类
Ctrl+Shift+A: 新建项
VS2005的隐藏快捷键：
这里我将会把一些无意中发现的VS2005中没有明确指出的快捷键共享出来，并不是所有的快捷键，或者常见的一些快捷键。
1、Ctrl+Space直接完成类或函数（本来这个并不算隐藏的快捷键，但是因为中文输入法抢占这个快捷键，所以。。。，替代的快捷键是Alt+Right）
2、Shift+Delete整行删除，并且将这一行放到剪贴板（这时候不能选中一段内容）
3、Shift+Insert粘贴，有点匪夷所思，Ctrl+V就可以了，大概是为了和Shift+Delete对应吧
4、Ctrl+Up，Ctrl+Down滚动编辑器，但尽量不移动光标，光标保证在可见范围内
5、Ctrl+BackSpace，Ctrl+Delete整词删除，有的时候很有用
6、Ctrl+Left，Ctrl+Right按整词移动光标（不算隐藏，和前面几条加起来就是Ctrl光标控制套件了）
7、Alt+Shift+F10打开执行改名，实现接口和抽象类的小窗口（还可以用Ctrl+.，不过有的中文输入法用到这个）
8、Shift+F9调试是打开QuickWatch，内容是当前光标所在处的内容
9、F12转跳到定义，很有用的快捷键
10、Shift+F12查找所有引用
 如有遗漏和建议，欢迎留言！


      排序算法是一种基本并且常用的算法。由于实际工作中处理的数量巨大，所以排序算法对算法本身的速度要求很高。而一般我们所谓的算法的性能主要是指算法的复杂度，一般用O方法来表示。在后面我将给出详细的说明。 

     简单排序算法，后面你将看到他们的共同点是算法复杂度为O(N＊N)：

1.冒泡排序：


#include ＜iostream.h＞ 

void BubbleSort(int＊ pData，int Count) 
{ 
int iTemp; 
for(int i＝1;i＜Count;i++) 
{ 
　for(int j＝Count-1;j＞＝i;j--) 
　{ 
　　if(pData[j]＜pData[j-1]) 
　　{ 
　　iTemp ＝ pData[j-1]; 
　　pData[j-1] ＝ pData[j]; 
　　pData[j] ＝ iTemp; 
　　} 
　} 
} 
} 

void main() 
{ 
int data[] ＝ {10，9，8，7，6，5，4}; 
BubbleSort(data，7); 
for (int i＝0;i＜7;i++) 
　cout＜＜data＜＜＂ ＂; 
cout＜＜＂＼n＂; 
} 
倒序(最糟情况) 
第一轮：10，9，8，7-＞10，9，7，8-＞10，7，9，8-＞7，10，9，8(交换3次) 
第二轮：7，10，9，8-＞7，10，8，9-＞7，8，10，9(交换2次) 
第一轮：7，8，10，9-＞7，8，9，10(交换1次) 
循环次数：6次 
交换次数：6次 
其他： 
第一轮：8，10，7，9-＞8，10，7，9-＞8，7，10，9-＞7，8，10，9(交换2次) 
第二轮：7，8，10，9-＞7，8，10，9-＞7，8，10，9(交换0次) 
第一轮：7，8，10，9-＞7，8，9，10(交换1次) 
循环次数：6次 
交换次数：3次 
      以上程序示例可以看出：影响我们算法性能的主要部分是循环和交换，显然，次数越多，性能就越差。从上面的程序我们可以看出循环的次数是固定的，为1+2+...+n-1。
写成公式就是1/2＊(n-1)＊n。 
      根据以上分析我们可以给出O方法的定义： 
      若存在一常量K和起点n0，使当n＞＝n0时，有f(n)＜＝K＊g(n)，则f(n) ＝ O(g(n))。
      现在我们来看1/2＊(n-1)＊n，当K＝1/2，n0＝1，g(n)＝n＊n时，1/2＊(n-1)＊n＜＝1/2＊n＊n＝K＊g(n)。所以f(n) ＝O(g(n))＝O(n＊n)。所以我们程序循环的复杂度为O(n＊n)。
      再看交换。从程序后面所跟的表可以看到，两种情况的循环相同，交换不同。其实交换本身同数据源的有序程度有极大的关系，当数据处于倒序的情况时，交换次数同循环一样（每次循环判断都会交换），复杂度为O(n＊n)。当数据为正序，将不会有交换。复杂度为O(0)。乱序时处于中间状态。正是由于这样的原因，我们通常都是通过循环次数来对比算法。 


2.交换排序： 
       交换法的程序最清晰简单，每次用当前的元素一一的同其后的元素比较并交换。 

#include ＜iostream.h＞ 
void ExchangeSort(int＊ pData，int Count) 
{ 
int iTemp; 
for(int i＝0;i＜Count-1;i++) 
{ 
　for(int j＝i+1;j＜Count;j++) 
　{ 
　　if(pData[j]＜pData) 
　　{ 
　　iTemp ＝ pData; 
　　pData ＝ pData[j]; 
　　pData[j] ＝ iTemp; 
　　} 
　} 
} 
} 

void main() 
{ 
int data[] ＝ {10，9，8，7，6，5，4}; 
ExchangeSort(data，7); 
for (int i＝0;i＜7;i++) 
　cout＜＜data＜＜＂ ＂; 
cout＜＜＂＼n＂; 
} 
倒序(最糟情况) ：
第一轮：10，9，8，7-＞9，10，8，7-＞8，10，9，7-＞7，10，9，8(交换3次) 
第二轮：7，10，9，8-＞7，9，10，8-＞7，8，10，9(交换2次) 
第一轮：7，8，10，9-＞7，8，9，10(交换1次) 
循环次数：6次 
交换次数：6次 
其他： 
第一轮：8，10，7，9-＞8，10，7，9-＞7，10，8，9-＞7，10，8，9(交换1次) 
第二轮：7，10，8，9-＞7，8，10，9-＞7，8，10，9(交换1次) 
第一轮：7，8，10，9-＞7，8，9，10(交换1次) 
循环次数：6次 
交换次数：3次 
        从运行的表格来看，交换几乎和冒泡一样糟。事实确实如此。循环次数和冒泡一样也是1/2＊(n-1)＊n，所以算法的复杂度仍然是O(n＊n)。由于我们无法给出所有的情况，所以只能直接告诉大家他们在交换上面也是一样的糟糕（在某些情况下稍好，在某些情况下稍差）。
3.选择排序： 
       现在我们终于可以看到一点希望：选择法，这种方法提高了一点性能（某些情况下） ,这种方法类似我们人为的排序习惯：从数据中选择最小的同第一个值交换，在从省下的部分中选择最小的与第二个交换，这样往复下去。


 #include ＜iostream.h＞ 
void SelectSort(int＊ pData，int Count) 
{ 
int iTemp; 
int iPos; 
for(int i＝0;i＜Count-1;i++) 
{ 
　iTemp ＝ pData; 
　iPos ＝ i; 
　for(int j＝i+1;j＜Count;j++) 
　{ 
　　if(pData[j]＜iTemp) 
　　{ 
　　iTemp ＝ pData[j]; 
　　iPos ＝ j; 
　　} 
　} 
　pData[iPos] ＝ pData; 
　pData ＝ iTemp; 
} 
} 

void main() 
{ 
int data[] ＝ {10，9，8，7，6，5，4}; 
SelectSort(data，7); 
for (int i＝0;i＜7;i++) 
　cout＜＜data＜＜＂ ＂; 
cout＜＜＂＼n＂; 
} 
倒序(最糟情况) :
第一轮：10，9，8，7-＞(iTemp＝9)10，9，8，7-＞(iTemp＝8)10，9，8，7-＞(iTemp＝7)7，9，8，10(交换1次) 
第二轮：7，9，8，10-＞7，9，8，10(iTemp＝8)-＞(iTemp＝8)7，8，9，10(交换1次) 
第一轮：7，8，9，10-＞(iTemp＝9)7，8，9，10(交换0次) 
循环次数：6次 
交换次数：2次 
其他： 
第一轮：8，10，7，9-＞(iTemp＝8)8，10，7，9-＞(iTemp＝7)8，10，7，9-＞(iTemp＝7)7，10，8，9(交换1次) 
第二轮：7，10，8，9-＞(iTemp＝8)7，10，8，9-＞(iTemp＝8)7，8，10，9(交换1次) 
第一轮：7，8，10，9-＞(iTemp＝9)7，8，9，10(交换1次) 
循环次数：6次 
交换次数：3次 
遗憾的是算法需要的循环次数依然是1/2＊(n-1)＊n。所以算法复杂度为O(n＊n)。 
我们来看他的交换。由于每次外层循环只产生一次交换（只有一个最小值）。所以f(n)＜＝n 
所以我们有f(n)＝O(n)。所以，在数据较乱的时候，可以减少一定的交换次数。 
4.插入排序： 
        插入法较为复杂，它的基本工作原理是抽出牌，在前面的牌中寻找相应的位置插入，然后继续下一张 



#include ＜iostream.h＞ 
void InsertSort(int＊ pData，int Count) 
{ 
int iTemp; 
int iPos; 
for(int i＝1;i＜Count;i++) 
{ 
　iTemp ＝ pData; 
　iPos ＝ i-1; 
　while((iPos＞＝0) &amp;&amp; (iTemp＜pData[iPos])) 
　{ 
　　pData[iPos+1] ＝ pData[iPos]; 
　　iPos--; 
　} 
　pData[iPos+1] ＝ iTemp; 
} 
} 

void main() 
{ 
int data[] ＝ {10，9，8，7，6，5，4}; 
InsertSort(data，7); 
for (int i＝0;i＜7;i++) 
　cout＜＜data＜＜＂ ＂; 
cout＜＜＂＼n＂; 
} 
倒序(最糟情况) ：
第一轮：10，9，8，7-＞9，10，8，7(交换1次)(循环1次) 
第二轮：9，10，8，7-＞8，9，10，7(交换1次)(循环2次) 
第一轮：8，9，10，7-＞7，8，9，10(交换1次)(循环3次) 
循环次数：6次 
交换次数：3次 
其他： 
第一轮：8，10，7，9-＞8，10，7，9(交换0次)(循环1次) 
第二轮：8，10，7，9-＞7，8，10，9(交换1次)(循环2次) 
第一轮：7，8，10，9-＞7，8，9，10(交换1次)(循环1次) 
循环次数：4次 
交换次数：2次 


       上面结尾的行为分析事实上造成了一种假象，让我们认为这种算法是简单算法中最好的，其实不是，因为其循环次数虽然并不固定，我们仍可以使用O方法。从上面的结果可以看出，循环的次数f(n)＜＝1/2＊n＊(n-1)＜＝1/2＊n＊n。所以其复杂度仍为O(n＊n)（这里说明一下，其实如果不是为了展示这些简单排序的不同，交换次数仍然可以这样推导）。现在看交换，从外观上看，交换次数是O(n)（推导类似选择法），但我们每次要进行与内层循环相同次数的‘＝’操作。正常的一次交换我们需要三次‘＝’而这里显然多了一些，所以我们浪费了时间。
       在简单排序算法中，选择法相对来说是最好的。  

       高级排序算法，复杂度为O(Log2(N))：
       高级排序算法中我们将只介绍这一种，同时也是目前我所知道（我看过的资料中）的最快的。 它的工作看起来仍然象一个二叉树。首先我们选择一个中间值middle程序中我们使用数组中间值，然后把比它小的放在左边，大的放在右边（具体的实现是从两边找，找到一对后交换）。然后对两边分别使用这个过程（最容易的方法——递归）。


1.快速排序：


 #include ＜iostream.h＞ 
void run(int＊ pData，int left，int right) 
{ 
int i，j; 
int middle，iTemp; 
i ＝ left; 
j ＝ right; 
middle ＝ pData[(left+right)/2]; //求中间值 
do{ 
　while((pData＜middle) &amp;&amp; (i＜right))//从左扫描大于中值的数 
　　i++;　　 
　while((pData[j]＞middle) &amp;&amp; (j＞left))//从右扫描大于中值的数 
　　j--; 
　if(i＜＝j)//找到了一对值 
　{ 
　　//交换 
　　iTemp ＝ pData; 
　　pData ＝ pData[j]; 
　　pData[j] ＝ iTemp; 
　　i++; 
　　j--; 
　} 
}while(i＜＝j);//如果两边扫描的下标交错，就停止（完成一次） 

//当左边部分有值(left＜j)，递归左半边 
if(left＜j) 
　run(pData，left，j); 
//当右边部分有值(right＞i)，递归右半边 
if(right＞i) 
　run(pData，i，right); 
} 

void QuickSort(int＊ pData，int Count) 
{ 
run(pData，0，Count-1); 
} 

void main() 
{ 
int data[] ＝ {10，9，8，7，6，5，4}; 
QuickSort(data，7); 
for (int i＝0;i＜7;i++) 
　cout＜＜data＜＜＂ ＂; 
cout＜＜＂＼n＂; 
} 
       这里我没有给出行为的分析，因为这个很简单，我们直接来分析算法：首先我们考虑最理想的情况
1.数组的大小是2的幂，这样分下去始终可以被2整除。假设为2的k次方，即k＝log2(n)。
2.每次我们选择的值刚好是中间值，这样，数组才可以被等分。
第一层递归，循环n次，第二层循环2＊(n/2)......
所以共有n+2(n/2)+4(n/4)+...+n＊(n/n)＝ n+n+n+...+n＝k＊n＝log2(n)＊n
所以算法复杂度为O(log2(n)＊n)
      其他的情况只会比这种情况差，最差的情况是每次选择到的middle都是最小值或最大值，那么他将变成交换法（由于使用了递归，情况更糟）。但是你认为这种情况发生的几率有多大？？呵呵，你完全不必担心这个问题。实践证明，大多数的情况，快速排序总是最好的。
如果你担心这个问题，你可以使用堆排序，这是一种稳定的O(log2(n)＊n)算法，但是通常情况下速度要慢于快速排序（因为要重组堆）。


      其它排序算法：

 1.双向冒泡：
      通常的冒泡是单向的，而这里是双向的，也就是说还要进行反向的工作。代码看起来复杂，仔细理一下就明白了，是一个来回震荡的方式。    


#include ＜iostream.h＞ 
void Bubble2Sort(int＊ pData，int Count) 
{ 
int iTemp; 
int left ＝ 1; 
int right ＝Count -1; 
int t; 
do 
{ 
　//正向的部分 
　for(int i＝right;i＞＝left;i--) 
　{ 
　　if(pData＜pData[i-1]) 
　　{ 
　　iTemp ＝ pData; 
　　pData ＝ pData[i-1]; 
　　pData[i-1] ＝ iTemp; 
　　t ＝ i; 
　　} 
　} 
　left ＝ t+1; 

　//反向的部分 
　for(i＝left;i＜right+1;i++) 
　{ 
　　if(pData＜pData[i-1]) 
　　{ 
　　iTemp ＝ pData; 
　　pData ＝ pData[i-1]; 
　　pData[i-1] ＝ iTemp; 
　　t ＝ i; 
　　} 
　} 
　right ＝ t-1; 
}while(left＜＝right); 
} 

void main() 
{ 
int data[] ＝ {10，9，8，7，6，5，4}; 
Bubble2Sort(data，7); 
for (int i＝0;i＜7;i++) 
　cout＜＜data＜＜＂ ＂; 
cout＜＜＂＼n＂; 
} 
2.SHELL排序
        这个排序非常复杂，看了程序就知道了。首先需要一个递减的步长，这里我们使用的是9、5、3、1（最后的步长必须是1）。工作原理是首先对相隔9-1个元素的所有内容排序，然后再使用同样的方法对相隔5-1个元素的排序以次类推。
#include ＜iostream.h＞ 
void ShellSort(int＊ pData，int Count) 
{ 
int step[4]; 
step[0] ＝ 9; 
step[1] ＝ 5; 
step[2] ＝ 3; 
step[3] ＝ 1; 

int iTemp; 
int k，s，w; 
for(int i＝0;i＜4;i++) 
{ 
　k ＝ step; 
　s ＝ -k; 
　for(int j＝k;j＜Count;j++) 
　{ 
　　iTemp ＝ pData[j]; 
　　w ＝ j-k;//求上step个元素的下标 
　　if(s ＝＝0) 
　　{ 
　　s ＝ -k; 
　　s++; 
　　pData[s] ＝ iTemp; 
　　} 
　　while((iTemp＜pData[w]) &amp;&amp; (w＞＝0) &amp;&amp; (w＜＝Count)) 
　　{ 
　　pData[w+k] ＝ pData[w]; 
　　w ＝ w-k; 
　　} 
　　pData[w+k] ＝ iTemp; 
　} 
} 
} 

void main() 
{ 
int data[] ＝ {10，9，8，7，6，5，4，3，2，1，-10，-1}; 
ShellSort(data，12); 
for (int i＝0;i＜12;i++) 
　cout＜＜data＜＜＂ ＂; 
cout＜＜＂＼n＂; 
} 
        如果觉得复杂，就把s＝＝0的块去掉再看代码，这里是避免使用0步长造成程序异常而写的代码。这个代码我认为很值得一看。这个算法的得名是因为其发明者的名字D.L.SHELL。依照参考资料上的说法：“由于复杂的数学原因避免使用2的幂次步长，它能降低算法效率。”另外算法的复杂度为n的1.2次幂。同样因为非常复杂并“超出本书讨论范围”的原因（我也不知道过程），我们只有结果了。
    基于模板的通用排序：
     MyData.h文件 class CMyData 
{ 
public: 
CMyData(int Index，char＊ strData); 
CMyData(); 
virtual ~CMyData(); 

int m_iIndex; 
int GetDataSize(){ return m_iDataSize; }; 
const char＊ GetData(){ return m_strDatamember; }; 
//这里重载了操作符： 
CMyData&amp; operator ＝(CMyData &amp;SrcData); 
bool operator ＜(CMyData&amp; data ); 
bool operator ＞(CMyData&amp; data ); 

private: 
char＊ m_strDatamember; 
int m_iDataSize; 
}; 

MyData.cpp文件 

CMyData::CMyData(): 
m_iIndex(0)， 
m_iDataSize(0)， 
m_strDatamember(NULL) 
{ 
} 

CMyData::~CMyData() 
{ 
if(m_strDatamember !＝ NULL) 
　delete[] m_strDatamember; 
m_strDatamember ＝ NULL; 
} 

CMyData::CMyData(int Index，char＊ strData): 
m_iIndex(Index)， 
m_iDataSize(0)， 
m_strDatamember(NULL) 
{ 
m_iDataSize ＝ strlen(strData); 
m_strDatamember ＝ new char[m_iDataSize+1]; 
strcpy(m_strDatamember，strData); 
} 

CMyData&amp; CMyData::operator ＝(CMyData &amp;SrcData) 
{ 
m_iIndex ＝ SrcData.m_iIndex; 
m_iDataSize ＝ SrcData.GetDataSize(); 
m_strDatamember ＝ new char[m_iDataSize+1]; 
strcpy(m_strDatamember，SrcData.GetData()); 
return ＊this; 
} 

bool CMyData::operator ＜(CMyData&amp; data ) 
{ 
return m_iIndex＜data.m_iIndex; 
} 

bool CMyData::operator ＞(CMyData&amp; data ) 
{ 
return m_iIndex＞data.m_iIndex; 
} 

//主程序部分 
#include ＜iostream.h＞ 
#include ＂MyData.h＂ 

template ＜class T＞ 
void run(T＊ pData，int left，int right) 
{ 
int i，j; 
T middle，iTemp; 
i ＝ left; 
j ＝ right; 
//下面的比较都调用我们重载的操作符函数 
middle ＝ pData[(left+right)/2]; //求中间值 
do{ 
　while((pData＜middle) &amp;&amp; (i＜right))//从左扫描大于中值的数 
　　i++;　　 
　while((pData[j]＞middle) &amp;&amp; (j＞left))//从右扫描大于中值的数 
　　j--; 
　if(i＜＝j)//找到了一对值 
　{ 
　　//交换 
　　iTemp ＝ pData; 
　　pData ＝ pData[j]; 
　　pData[j] ＝ iTemp; 
　　i++; 
　　j--; 
　} 
}while(i＜＝j);//如果两边扫描的下标交错，就停止（完成一次） 

//当左边部分有值(left＜j)，递归左半边 
if(left＜j) 
　run(pData，left，j); 
//当右边部分有值(right＞i)，递归右半边 
if(right＞i) 
　run(pData，i，right); 
} 

template ＜class T＞ 
void QuickSort(T＊ pData，int Count) 
{ 
run(pData，0，Count-1); 
} 

void main() 
{ 
CMyData data[] ＝ { 
　CMyData(8，＂xulion＂)， 
　CMyData(7，＂sanzoo＂)， 
　CMyData(6，＂wangjun＂)， 
　CMyData(5，＂VCKBASE＂)， 
　CMyData(4，＂jacky2000＂)， 
　CMyData(3，＂cwally＂)， 
　CMyData(2，＂VCUSER＂)， 
　CMyData(1，＂isdong＂) 
}; 
QuickSort(data，8); 
for (int i＝0;i＜8;i++) 
　cout＜＜data.m_iIndex＜＜＂ ＂＜＜data.GetData()＜＜＂＼n＂; 
cout＜＜＂＼n＂;
经典C++双向冒泡排序算法：
#include《iostream.h》
#define max 20 //最多记录个数
typedef int elemtype;
typedef elemtype recs[max];
void bibubble(recs r,int n)
{
int flag=1; //继续遍历时flag置1,已排好序不需遍历时为0
int i=0, j;
elemtype temp;
while(flag==1)
{
flag=0;
for(j=i+1;j《n-1;j++) //正向遍历找最大值
if(r[j]》r[j+1])
{
flag=1; //能交换时,说明未排好序,需继续
temp=r[j];
r[j]=r[j+1];
r[j+1]=temp;
}
for(j=n-i-1;j》=i+1;j--) //反向遍历
if(r[j]》r[j-1])
{
flag=1; //能交换时,说明未排好序,需继续
temp=r[j];
r[j]=r[j-1];
r[j-1]=temp;
}
i++;
}
}
void main()
{
recs A={2,5,3,4,6,10,9,8,7,1};
int n=10, i;
cout《《"双向冒泡排序"《《endl《《"排序前:";
for(i=0;i《n;i++)
cout《《A[i]《《"";
cout《《endl;
cout《《" 排序后: ";
bibubble(A,n);
for(i=0;i《n;i++)
cout《《A[i]《《"";
cout《《endl;
}






      




C++中的类型转换分为两种：
1.      隐式类型转换（而对于隐式变换，就是标准的转换，在很多时候，不经意间就发生了，比如int类型和float类型相加时，int类型就会被隐式的转换位float类型，然后再进行相加运算。）；
2.      显式类型转换。
        关于强制类型转换的问题，很多书都讨论过，写的最详细的是C++ 之父的《C++的设计和演化》。最好的解决方法就是不要使用C风格的强制类型转换，而是使用标准C++的类型转换符：static_cast, dynamic_cast。标准C++中有四个类型转换符：static_cast、dynamic_cast、reinterpret_cast、和 const_cast。下面对它们一一进行介绍。
1.     static_cast
static_cast的转换格式：static_cast&lt; type-id &gt; ( expression )
该运算符把expression转换为type-id类型，但没有运行时类型检查来保证转换的安全性。它主要有如下几种用法：
①用于类层次结构中基类和子类之间指针或引用的转换。
进行上行转换（把子类的指针或引用转换成基类表示）是安全的；
进行下行转换（把基类指针或引用转换成子类表示）时，由于没有动态类型检查，所以是不安全的。
②用于基本数据类型之间的转换。如把int转换成char，把int转换成enum。这种转换的安全性也要开发人员来保证。
③把空指针转换成目标类型的空指针。
④把任何类型的表达式转换成void类型。
注意：static_cast不能转换掉expression的const、volitale、或者__unaligned属性。
                                                                                                                                                
2.     dynamic_cast
        主要用于执行“安全的向下转型（safe down casting）”，也就是说，要确定一个对象是否是一个继承体系中的一个特定类型。
dynamic_cast的转换格式：dynamic_cast&lt; type-id &gt; ( expression )
        该运算符把expression转换成type-id类型的对象。Type-id必须是类的指针、类的引用或者void*；如果type-id是类指针类型，那么expression也必须是一个指针，如果type-id是一个引用，那么expression也必须是一个引用。
dynamic_cast主要用于类层次间的上行转换和下行转换，还可以用于类之间的交叉转换。
        在类层次间进行上行转换时，dynamic_cast和static_cast的效果是一样的；在进行下行转换时，dynamic_cast具有类型检查的功能，比static_cast更安全。在多态类型之间的转换主要使用dynamic_cast，因为类型提供了运行时信息。
（1）.上行转换
        比如B继承自A，B转换为A，进行上行转换时，是安全的，如下：

#include &lt;iostream&gt;
using namespace std;
class A
{
	// ......
};
class B : public A
{
	// ......
};
int main()
{
	B *pB = new B;
	A *pA = dynamic_cast&lt;A *&gt;(pB); // Safe and will succeed
}

（2）.多重继承之间的上行转换：
         C继承自B，B继承自A，这种多重继承的关系；但是，关系很明确，使用dynamic_cast进行转换时，也是很简单的：
class A
{
	// ......
};
class B : public A
{
	// ......
};
class C : public B
{
	// ......
};
int main()
{
	C *pC = new C;
	B *pB = dynamic_cast&lt;B *&gt;(pC); // OK
	A *pA = dynamic_cast&lt;A *&gt;(pC); // OK
}
         而上述的转换，static_cast和dynamic_cast具有同样的效果。而这种上行转换，也被称为隐式转换；比如我们在定义变量时经常这么写：B *pB = new C;这和上面是一个道理的，只是多加了一个dynamic_cast转换符而已。
（3）.转换成void*
可以将类转换成void *，例如：
class A
{
public:
	virtual void f(){}
	// ......
};
class B
{
public:
	virtual void f(){}
	// ......
};
int main()
{
	A *pA = new A;
	B *pB = new B;
	void *pV = dynamic_cast&lt;void *&gt;(pA); // pV points to an object of A
	pV = dynamic_cast&lt;void *&gt;(pB); // pV points to an object of B
}
        但是，在类A和类B中必须包含虚函数，为什么呢？因为类中存在虚函数，就说明它有想让基类指针或引用指向派生类对象的情况，此时转换才有意义；由于运行时类型检查需要运行时类型信息，而这个信息存储在类的虚函数表中，只有定义了虚函数的类才有虚函数表。

（4）.如果expression是type-id的基类，使用dynamic_cast进行转换时，在运行时就会检查expression是否真正的指向一个type-id类型的对象，如果是，则能进行正确的转换，获得对应的值；否则返回NULL，如果是引用，则在运行时就会抛出异常；例如：
class B
{
	virtual void f(){};
};
class D : public B
{
	virtual void f(){};
};
void main()
{
	B* pb = new D;   // unclear but ok
	B* pb2 = new B;
	D* pd = dynamic_cast&lt;D*&gt;(pb);   // ok: pb actually points to a D
	D* pd2 = dynamic_cast&lt;D*&gt;(pb2);   // pb2 points to a B not a D, now pd2 is NULL
}
这个就是下行转换，从基类指针转换到派生类指针。
对于一些复杂的继承关系来说，使用dynamic_cast进行转换是存在一些陷阱的；比如，钻石结构：
D类型可以安全的转换成B和C类型，但是D类型要是直接转换成A类型呢？

class A
{
	virtual void Func() = 0;
};
class B : public A
{
	void Func(){};
};
class C : public A
{
	void Func(){};
};
class D : public B, public C
{
	void Func(){}
};
int main()
{
	D *pD = new D;
	A *pA = dynamic_cast&lt;A *&gt;(pD); // You will get a pA which is NULL
}

        如果进行上面的直接转，你将会得到一个NULL的pA指针；这是因为，B和C都继承了A，并且都实现了虚函数Func，导致在进行转换时，无法进行抉择应该向哪个A进行转换。正确的做法是：
int main()
{
	D *pD = new D;
	B *pB = dynamic_cast&lt;B *&gt;(pD);
	A *pA = dynamic_cast&lt;A *&gt;(pB);
}
这就是我在实现QueryInterface时，得到IUnknown的指针时，使用的是*ppv = static_cast&lt;IX *&gt;(this);而不是*ppv = static_cast&lt;IUnknown *&gt;(this);
dynamic_cast的讨论：
         在探究 dynamic_cast 的设计意图之前，值得留意的是很多 dynamic_cast 的实现都相当慢。
         例如，至少有一种通用的实现部分地基于对类名字进行字符串比较。如果你在一个位于四层深的单继承体系中的对象上执行 dynamic_cast，在这样一个实现下的每一个 dynamic_cast 都要付出相当于四次调用strcmp 来比较类名字的成本。对于一个更深的或使用了多继承的继承体系，付出的代价会更加昂贵。
        对 dynamic_cast 的需要通常发生在这种情况下：你要在一个你确信为派生类的对象上执行派生类的操作，但是你只能通过一个基类的指针或引用来操控这个对象。
有两个一般的方法可以避免这个问题：
        第一个，使用存储着直接指向派生类对象的指针的容器，从而消除通过基类接口操控这个对象的需要。当然，这个方法不允许你在同一个容器中存储所有可能的基类的派生类的指针。为了与不同的窗口类型一起工作，你可能需要多个类型安全（type-safe）的容器。
        一个候选方法可以让你通过一个基类的接口操控所有可能的 Window 派生类，就是在基类中提供一个让你做你想做的事情的虚函数。例如，尽管只有 SpecialWindows 能 blink，在基类中声明这个函数，并提供一个什么都不做的缺省实现或许是有意义的：
所以：
（1）避免强制转型的随时应用，特别是在性能敏感的代码中应用 dynamic_casts，如果一个设计需要强制转型，设法开发一个没有强制转型的侯选方案。
（2）如果必须要强制转型，设法将它隐藏在一个函数中。客户可以用调用那个函数来代替在他们自己的代码中加入强制转型。
（3）尽量用 C++ 风格的强制转型替换旧风格的强制转型。它们更容易被注意到，而且他们做的事情也更加明确。
3.     reinpreter_cast
用法：reinpreter_cast&lt;type-id&gt;(expression)
        type-id必须是一个指针、引用、算术类型、函数指针或者成员指针。它可以把一个指针转换成一个整数，也可以把一个整数转换成一个指针（先把一个指针转换成一个整数，在把该整数转换成原类型的指针，还可以得到原先的指针值）。
该运算符的用法比较多。
4.     const_cast
const_cast的转换格式：const_cast&lt;type_id&gt; (expression)
       该运算符用来修改类型的const或volatile属性。除了const 或volatile修饰之外， type_id和expression的类型是一样的。
常量指针被转化成非常量指针，并且仍然指向原来的对象；常量引用被转换成非常量引用，并且仍然指向原来的对象；常量对象被转换成非常量对象。
Voiatile和const类试。举如下一例：
 
#include &lt;iostream&gt;
using namespace std;
class CA
{
public:
	CA() :m_iA(10){}
	int m_iA;
};
int main()
{
	const CA *pA = new CA;
	// pA-&gt;m_iA = 100; // Error
	CA *pB = const_cast&lt;CA *&gt;(pA);
	pB-&gt;m_iA = 100;
	// Now the pA and the pB points to the same object
	cout &lt;&lt; pA-&gt;m_iA &lt;&lt; endl;
	cout &lt;&lt; pB-&gt;m_iA &lt;&lt; endl;
	const CA &amp;a = *pA;
	// a.m_iA = 200; // Error
	CA &amp;b = const_cast&lt;CA &amp;&gt;(a);
	b.m_iA = 200;
	// Now the a and the b reference to the same object
	cout &lt;&lt; b.m_iA &lt;&lt; endl;
	cout &lt;&lt; a.m_iA &lt;&lt; endl;
}










          在VC/C++编程中，我们会经常遇到打开文件、网页、可执行程序的应用场景，ShellExecute API函数就可以做到这一点。现在我们看看它的强大！
          ShellExecute函数原型：
          HINSTANCE ShellExecute(
                           HWND hwnd, 
                           LPCTSTR lpOperation,
                           LPCTSTR lpFile, 
                           LPCTSTR lpParameters, 

                           LPCTSTR lpDirectory,
                           INT nShowCmd
);   
        ShellExecute函数参数说明：
       hwnd: 
用于指定父窗口句柄。当函数调用过程出现错误时，它将作为Windows消息窗口的父窗口。
lpOperation: 
用于指定要进行的操作。
“open”操作表示执行由lpFile参数指定的程序，或打开由lpFile参数指定的文件或文件夹；
“print”操作表示打印由lpFile参数指定的文件；
“explore”操作表示浏览由lpFile参数指定的文件夹。
当参数设为NULL时，表示执行默认操作“open”。
      lpFile:
用于指定要打开的文件名、要执行的程序文件名或要浏览的文件夹名。（设置该参数用相对路径时，应将调用的文件与源程序放在同一目录下，否则不出错，但也打不开文件）
     lpParameters:
若lpFile参数是一个可执行程序，则此参数指定命令行参数，否则此参数应为NULL.
     lpDirectory:
用于指定默认目录.
    nShowCmd:
若lpFile参数是一个可执行程序，则此参数指定程序窗口的初始显示方式，否则此参数应设置为0。
这个参数常用的常数：
SW_HIDE 隐藏窗口，活动状态给令一个窗口 
SW_MINIMIZE 最小化窗口，活动状态给令一个窗口 
SW_RESTORE 用原来的大小和位置显示一个窗口，同时令其进入活动状态 
SW_SHOW 用当前的大小和位置显示一个窗口，同时令其进入活动状态 
SW_SHOWMAXIMIZED 最大化窗口，并将其激活 
SW_SHOWMINIMIZED 最小化窗口，并将其激活 
SW_SHOWMINNOACTIVE 最小化一个窗口，同时不改变活动窗口 
SW_SHOWNA 用当前的大小和位置显示一个窗口，不改变活动窗口 
SW_SHOWNOACTIVATE 用最近的大小和位置显示一个窗口，同时不改变活动窗口 
SW_SHOWNORMAL 与SW_RESTORE相同
若ShellExecute函数调用成功，则返回值为被执行程序的实例句柄。若返回值小于32，则表示出现错误。  


        ShellExecute函数使用方法：
例如：
        ShellExecute(NULL,"open","iloveu.bmp",NULL,NULL,SW_SHOWNORMAL);      
用缺省的位图编辑器打开一个叫iloveu.bmp的位图文件，这个缺省的位图编辑器可能是 Microsoft Paint, Adobe Photoshop,或者Corel
 PhotoPaint。
        这个函数能打开任何文件，甚至是桌面和URL快捷方式（.ink或.url）。ShellExecute解析系统注册表HKEY_CLASSES_ROOT中所有的内容，判断启动那一个执行程序，并且启动一个新的实例或使用DDE将文件名连到一打开的实例。然后，ShellExecute返回打开文件的应用的实例句柄。
ShellExecute(NULL, "open", "http://www.microsoft.com",NULL, NULL, SW_SHOWNORMAL); 
       这个代码使你能访问微软的主页。当ShellExecute遇到文件名前面的“http:”时，可以判断出要打开的文件是Web文件，随之启动Internet
 Explorer 或者 Netscape Navigator或者任何你使用的别的浏览器打开文件。
ShellExecute还能识别其它协议，象FTP、GOPHER。甚至识别“mailto”，如果文件名指向“mailto:zxn@hq.cninfo.net”,它启动电子邮件程序并打开一个待编辑的新邮件，例如：
       ShellExecute(NULL,"open",“mailto:zxn@hq.cninfo.net”, NULL, NULL, SW_SHOWNORMAL);打开新邮件窗口。
       总之，ShellExecute函数就是如此简单地打开磁盘文件和Internet文件。如果将第二个参数“OPEN”改为“PRINT”或者“EXPLORE”，ShellExecute将能打印文件和打开文件夹。ShellExecute还有一个扩展函数ShellExecuteEx，所带参数中有一个特殊的结构，功能更强，或者任何你使用的别的浏览器打开文件。




Q: 如何打开一个应用程序？
ShellExecute(this-&gt;m_hWnd,"open","calc.exe","","", SW_SHOW ); 或 ShellExecute(this-&gt;m_hWnd,"open","notepad.exe","c:\\MyLog.log","",SW_SHOW );正如您所看到的，我并没有传递程序的完整路径。
Q: 如何打开一个同系统程序相关连的文档？
ShellExecute(this-&gt;m_hWnd,"open","c:\\abc.txt","","",SW_SHOW );
Q: 如何打开一个网页？
ShellExecute(this-&gt;m_hWnd,"open","http://www.google.com","","", SW_SHOW );
Q: 如何激活相关程序，发送EMAIL？
ShellExecute(this-&gt;m_hWnd,"open","mailto:nishinapp@yahoo.com","","", SW_SHOW );
Q: 如何用系统打印机打印文档？
ShellExecute(this-&gt;m_hWnd,"print","c:\\abc.txt","","", SW_HIDE);
Q: 如何用系统查找功能来查找指定文件？
ShellExecute(m_hWnd,"find","d:\\nish",NULL,NULL,SW_SHOW);
Q: 如何启动一个程序，直到它运行结束？
SHELLEXECUTEINFO ShExecInfo = {0};
ShExecInfo.cbSize = sizeof(SHELLEXECUTEINFO);
ShExecInfo.fMask = SEE_MASK_NOCLOSEPROCESS;
ShExecInfo.hwnd = NULL;
ShExecInfo.lpVerb = NULL;
ShExecInfo.lpFile = "c:\\MyProgram.exe";
ShExecInfo.lpParameters = "";
ShExecInfo.lpDirectory = NULL;
ShExecInfo.nShow = SW_SHOW;
ShExecInfo.hInstApp = NULL;
ShellExecuteEx(&amp;ShExecInfo);
WaitForSingleObject(ShExecInfo.hProcess,INFINITE);
或：
PROCESS_INFORMATION ProcessInfo;
STARTUPINFO StartupInfo; //This is an [in] parameter
ZeroMemory(&amp;StartupInfo, sizeof(StartupInfo));
StartupInfo.cb = sizeof StartupInfo ; //Only compulsory field
if(CreateProcess("c:\\winnt\\notepad.exe", NULL,
     NULL,NULL,FALSE,0,NULL,
     NULL,&amp;StartupInfo,&amp;ProcessInfo))
{
     WaitForSingleObject(ProcessInfo.hProcess,INFINITE);
     CloseHandle(ProcessInfo.hThread);
     CloseHandle(ProcessInfo.hProcess);
}  
else
{
     MessageBox("The process could not be started...");
}
Q: 如何显示文件或文件夹的属性？
SHELLEXECUTEINFO ShExecInfo ={0};
ShExecInfo.cbSize = sizeof(SHELLEXECUTEINFO);
ShExecInfo.fMask = SEE_MASK_INVOKEIDLIST ;
ShExecInfo.hwnd = NULL;
ShExecInfo.lpVerb = "properties";
ShExecInfo.lpFile = "c:\\"; //can be a file as well
ShExecInfo.lpParameters = "";
ShExecInfo.lpDirectory = NULL;
ShExecInfo.nShow = SW_SHOW;
ShExecInfo.hInstApp = NULL;
ShellExecuteEx(&amp;ShExecInfo);




 
 



一．线程的状态：

线程共有下面4种状态：

1.新建状态（New）：

新创建了一个线程对象，当你用new创建一个线程时，该线程尚未运行。

2.就绪状态（Runnable）：

线程对象创建后，其他线程调用了该对象的start（）方法。该状态的线程位于可运行线程池中，变得可运行，等待获取CPU的使用权。

3.运行状态（Running）：

就绪状态的线程获取了CPU，执行程序代码。

4.阻塞状态（Blocked）：

阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。

阻塞的情况分三种：

a. 等待阻塞：运行的线程执行wait（）方法，JVM会把该线程放入等待池中。 
b. 同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM把该线程放入锁。 
c. 其他阻塞：运行的线程执行sleep（）或join（）方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep（）状态超时、join（）等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。

5.死亡状态（Dead）：

a. 由于run方法的正常退出而自然死亡; 
b. 没有捕获到的异常事件终止了run方法的执行，从而导致线程突然死亡

如下图：




线程的优先级：

每一个Java线程都有一个优先级，这样有助于操作系统确定线程的调度顺序。 
Java线程的优先级是一个整数，其取值范围是1（Thread.MIN_PRIORITY） - 10（Thread.MAX_PRIORITY）。 
默认情况下，每一个线程都会分配一个优先级NORM_PRIORITY（5）。 
具有较高优先级的线程对程序更重要，并且应该在低优先级的线程之前分配处理器资源。但是，线程优先级不能保证线程执行的顺序，而且非常依赖于平台。

若要确定某个线程当前是否活着，可以使用 isAlive方法。

如果该线程是可运行线程或者被中断线程，那么该方法返回true；如果该线程仍然是个新建线程，或者该线程是个死线程，那么该方法返回false

线程在不同情况下的状态变化。




JAVA多线程实现方式：

1、继承Thread类 
2、实现Runnable接口 
3、使用ExecutorService、Callable、Future实现有返回结果的多线程。

其中前两种方式线程执行完后都没有返回值，只有最后一种是带返回值的。其中最常用的也是前两种实现方式。

Runnable和Thread实现多线程的区别：

Runnable接口相比继承Thread类有如下优势：

1、可以避免由于Java的单继承特性而带来的局限；

2、增强程序的健壮性，代码能够被多个线程共享，代码与数据是独立的；

3、适合多个相同程序代码的线程区处理同一资源的情况。

下面以典型的买票程序（基本都是以这个为例子）为例，来说明二者的区别。

&lt;code class="hljs axapta has-numbering" style="display: block; padding: 0px; color: inherit; box-sizing: border-box; font-family: 'Source Code Pro', monospace;font-size:undefined; white-space: pre; border-radius: 0px; word-wrap: normal; background: transparent;"&gt;&lt;span class="hljs-class" style="box-sizing: border-box;"&gt;&lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;class&lt;/span&gt; &lt;span class="hljs-title" style="box-sizing: border-box; color: rgb(102, 0, 102);"&gt;MyThread&lt;/span&gt; &lt;span class="hljs-inheritance" style="box-sizing: border-box;"&gt;&lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;extends&lt;/span&gt;&lt;/span&gt; &lt;span class="hljs-title" style="box-sizing: border-box; color: rgb(102, 0, 102);"&gt;Thread&lt;/span&gt;{&lt;/span&gt; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;private&lt;/span&gt; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;int&lt;/span&gt; ticket = &lt;span class="hljs-number" style="color: rgb(0, 102, 102); box-sizing: border-box;"&gt;5&lt;/span&gt;; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;public&lt;/span&gt; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;void&lt;/span&gt; run(){ &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;for&lt;/span&gt; (&lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;int&lt;/span&gt; i=&lt;span class="hljs-number" style="color: rgb(0, 102, 102); box-sizing: border-box;"&gt;0&lt;/span&gt;;i&lt;&lt;span class="hljs-number" style="color: rgb(0, 102, 102); box-sizing: border-box;"&gt;10&lt;/span&gt;;i++) { &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;if&lt;/span&gt;(ticket &gt; &lt;span class="hljs-number" style="color: rgb(0, 102, 102); box-sizing: border-box;"&gt;0&lt;/span&gt;){ System.out.println(&lt;span class="hljs-string" style="color: rgb(0, 136, 0); box-sizing: border-box;"&gt;"ticket = "&lt;/span&gt; + ticket--); } } } } &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;public&lt;/span&gt; &lt;span class="hljs-class" style="box-sizing: border-box;"&gt;&lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;class&lt;/span&gt; &lt;span class="hljs-title" style="box-sizing: border-box; color: rgb(102, 0, 102);"&gt;ThreadDemo&lt;/span&gt;{&lt;/span&gt; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;public&lt;/span&gt; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;static&lt;/span&gt; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;void&lt;/span&gt; main(String[] args){ &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;new&lt;/span&gt; MyThread().start(); &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;new&lt;/span&gt; MyThread().start(); &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;new&lt;/span&gt; MyThread().start(); } &lt;/code&gt;&lt;ul class="pre-numbering" style="box-sizing: border-box; position: absolute; width: 50px; top: 0px; left: 0px; margin: 0px; padding: 6px 0px 40px; border-right-width: 1px; border-right-style: solid; border-right-color: rgb(221, 221, 221); list-style: none; text-align: right; background-color: rgb(238, 238, 238);"&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;1&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;2&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;3&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;4&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;5&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;6&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;7&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;8&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;9&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;10&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;11&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;12&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;13&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;14&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;15&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;16&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;17&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;18&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;19&lt;/li&gt;&lt;/ul&gt;&lt;ul class="pre-numbering" style="box-sizing: border-box; position: absolute; width: 50px; top: 0px; left: 0px; margin: 0px; padding: 6px 0px 40px; border-right-width: 1px; border-right-style: solid; border-right-color: rgb(221, 221, 221); list-style: none; text-align: right; background-color: rgb(238, 238, 238);"&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;1&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;2&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;3&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;4&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;5&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;6&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;7&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;8&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;9&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;10&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;11&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;12&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;13&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;14&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;15&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;16&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;17&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;18&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;19&lt;/li&gt;&lt;/ul&gt;  







 



从结果中可以看出，每个线程单独卖了5张票，即独立地完成了买票的任务，但实际应用中，比如火车站售票，需要多个线程去共同完成任务，在本例中，即多个线程共同买5张票。

下面是通过实现Runnable接口实现的多线程程序，代码如下：





[java] view
 plain copy


 print?



&lt;code class="hljs axapta has-numbering" style="display: block; padding: 0px; color: inherit; box-sizing: border-box; font-family: 'Source Code Pro', monospace;font-size:undefined; white-space: pre; border-radius: 0px; word-wrap: normal; background: transparent;"&gt;&lt;span class="hljs-class" style="box-sizing: border-box;"&gt;&lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;class&lt;/span&gt; &lt;span class="hljs-title" style="box-sizing: border-box; color: rgb(102, 0, 102);"&gt;MyThread&lt;/span&gt; &lt;span class="hljs-inheritance" style="box-sizing: border-box;"&gt;&lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;implements&lt;/span&gt;&lt;/span&gt; &lt;span class="hljs-title" style="box-sizing: border-box; color: rgb(102, 0, 102);"&gt;Runnable&lt;/span&gt;{&lt;/span&gt; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;private&lt;/span&gt; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;int&lt;/span&gt; ticket = &lt;span class="hljs-number" style="color: rgb(0, 102, 102); box-sizing: border-box;"&gt;5&lt;/span&gt;; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;public&lt;/span&gt; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;void&lt;/span&gt; run(){ &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;for&lt;/span&gt; (&lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;int&lt;/span&gt; i=&lt;span class="hljs-number" style="color: rgb(0, 102, 102); box-sizing: border-box;"&gt;0&lt;/span&gt;;i&lt;&lt;span class="hljs-number" style="color: rgb(0, 102, 102); box-sizing: border-box;"&gt;10&lt;/span&gt;;i++) { &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;if&lt;/span&gt;(ticket &gt; &lt;span class="hljs-number" style="color: rgb(0, 102, 102); box-sizing: border-box;"&gt;0&lt;/span&gt;){ System.out.println(&lt;span class="hljs-string" style="color: rgb(0, 136, 0); box-sizing: border-box;"&gt;"ticket = "&lt;/span&gt; + ticket--); } } } } &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;public&lt;/span&gt; &lt;span class="hljs-class" style="box-sizing: border-box;"&gt;&lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;class&lt;/span&gt; &lt;span class="hljs-title" style="box-sizing: border-box; color: rgb(102, 0, 102);"&gt;RunnableDemo&lt;/span&gt;{&lt;/span&gt; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;public&lt;/span&gt; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;static&lt;/span&gt; &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;void&lt;/span&gt; main(String[] args){ MyThread my = &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;new&lt;/span&gt; MyThread(); &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;new&lt;/span&gt; Thread(my).start(); &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;new&lt;/span&gt; Thread(my).start(); &lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;new&lt;/span&gt; Thread(my).start(); } } &lt;/code&gt;&lt;ul class="pre-numbering" style="box-sizing: border-box; position: absolute; width: 50px; top: 0px; left: 0px; margin: 0px; padding: 6px 0px 40px; border-right-width: 1px; border-right-style: solid; border-right-color: rgb(221, 221, 221); list-style: none; text-align: right; background-color: rgb(238, 238, 238);"&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;1&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;2&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;3&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;4&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;5&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;6&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;7&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;8&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;9&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;10&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;11&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;12&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;13&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;14&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;15&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;16&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;17&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;18&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;19&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;20&lt;/li&gt;&lt;/ul&gt;&lt;ul class="pre-numbering" style="box-sizing: border-box; position: absolute; width: 50px; top: 0px; left: 0px; margin: 0px; padding: 6px 0px 40px; border-right-width: 1px; border-right-style: solid; border-right-color: rgb(221, 221, 221); list-style: none; text-align: right; background-color: rgb(238, 238, 238);"&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;1&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;2&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;3&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;4&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;5&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;6&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;7&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;8&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;9&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;10&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;11&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;12&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;13&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;14&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;15&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;16&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;17&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;18&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;19&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;20&lt;/li&gt;&lt;/ul&gt;  






 
从结果中可以看出，三个线程一共卖了5张票，即它们共同完成了买票的任务，实现了资源的共享。

区别总结：

1、在第二种方法（Runnable）中，ticket输出的顺序并不是54321，这是因为线程执行的时机难以预测，ticket–并不是原子操作。

2、在第一种方法中，我们new了3个Thread对象，即三个线程分别执行三个对象中的代码，因此便是三个线程去独立地完成卖票的任务；而在第二种方法中，我们同样也new了3个Thread对象，但只有一个Runnable对象，3个Thread对象共享这个Runnable对象中的代码，因此，便会出现3个线程共同完成卖票任务的结果。如果我们new出3个Runnable对象，作为参数分别传入3个Thread对象中，那么3个线程便会独立执行各自Runnable对象中的代码，即3个线程各自卖5张票。

3、在第二种方法中，由于3个Thread对象共同执行一个Runnable对象中的代码，因此可能会造成线程的不安全，比如可能ticket会输出-1（如果我们System.out….语句前加上线程休眠操作，该情况将很有可能出现），这种情况的出现是由于，一个线程在判断ticket为1&gt;0后，还没有来得及减1，另一个线程已经将ticket减1，变为了0，那么接下来之前的线程再将ticket减1，便得到了-1。这就需要加入同步操作（即互斥锁），确保同一时刻只有一个线程在执行每次for循环中的操作。而在第一种方法中，并不需要加入同步操作，因为每个线程执行自己Thread对象中的代码，不存在多个线程共同执行同一个方法的情况。

介绍完以上几个实例，我们下面对sleep（）、wait（）、yeid（）、join（）几个方法进行下区别总结

sleep方法与wait方法的区别：

1.sleep方法是静态方法，wait方法是非静态方法。 
2.sleep方法在时间到后会自己“醒来”，但wait不能，必须由其它线程通过notify（All）方法让它“醒来”。 
3.sleep方法通常用在不需要等待资源情况下的阻塞，像等待线程、数据库连接的情况一般用wait。

sleep/wait与yeld方法的区别：

调用sleep或wait方法后，线程即进入block状态，而调用yeld方法后，线程进入runnable状态。

wait与join方法的区别：

1.wait方法体现了线程之间的互斥关系，而join方法体现了线程之间的同步关系。 
2.wait方法必须由其它线程来解锁，而join方法不需要，只要被等待线程执行完毕，当前线程自动变为就绪。 
3.join方法的一个用途就是让子线程在完成业务逻辑执行之前，主线程一直等待直到所有子线程执行完毕。


顶尖程序员不同于常人的5个区别:

  原文作者：Edmond Lau
《THe Effective Engineer》：https://www.theeffectiveengineer.com/book




       程序员的区别，采访了很多硅谷顶级科技公司的顶尖软件工程师。他发现这些给世界带来巨大影响的的工程师们至少有以下5个共同的思维模式：

1.勇于去研究你不懂的代码

一般人都不愿意去研究自己不曾接触过的代码，很多人都没有尝试就放弃了。如果你经常去研究你没有接触过的代码，你就会越来越熟悉不同的代码结构和设计模式。现在人们很容易就接触到优秀的开源代码资源，你可以很方便的就下载下来做一些改动或者调试，去研究为什么代码可以这么写。

除了代码之外，很多人对于陌生的工作内容也会感到恐惧。每次换工作的时候，你可能都会遇到新公司的工作内容和以前工作的内容不一样的情况，以至于刚开始的时候工作效率没有以前那么高。很多人甚至觉得，他们是不是骗了面试官。

其实，大家都是在学习的过程中。在一个陌生的领域，没有人从一开始就是大神。如果你想变得越来越好，无论是写代码，与人沟通或者其它的技能，都是需要投入时间去学习的。

2.精通代码调试(debug)

很多人在写代码的过程中，经常会有的一个问题就是：为什么我写出来的代码不能运行？为什么运行的结果不是我想要的？

几乎所有的程序员写代码都不是一遍就能写好的。但是顶尖的程序员非常快的就明白自己代码的问题可能是什么。这是一个很重要的能力，但是偏偏学校里不教，面试的时候考官也不经常提及。

那么怎么去调试代码呢？其实核心就是以下几个方法：

不妨先猜测一下到底发生了什么。假设你的猜测是对的，想想你的猜测会导致程序有什么结果。试着观察这些结果有没有异常的地方。如果你没有发现异样，那么说明你的猜测就是对的。如果你发现了异样，那么说明你的猜测是错的，接下来换一个猜测试试。

对于顶尖程序员来说，这个过程在脑海中就是电光火石的一瞬间。只要你解决的问题足够多，你做出来的猜测就会越准确。

至于如何发现异样？你就需要有一套自己的工具或者方法论了。最简单的就是在代码里输出日志来判断。但是这是比较笨的办法，你需要去接触一些高级的工具或者直接带有Debug功能的编辑器。

3.重视能够节约时间的工具

最近打败人类的AlphaGo每天可以进行上百万局的下棋训练，我们人类一万个小时的训练却需要10年之久。也就是说，电脑运行几分钟，可能就等于人类工作好几年。

曾经在Facebook担任技术总监的Bobby Johnson描述过，高效率的程序员都把时间花在制作工具上。

很多人也认为工具是很重要的，但是他们并没有花时间去制作、整合自己的工具。但是，Jonson团队最出色的员工耗费了他们1/3的时间在工具制作上，这些工具可以用来发布代码，监控系统，以及能让他们花更少的时间去做更多事情。

总之，不要花时间去做机器可以代替你去做的事情。

4.优化你的迭代速度

假设你要花12秒钟去搜索某个函数是在哪里定义的。再假设你每天做这个动作60次，那么你每天就要花12分钟去搜索函数定义。

如果你用一个好一点的编辑器，每次找到函数定义只要2秒钟，那么你每天就会节约10分钟。每年你就可以节约40个小时。

如果你能找到3个这样的场景去优化一下，那么你每年可以节约一个月的时间。想想这一个月你可以做多少有意义的事情。

再假如你在调试一个App的bug的时候，改完一次代码都需要重启一下App，然后点击4、5次才能看到bug有没有改好。那么你是不是可以先花几分钟设置以下，让App一启动就转到显示Bug的页面呢？

千万不要小看这些琐碎的细节，改善它们的回报是巨大。

5.系统性的思考方式

当你在写代码的时候，你很容易就认为只要你按照需求实现了指定的功能，你的代码就写完了。但是这其实只是冰山一角。任何没有发布到生产环境的代码都不会产生任何价值。

如果想写出真正有影响力的代码，你需要从整个系统去理解你的工作：

你的代码和其他人写的代码在功能上是什么关系？你有没有好好测试你的代码？或者其他人是否很容易测试你的代码？为了部署你的代码，线上生产环境的代码是不是需要改动？新的代码会不会影响到已经运行的代码？在新的功能下，你的目标用户的行为是不是你期望的？你的代码有没有产生商业上的影响？

        这些问题都不是很容易就能回答的，但是在写代码的时候，你需要明白你的代码最后会不会得到最好的结果。这些只是顶尖程序员的一些基本思维方式，如果你想要更详细的了解在Google，Facebook，Twitter这些公司的顶尖程序员是如何工作的，去关注作者的博客或者他的书吧。

转自：http://www.jianshu.com/p/05de8f667eeahmsr=toutiao.io&amp;amp;utm_medium=toutiao.io&amp;amp;utm_source=toutiao.io



什么是集合竞价？
                   
       所谓集合竞价就是在当天还没有成交价的时候，你可根据前一天的收盘价和对当日股市的预测来输入股票价格，而在这段时间里输入计算机主机的所有价格都是平等的，不需要按照时间优先和价格优先的原则交易，而是按最大成交量的原则来定出股票的价位，这个价位就被称为集合竞价的价位，而这个过程被称为集合竞价。






1、  9:15—-9:20这五分钟开放式集合竞价可以委托买进和卖出的单子，你看到的匹配成交量可能是虚假的，因这5分钟是可以撤单，很多主力在9:19:30左右撤单，当你买进时，你不撤单，他可撤出，然后他卖给你，因此你一定要把撤单键放在手上。
2、  9:20—9:25这五分钟开放式集合竞价可以输委托买进和卖出的单子，但不能撤单，有的投资者认为他己撤单就完事了，事实上这五分钟撤单是无效的。这五分钟你看到的委托是真实的，因此要抢涨停板的，一定要看准这五分钟，但你不知道这五分钟哪些股票要涨停板，利用按61和63能看到。
3、   9:25—9:30 这五分钟不叫集合竞价时间，电脑这五分钟可接收买和卖委托，也可接收撤单，这五分钟电脑不处理，如果你进的委托价格估计能成交，那么你的撤单是排在后面来不及的，对于高手而言，这五分钟换股票一定要利用，比如你集合竞价卖出股票后，资金在9:25就可利用，你可在9:26买进另一只股票。
4、  深圳股票在收盘14:57—15:00是收盘集合竞价时间，这3分钟不能撤单。
5、  深圳停牌一小时的股票可以提前委托，上海停牌一小时的股票不能提前委托，上海交易在10:30准时开门，早一秒就弹回来了返回“废单”，对于连续涨停的股票，如果你要追进，应该在10:29:53—-10:29:56输入确认键，因网络速度时快时慢，要几秒钟在路上行走，相反出现重大利空时，你也可跑得快。对于深圳停牌一小时的股票。10:30开盘时，一般10:29分30秒，交易系统就已经打开了。所以如果你想抢先买入一只复牌即涨停的股票，最好在10:29分20秒挂涨停价买入，因为委单要进入证券公司系统再进行转换是需要几秒钟的时间，如果你挂早了传到上或深交所还没到10:29分30秒，交易所就把它当作废单处理，所以一定要把握好。
6、  9点25分才是集合竞价期间唯一一次真正的成交，所以会显示成交笔数。当然这期间可以挂单，也可以撤单，但9点20到9点25是不能撤单的，集合竞价期间最好不要撤单，成功概率很小，虽然允许撤单，但还有许多其它原因导致撤单不那么容易成功。所以如果你想买入一只股票，你直接挂涨停价买入即可，如果你想卖出一只股票，你直接挂跌停价即可，这样基本都可以买到或抛出，但它的实际成交价格不是你所挂的涨停价或跌停价，而是9点25分成交的那个价格，也就是开盘价。所有的人成交价格都是同一个价格，这个价格是根据最大成交量撮合出来的，当然如果是以涨停板和跌停板开盘的话，你就不一定可以买到，因为，集合竞价期间，价格第一优先，时间第二优先。
7、  9:15~9:20可以挂单也可以撤单！
9:20~9:25只能挂单，不接受撤单
9:25-9:30不能挂单也不能撤单
所以有些庄在9:15~9:20期间挂大单买入，引诱散户，到了9:20单子被他们偷偷撤了，这种股票很可能会跌，千万不要碰。
8.   集合竞价就是在9:15到9:25卖出或买入自己的股票。但集合竞价并非那么简单，首先如果你卖出股票，必须判断出集合竞价就是最高点，比方说，昨天涨停，今天高开到六，七个点附近，那集合竞价肯定是最高点，即使不是最高点，该股在开盘后肯定回落，毕竟有获利盘涌出，所以你要在集合竞价卖掉股票，然后在低点买入，或者观看，这就要看个股了。如果该股今天被黑嘴股评推荐，明天高开，一个字“出”。管它后面是否阳光灿烂，落袋为安。
9.   那么如何在集合竞价填单，这是个学问。比如你9:15填单，结果看到股价越来越低，等你犹豫的时候已经过了9:20，那撤单已经来不急了，所以最好的填单时间就是9:25之前，时间越接近9:25越好。那价格如何填写呢，你们知道集合竞价先是价格优先，然后才是时间优先，所以你只要在9:25之前报比当时价低一毛钱肯定成交，有的人担心，是不是报的价低了，自己就亏了，放心好了，你毕竟是散户，你的影响很小，如果成交，肯定是开盘价，不用担心自己的价报的低，报低是为了肯定能成交。
 举例1：开盘价的形成(集合竞价后撮合成交价的形成)
  比如某股票上一个交易日的收盘价为10元，在集合竞价时，报入的申买单、申卖单如下表所示：



根据成交价格的确定原则，10.01元将成为该股当日的开盘价，即集合竞价后所撮合的最终成交价。
       从上表中我们看到，9.99元、10元、10.01元、10.02元、10.03元、10.04元这六个价格是买单和卖单共同覆盖的价格，那么为什么最终撮合价是10.01元，而不是9.99元、10元或是10.02元呢？
      因为撮合的成交价格要满足三个条件，即(一)可实现最大成交量的价格；(二)高于该价格的买入申报与低于该价格的卖出申报全部成交的价格；(三)与该价格相同的买方或卖方至少有一方全部成交的价格。
  10.01元的成交价格，可使买单中高于10.01元的6000股全部成交，卖单中高于10.01元7000股全部成交，10.01元的4000股买单全部成交，10.01元的6000股卖单成交3000股，总成交量为1万股。(低于10.01元的买单和高于10.01元的卖单，以及未成交的3000股10.01元卖单将自动进入开盘后的连续竞价)
     如果成交价格为10元，则买单中高于10元的1万股就无法全部成交；如果成交价格为10.02元，则卖单中低于10.02元的13000股就无法全部成交。
  举例2：竞合竞价时若出现两个以上可撮合价格，上交所和深交所的不同选择
  比如某股票上一个交易日的收盘价为10元，在集合竞价时，报入的申买单、申卖单如下表所示：



 则根据“（一）可实现最大成交量的价格；（二）高于该价格的买入申报与低于该价格的卖出申报全部成交的价格；（三）与该价格相同的买方或卖方至少有一方全部成交的价格。”这三个条件，可撮合的价格为10.13元、10.14元、10.15元。
       如果该股是上交所上市的股票，则最终撮合的成交价为10.13元、10.14元、10.15元三个价格的中间价，即10.14元；如果该股是深交所上市的股票，则最终撮合的成交价为10.13元、10.14元、10.15元的中最接近上一个交易日10元收盘价的价格，即10.13元。
  举例3：若集合竞价时没有可撮合的成交价，上交所和深交所确定开盘价的不同之处
  比如某股票上一个交易日的收盘价为10元，在集合竞价时，报入的申买单、申卖单如以下3个表格所示：



表格B（最低申卖价低于上一交易日的收盘价）



表格C（最高申买价低于上一交易日的收盘价、最低申卖价高于上一交易日的收盘价）



       在以上表格A、表格B、表格C的三种情况中，根据上交所的规则“将开盘价空缺，将连续竟价后产生的第一笔成交价格作为开盘价”，三种情况的开盘价均空缺，9：30进入连续竞价后，成交的第一笔价格就作为该股当天的开盘价。
      而根据深交所“若最高申买价高于上一交易日的收盘价，就选取该价格为开盘价；若最低申卖价低于上一交易日的收盘价，就选取该价格为开盘价；若最高申买价低于上一交易日的收盘价、最低申卖价高于上一交易日的收盘价，则选取前一交易日的收盘价为当日开盘价。”的规则，表格A、表格B、表格C的情况，分别的开盘价为：10.18元、9.82元、10元。
 
集合竞价最大成交量原则：
      即以此价格成交能够得到最大成交量。高于集合竞价产生的价格的买入申报全部成交；低于集合竞价产生的价格的卖出申报全部成交；等于集合竞价产生的价格的买入或卖出申报，根据买入申报量，卖出申报量的多少，按少的一方的申报量成交。
      集合定价由电脑交易处理系统对全部申报按照价格优先、时间优先的原则排序，并在此基础上，找出一个基准价格，使它同时能满足以下3个条件：
1.成交量最大。
2.高于基准价格的买入申报和低于基准价格的卖出申报全部满足（成交）。
3.与基准价格相同的买卖双方中有一方申报全部满足（成交）。
      该基准价格即被确定为成交价格，集合竞价方式产生成交价格的全部过程，完全由电脑交易系统进行程序化处理，将处理后所产生的成交价格显示出来。这里需要说明的是：
第一，集合竞价方式下价格优先、时间优先原则体现在电脑主机将所有的买入和卖出申报按价格由高到低排出序列，同一价格下的申报原则按电脑主机接受的先后顺序排序；
第二，集合竞价过程中，若产生一个以上的基准价格，即有一个以上的价格同时满足集合竞价的3个条件时，沪市选取这几个基准价格的中间价格为成交价格，深市则选取离前收盘价最近的价格为成交价格。
总结集合竞价时间：
上海市场：9：15—9：25深圳市场：9：15—9：25（开盘集合竞价时间）14：57—15：00（收盘集合竞价时间）
集合竞价申报规定：
每个交易日9:15至9:25（深圳包括14:57至15:00），证券交易所交易主机接受参与竞价交易的申报。
每个交易日9:25至9:30，交易主机只接受申报，但不对买卖申报或撤销申报作处理。交易所认为必要时，可以调整接受申报时间。
集合竞价阶段撤单规定：
       沪深两市每个交易日9:25至9:30的开盘集合竞价阶段，两个交易所交易主机都不接受撤单申报。
不过在接受交易申报的其它时间内，两个交易所规定有所不同。沪市未成交申报在其接受交易申报的时间内可以撤销，具体为每个交易日9:15至9：25、9：30至11:30、13:00至15:00；而深交所规定14:57至15:00，深交所交易主机也不接受参与竞价交易的撤销申报，在它接受申报的其它时间里，未成交申报可以撤销。
 
你所不知道的竞价规则：
       “集合竞价规则”：集合竞价是指在每日的开盘前，即9：15—9：25这十分钟之内，投资者按照自己所能接受的心理价格自由地进行买卖申报，电脑交易主机系统对全部有效委托进行一次集中撮合处理过程。集合竞价规则是指在所谓集合竞价在成交时所遵守的一些原则。
一、集合竞价规则：三个原则
       集合竞价只有满足以下三个原则，才会形成交易，也就是说这三个原则是集合竞价必须满足的三个条件：
1.可实现最大成交量的价格;
2.高于该价格的买入申报与低于该价格的卖出申报全部成交的价格;
3.与该价格相同的买方或卖方至少有一方全部成交的价格。
       两个以上申报价格符合上述条件的，使未成交量最小的申报价格为成交价格;仍有两个以上使未成交量最小的申报价格符合上述条件的，其中间价为成交价格。
1）9：15----9：20这五分钟开放式集合竞价可以委托买进和卖出的单子，你看到的匹配成交量可能是虚假的，因这5分钟是可以撤单，很多主力在9：19：30左右撤单，当你买进时，你不撤单，他可撤出，然后他卖给你，因此你一定要把撤单键放在手上。
2）9：20---9：25这五分钟开放式集合竞价可以输委托买进和卖出的单子，但不能撤单，有的投资者认为他己撤单就完事了，事实上这五分钟撤单无效，白白输入的。这五分钟你看到的委托是真实的，因此要抢涨停板的，一定要看准这五分钟，但你不知道这五分钟哪些股票要涨停板，利用按81和83不能看到，但钱龙和分析家可以按81和83看到。
3）9：25---9：30这五分钟不叫集合竞价时间，电脑这五分钟可接收买和卖委托，也可接收撤单，这五分钟电脑不处理，如果你进的委托价格估计能成交，那么你的撤单是排在后面来不及的，对于高手而言，这五分钟换股票一定要利用，比如你集合竞价卖出股票后，资金在9：25就可利用，你可在9：26买进另一只股票。还有你要得到新股开盘价，你高价格得开盘价，你高出开盘价的部份在9：25返回，你又可利用这返回的资金。
4）9：25产生的开盘价都是按成交量最大化原则定出的，深圳股票在收盘14：57---15：00是收盘集合竞价时间，这3分钟不能撤单，只能输买进和卖出单子，因此深圳收盘价也是集合竞价得来的，而你看到的上海最后一笔只有价格，股数为0，这是因为上海收盘价格是按最后一笔倒推1分钟的加权平均价算出的，不是集合竞价得来的，在防止操纵收盘价方面不如深交所。
5）上海新开户当天不能买进股票，因为开户是在登记结算公司，交易是在上交所，两个机构之间数据库不能及时对接，要等晚上清算后接上，因此次日才能买股票，上海股票当天在其它营业部撤销指定交易，在另一个营业部当天可指定，但你的股票在新营业部看不到明细，只能根据你的记忆卖出。深圳股票上一个交易日办理转托管的，第二天可在新营业部卖出。
6）深圳停牌一小时的股票可以提前委托，上海停牌一小时的股票不能提前委托，上交易在10：30准时开门，早一秒就弹回来了返回“废单”，对于连续涨停的股票，如果你要追进，应该在10：29：53----10：29：56输入确认键，因网络速度时快时慢，要几秒钟在路上行走，相反出现重大利空时，你也可跑得快。
7）零股处理，配股可以买进零股，但正常买进时，你只能委托100股整数倍，如果你有零股要卖出，如57股，你只能一次性出，不能分两次卖零股，债券委托是一手整数倍，即而值1000元整数倍，新股申购时，中小板是500股整数倍，上海新股是1000股整数倍，新股申购委托后不能撤单，一个申购流程前后五个交易日，如周二申购，下周一解冻。
8）股票、基金、权证一笔委托数量不能超过100万份，特别是权证在末日轮时，几分钱不适合大资金操作。
9）没涨跌幅限制的股票有：新股上市第一天，股改对价股上市第一天，暂停交易的ST盈利后恢复交易第一天，新股第一天最高价上海不超过发行价2倍，中小板新股第一天最高价不超过发行价9倍，股改对价上市第一天虽无涨跌幅限制，但跌幅不超过50％，上交所是这样表述的，买卖无价格涨跌幅限制的证券，集合
竞价阶段的有效申报价格应符合下列规定：
（一）股票交易申报价格不高于前收盘价格的200％，并且不低于前收盘价格的50％；
（二）基金、债券交易申报价格最高不高于前收盘价格的150％，并且不低于前收盘价格的70％。
10）证券开市期间停牌的，停牌前的申报参加当日该证券复牌后的交易；停牌期间，可以继续申报，也可以撤销申报；复牌时对已接受的申报实行集合竞价，集合竞价期间不揭示虚拟开盘参考价格、虚拟匹配量、虚拟未匹配量。这种情况在权证末日轮交易所紧急停牌出现过，还有中小板上市当天换手超过70％的情况，都要停牌5分钟，记住这5分钟可以委托，也可撤单。
集合竞价时，成交价格如何确定
集合竞价的所有交易在9点25分以同一价格成交，且为开盘价.
      集合竞价期间显示的是虚拟成交，所以成交笔数都是零，目的是为了撮合出一个最大成交量的价格，如果有新的买单和抛单加入，每十秒就会进行一次撮合，显示的结果是在这个价格下的最大成交量，但笔数是零，这个时候价格一般会变，最大成交量即现手是逐渐增多的，因为只有在这个价格的撮合下它的成交量比上一次要大，才能取代上一次的成交价格，但如果有撤单的话，最大成交量也有可能减少，9点25分才是集合竞价期间唯一一次真正的成交，所以会显示成交笔数。当然这期间可以挂单，也可以撤单，但9点20到9点25是不能撤单的，集合竞价期间最好不要撤单，成功概率很小，虽然允许撤单，但还有许多其它原因导致撤单不那么容易成功。
所以如果你想买入一只股票，你直接挂涨停价买入即可，如果你想卖出一只股票，你直接挂跌停价即可，这样基本都可以买到或抛出，但它的实际成交价格不是你所挂的涨停价或跌停价，而是9点25分成交的那个价格，也就是开盘价。所有的人成交价格都是同一个价格，这个价格是根据最大成交量撮合出来的，当然如果是以涨停板和跌停板开盘的话，你就不一定可以买到，因为，集合竞价期间，价格第一优先，时间第二优先，还有一个数量第三优先，连续竞价期间没有这个优先，9点15分之前的所有时间为同一时间，故挂单时间相同，挂单价格也相同，由于主力挂单的数量比散户大，故先成交主力。
集合竞价按买盘的价格从高到低进行排序，按卖盘的价格从低到高进行排序，然后按照序号从上到下一对一对地进行撮合，直到买盘和卖盘的价格相同或卖盘的价格高于买盘的价格，就停止撮合，然后选出一个满足所有撮合的价格。最后显示当前的撮合价格和成交量，但笔数是零，因为只是撮合，没有实际成交，如果实际成交了就会显示笔数，并且笔数为买卖盘中成交和大的笔数，比如说买盘有六笔成交，卖盘有八笔成交，那么成交的笔数显示为八笔，如果说买盘只有一笔，卖盘有一百笔，则为主力大笔买入，反之主力大笔抛出。
集合竞价时，成交价格的确定原则为：
①可实现最大成交量的价格；
②高于该价格的买入申报与低于该价格的卖出申报全部成交的价格；
③与该价格相同的买方或卖方至少有一方全部成交的价格。
两个以上申报价格符合上述条件的，使未成交量最小的申报价格为成交价格；仍有两个以上使未成交量最小的申报价格符合上述条件的，其中间价为成交价格。
停牌期间不允许挂单，但停牌前的挂单可以撤销。
对于停牌一小时的股票，在停牌期间（9：30-10：30）不能挂单也不能撤单。当然也没有集合竞价，集合竞价期间的任何挂单和撤单都是无效的，这就要看十点半时谁的手快，但停牌前的挂单是有效的。
10：30开盘时，一般10：29分30秒，交易系统就已经打开了。所以如果你想抢先买入一只复牌即涨停的股票，最好在10：29分20秒挂涨停价买入，因为委单要进入证券公司系统再进行转换是需要几秒钟的时间，如果你挂早了传到上或深交所还没到10：29分30秒，交易所就把它当作废单处理，所以一定要把握好。
关于撤单问题：
9：15～9：20可以挂单也可以撤单！9:20～9:25只能挂单，不接受撤单，9：25-9：30不能挂单也不能撤单，所以有些庄在9：15～9：20期间挂大单买入，引诱散户，到了9：20单子被他们偷偷撤了。
在非交易时间内可以挂单，也可以撤单。
9时30分前委托再撤单要当心，因为撤单失败的机率比较大。
9时5分之后不要撤单，撤单基本不会成功，9时15分之后撤单可以。
营业部的转换系统在没有连到交易所时，委托再撤单是可以成功的。
营业部在每个交易日的9时5分左右,会将营业部的转换系统打开,这一系统负责将证券营业部客户的委托指令转换成DBF库数据发送至交易所。
9时15 分准时传送至交易所电脑主机。
另外还有深市的最后三分钟也是集合竞价的.
深市：14。57---15。00 收盘集合竞价时间.
 涨姿势:
集合竞价：就是在9:15 到9:25卖出或买入自己的股票。
但集合竞价并非那么简单，首先如果你卖出股票，必须判断出集合竞价就是最高点，比方说，昨天涨停，今天高开到六，七个点附近，那集合竞价肯定是最高点，即使不是最高点，该股在开盘后肯定回落，毕竟有获利盘涌出，所以你要在集合竞价卖掉股票，然后在低点买入，或者观看，这就要看个股了。如果该股今天被股评推荐，明天高开，一个字“出”。管它后面是否阳光灿烂，落袋为安。
那么如何在集合竞价填单，这是个学问？
比如你9:15填单，结果看到股价越来越低，等你犹豫的时候已经过了9:20，那撤单已经来不急了，所以最好的填单时间就是9:25之前，时间越接近9:25越好。
那价格如何填写呢，你们知道集合竞价先是价格优先，然后才是时间优先，所以你只要在9:25之前报比当时价低一毛钱肯定成交，有的人担心，是不是报的价低了，自己就亏了，放心好了，你毕竟是散户，你的影响很小，如果成交，肯定是开盘价，不用担心自己的价报的低，报低是为了肯定能成交。
一个股票符合我给的集合竞价条件时，如果你的单子能够在9：25以前委托成功，一般可以打一个比较高的价格，因为它会以开盘价成交。但是大多情况下，由于稍微片刻的犹豫，开盘价已经出来了。什么价格委托合适哪？
经验告诉我们：10元以下的股票，开盘价加0.1元委托比较合适。高于10远的运用下列公式开盘价+开盘价乘以0.015竞价。即高于开盘价的1.5%的价格去挂单，一般都能成交。开盘价出来后马上委托。若犹豫几分钟，就不好说了。所以新股操作，强调纪律，做好计划，不能临时随意变化。
挂单还跟大盘趋势，高低位，股票质地，价格有关。
大盘不好，在高位，开盘后易出下影线。
与价格有关，价格低，很少有下影线，于股票质地有关，必炒得股票，由于众多机构，游资抢筹，就无下影线。
集合竞价的基本过程：
设股票G在开盘前分别有5笔买入委托和6笔卖出委托，根据价格优先的原则，按买入价格由高至低和卖出价格由低至高的顺序将其分别排列如下：
序号 | 委托买入价 | 数量（手） 序号 | 委托卖出价 |数量（手）
1 。。。。3．80。。。 2——————1。。。。3．52。。。。5
2 。。。。3．76。。。6——————2。。。。3．57。。。。1
3 。。。。3．65。。。4——————3。。。。
 3．60。。。。2
4 。。。。3．60。。。7——————4。。。。3．65。。。。6
5 。。。。3．54。。。6——————5。。。。3．70。。。。6
按不高於申买价和不低於申卖价的原则，首先可成交第一笔，即3．80元买入委托和3．52元的卖出委托，若要同时符合申买者和申卖者的意愿，其成交价格必须是在3．52元与3．80元之间，但具体价格要视以后的成交情况而定。     这对委托成交后其它的委托排序如下：
序号 | 委托买入价 | 数量（手） 序号 | 委托卖出价 |数量（手）
1第一笔买入的已经全部成交这里为空 1。。。。3．52。。。。3这里是经过第一笔成交后委卖由于多了3手而剩下的
2 。。。。3．76。。。。6—————2。。。。3．57。。。。1
3 。。。。3．65。。。。4—————3。。。。3．60。。。。2
4 。。。。3．60。。。。7—————4。。。。
 3．65。。。。6
5 。。。。3．54。。。。6—————5。。。。3．70。。。。6
在第一次成交中，由于卖出委托的数量多于买入委托，按交易规则，序号1的买入委托2手全部成交，序号1的卖出委托还剩余3手。
第二笔成交情况：序号2的买入委托价格为不高于3．76元，数量为6手。在卖出委托中，序号1—3的委托的数量正好为6手，其价格意愿也符合要求，正好成交，其成交价格在3．60元—3．76元的范围内，成交数量为6手。应注意的是，第二笔成交价格的范围是在第一笔成交价格的范围之内，且区间要小一些。    第二笔成交后剩下的委托情况为：
序号委托买入价数量（手） 序号委托卖出价数量（手）
3。。。。3．65。。。。4
4 。。。。3．60。。。。7 4。。。。3．65。。。。6
5 。。。。3．54。。。。6 5。。。。3．70。。。.6
第三笔成交情况：序号3的买入委托其价格要求不超过3．65元，而卖出委托序号4的委托价格符合要求，这样序号3的买入委托与序号4的卖出委托就正好配对成交，其价格为3．65元，因卖出委托数量大于买入委托，故序号4的卖出委托仅只成交了4手。第三笔成交后的委托情况如下：
序号 委托买入价 数量（手） 序号委托卖出价数量（手）
4 。。。。3．60。。。。74.。。。。3．65。。。。2
5 。。。。3．54。。。。6 5。。。。3．70。。。。6
完成以上三笔委托後，因最高买入价为3．60元，而最低卖出价为3．65，买入价与卖出价之间再没有相交部分，所以这一次的集合竞价就已完成，最後一笔的成交价就为集合竞价的平均价格。剩下的其他委托将自动进入开盘后的连续竞价。
在以上过程中，通过一次次配对，成交的价格范围逐渐缩小，而成交的数量逐渐增大，直到最后确定一个具体的成交价格，并使成交量达到最大。在最后一笔配对中，如果买入价和卖出价不相等，其成交价就取两者的平均。
在这次的集合竞价中，三笔委托共成交了12手，成交价格为3．65元，按照规定，所有这次成交的委托无论是买入还是卖出，其成交价都定为3．65元，交易所发布的股票G的开盘价就为3．65元，成交量12手。
当股票的申买价低而申卖价高而导致没有股票成交时，上海股市就将其开盘价空缺，将连续竞价后产生的第一笔价格作为开盘价。而深圳股市对此却另有规定：
若最高申买价高于前一交易日的收盘价，就选取该价格为开盘价；若最低申卖价低于前一交易日的收盘价，就选取该价格为开盘价；若最低申买价不高于前一交易日的收盘价、最高申卖价不低于前一交易日的收盘价，则选取前一交易日的收盘价为今日的开盘价。
集合竞价原则- 开放式集合竞价:
开放式集合竞价制度是指在集合竞价期间，即时行情实时揭示集合竞价参考价格等信息的集合竞价方式。新《交易规则》中，根据交易时间不同，开放式集合竞价分为开盘开放式集合竞价和收盘开放式集合竞价两种。
所谓集合竞价就是在当天还没有成交价的时候，欲要入市的投资者可根据前一天的收盘价和对当日股市的预测来输入股票价格，而在这段时间里输入计算机主机的所有价格都是平等的，不需要按照时间优先和价格优先的原则交易，而是按最大成交量的原则来定出股票的价位，这个价位就被称为集合竞价的价位，而这个过程就被称为集合竞价。

          什么是JSON？

         JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式。它基于ECMAScript的一个子集。
 JSON采用完全独立于语言的文本格式，但是也使用了类似于C语言家族的习惯（包括C、C++、C#、Java、JavaScript、Perl、Python等）。这些特性使JSON成为理想的数据交换语言。
 易于人阅读和编写，同时也易于机器解析和生成(一般用于提升网络传输速率)。 
         JSON  VS   XML：
    1.JSON和XML的数据可读性基本相同
    2.JSON和XML同样拥有丰富的解析手段
    3.JSON相对于XML来讲，数据的体积小
    4.JSON与JavaScript的交互更加方便
    5.JSON对数据的描述性比XML较差
    6.JSON的速度要远远快于XML
    android2.3提供的json解析类 ：android的json解析部分都在包org.json下，主要有以下几个类： 
    JsonObject：可以看作是一个JSON对象,这是系统中有关JSON定义的基本单元，其包含一对儿(Key/Value)数值。它对外部(External：   应用toString()方法输出的数值)调用的响应体现为一个标准的字符串（例如：{"JSON": "Hello, World"}，最外被大括号包裹，其中的Key和Value被冒号":"分隔）。其对于内部(Internal)行为的操作格式略微，例如：初始化一个JSONObject实例，引用内部的put()方法添加数值：new JSONObject().put("JSON","Hello,
 World!")，在Key和Value之间是以逗号","分隔。Value的类型包括：Boolean、JSONArray、JSONObject、Number、String或者默认值JSONObject.NULL object 。
    JSONStringer：JSON文本构建类 ，根据官方的解释，这个类可以帮助快速和便捷的创建JSON text。其最大的优点在于可以减少由于 格式的错误导致程序异常，引用这个类可以自动严格按照JSON语法规则（syntax rules）创建JSON text。每个JSONStringer实体只能对应创建一个JSONtext。。其最大的优点在于可以减少由于格式的错误导致程序异常，引用这个类可以自动严格按照JSON语法规则（syntax rules）创建JSON text。每个JSONStringer实体只能对应创建一个JSONtext。
    JSONArray：它代表一组有序的数值。将其转换为String输出(toString)所表现的形式是用方括号包裹，数值以逗号”,”分隔（例如：     [value1,value2,value3]，大家可以亲自利用简短的代码更加直观的了解其格式）。这个类的内部同样具有查询行为，     get()和opt()两种方法都可以通过index索引返回指定的数值，put()方法用来添加或者替换数值。同样这个类的value类型可以包括：Boolean、JSONArray、JSONObject、Number、String或者默认值JSONObject.NULL
 object。
    JSONTokener：json解析类 
    JSONException：json中用到的异常 
   JSONObject, JSONArray来构建JSON文本 
代码  
   1. // 假设现在要创建这样一个JSON文本  
   2. //  {  
   3. //      "phone" :["12345678", "87654321"], // 数组  
   4. //      "name" :"yuanzhifei89", // 字符串  
   5. //      "age" : 100, // 数值  
   6. //      "address" : {"country" : "china", "province" :"jiangsu" }, // 对象  
   7. //      "married" : false // 布尔值  
   8. //  }  
   9.   
  10. try {  
  11.     // 首先最外层是{}，是创建一个对象  
  12.     JSONObject person = new JSONObject();  
  13.     // 第一个键phone的值是数组，所以需要创建数组对象  
  14.     JSONArray phone = new JSONArray();  
  15.    phone.put("12345678").put("87654321");  
  16.     person.put("phone", phone);  
  17.   
  18.     person.put("name","yuanzhifei89");  
  19.     person.put("age", 100);  
  20.     // 键address的值是对象，所以又要创建一个对象  
  21.     JSONObject address = new JSONObject();  
  22.     address.put("country", "china"); 
  23.     address.put("province","jiangsu");  
  24.     person.put("address", address);   
  25.     person.put("married", false);  
  26. } catch (JSONException ex) {  
  27.     // 键为null或使用json不支持的数字格式(NaN, infinities)  
  28.     throw new RuntimeException(ex);  
  29. }  
getType和optTypeapi的使用 :
    getType可以将要获取的键的值转换为指定的类型，如果无法转换或没有值则抛出JSONException 
    optType也是将要获取的键的值转换为指定的类型，无法转换或没有值时返回用户提供或这默认提供的值 
代码 
   1. try {  
   2.     // 所有使用的对象都是用上面创建的对象  
   3.     // 将第一个电话号码转换为数值和将名字转换为数值  
   4.     phone.getLong(0);  
   5.     person.getLong("name"); // 会抛异常，因为名字无法转换为long        
   6.     phone.optLong(0); // 代码内置的默认值  
   7.     phone.optLong(0, 1000); // 用户提供的默认值  
   8.     person.optLong("name");  
   9.     person.optLong("name", 1000); // 不像上面那样抛异常，而是返回1000  
  10. } catch (JSONException ex) {  
  11.     // 异常处理代码  
  12. }  
除了上面的两个类，还可以使用JSONStringer来构建json文本 
Java代码 ：
   1. try {  
   2.     JSONStringer jsonText = new JSONStringer(); 
   3.     // 首先是{，对象开始。object和endObject必须配对使用  
   4.     jsonText.object();  
   5.       
   6.     jsonText.key("phone");  
   7.     // 键phone的值是数组。array和endArray必须配对使用  
   8.     jsonText.array();  
   9.    jsonText.value("12345678").value("87654321");  
  10.     jsonText.endArray();  
  11.       
  12.     jsonText.key("name");  
  13.     jsonText.value("yuanzhifei89");  
  14.     jsonText.key("age");  
  15.     jsonText.value(100);  
  16.       
  17.     jsonText.key("address");  
  18.     // 键address的值是对象  
  19.     jsonText.object();  
  20.     jsonText.key("country");  
  21.     jsonText.value("china");  
  22.     jsonText.key("province");  
  23.     jsonText.value("jiangsu");  
  24.     jsonText.endObject();  
  25.       
  26.     jsonText.key("married");  
  27.     jsonText.value(false);  
  28.       
  29.     // }，对象结束  
  30.     jsonText.endObject();  
  31. } catch (JSONException ex) {  
  32.     throw new RuntimeException(ex);  
  33. }  
json文本解析类JSONTokener 
按照RFC4627规范将json文本解析为相应的对象。 
对于将json文本解析为对象，只需要用到该类的两个api： 
构造函数 
public Object nextValue(); 
代码  :
   1. //  {  
   2. //      "phone" :["12345678", "87654321"], // 数组  
   3. //      "name" :"yuanzhifei89", // 字符串  
   4. //      "age" : 100, // 数值  
   5. //      "address" : { "country": "china", "province" : "jiangsu" }, // 对象  
   6. //      "married" : false // 布尔值  
   7. //  }  
   8.   
   9. private static final String JSON =   
  10. "{" +  
  11.     "   \"phone\" :[\"12345678\", \"87654321\"]," +  
  12.     "   \"name\" :\"yuanzhifei89\"," +  
  13.     "   \"age\" : 100," +  
  14.     "   \"address\" : {\"country\" : \"china\", \"province\" :\"jiangsu\" }," +  
  15.     "   \"married\" : false," + 
  16. "}";  
  17.   
  18. try {  
  19.     JSONTokener jsonParser = new JSONTokener(JSON);  
  20.     // 此时还未读取任何json文本，直接读取就是一个JSONObject对象。  
  21.     // 如果此时的读取位置在"name" : 了，那么nextValue就是"yuanzhifei89"（String）  
  22.     JSONObject person = (JSONObject)jsonParser.nextValue();  
  23.     // 接下来的就是JSON对象的操作了  
  24.     person.getJSONArray("phone");  
  25.     person.getString("name");  
  26.     person.getInt("age");  
  27.     person.getJSONObject("address");  
  28.     person.getBoolean("married");  
  29. } catch (JSONException ex) {  
  30.     // 异常处理代码  
  31. }  
其它的api基本就是用来查看json文本中的文本的 代码  ：
   1. try {  
   2.     JSONTokener jsonParser = new JSONTokener(JSON); 
   3.     // 继续向下读8个json文本中的字符。此时刚开始，即在{处  
   4.     jsonParser.next(8); //{    "phone。tab算一个字符  
   5.       
   6.     // 继续向下读1个json文本中的字符  
   7.     jsonParser.next(); //"  
   8.       
   9.     // 继续向下读取一个json文本中的字符。该字符不是空白、同时也不是注视中的字符  
  10.     jsonParser.nextClean(); //:  
  11.       
  12.     // 返回当前的读取位置到第一次遇到'a'之间的字符串（不包括a）。  
  13.     jsonParser.nextString('a'); //  ["12345678","87654321"],    "n（前面有两个空格）  
  14.       
  15.     // 返回当前读取位置到第一次遇到字符串中(如"0089")任意字符之间的字符串，同时该字符是trimmed的。（此处就是第一次遇到了89）  
  16.     jsonParser.nextTo("0089"); //me" :"yuanzhifei  
  17.       
  18.     // 读取位置撤销一个  
  19.     jsonParser.back();  
  20.     jsonParser.next(); //i  
  21.       
  22.     // 读取位置前进到指定字符串处（包括字符串）  
  23.     jsonParser.skipPast("address");  
  24.     jsonParser.next(8); //" : { "c  
  25.       
  26.     // 读取位置前进到执行字符处（不包括字符）  
  27.     jsonParser.skipTo('m');  
  28.     jsonParser.next(8); //married"  
  29. } catch (JSONException ex) {  
  30.     // 异常处理代码  
  31. }  
      以下是一个标准的JSON请求实现过程：
01 HttpPost request = new HttpPost(url); 
02 // 先封装一个 JSON 对象 
03 JSONObject param = new JSONObject(); 
04 param.put("name", "rarnu"); 
05 param.put("password", "123456"); 
06 // 绑定到请求 Entry 
07 StringEntity se = new StringEntity(param.toString());  
08 request.setEntity(se); 
09 // 发送请求 
10 HttpResponse httpResponse = new DefaultHttpClient().execute(request); 
11 // 得到应答的字符串，这也是一个 JSON 格式保存的数据 
12 String retSrc = EntityUtils.toString(httpResponse.getEntity()); 
13 // 生成Json对象 
14 JSONObject result = new JSONObject( retSrc); 
15 String token = result.get("token");
        下面这个是自己修改别人的小例子，主要是加一些注释和讲解，这个例子主要是使用android进行json解析。
1 单数据{'singer':{'id':01,'name':'tom','gender':'男'}}
2 多个数据{"singers":[
3        {'id':02,'name':'tom','gender':'男'},
4         {'id':03,'name':'jerry,'gender':'男'},
5 {'id':04,'name':'jim,'gender':'男'},
6 {'id':05,'name':'lily,'gender':'女'}]}
        下面的类主要是解析单个数据parseJson（）和多个数据的方法parseJsonMulti（）:
01 public class JsonActivity extends Activity { 
02    /** Called when the activity is first created. */ 
03    private TextView tvJson; 
04    private Button btnJson; 
05    private Button btnJsonMulti; 
06    @Override 
07    public void onCreate(Bundle savedInstanceState) { 
08        super.onCreate(savedInstanceState); 
09        setContentView(R.layout.main); 
10        tvJson = (TextView) this.findViewById(R.id.tvJson); 
11        btnJson = (Button)this.findViewById(R.id.btnJson); 
12        btnJsonMulti = (Button)this.findViewById(R.id.btnJsonMulti); 
13        btnJson.setOnClickListener(newView.OnClickListener() { 
14            @Override 
15            public void onClick(View v) { 
16                // url 
17                // String strUrl ="http://10.158.166.110:8080/AndroidServer/JsonServlet"; 
18                String strUrl =ServerPageUtil.getStrUrl(UrlsOfServer.JSON_SINGER); 
19                //获得返回的Json字符串 
20                String strResult =connServerForResult(strUrl); 
21                //解析Json字符串 
22               parseJson(strResult); 
23            } 
24        }); 
25        btnJsonMulti.setOnClickListener(newView.OnClickListener() { 
26            @Override 
27            public void onClick(View v) { 
28                String strUrl =ServerPageUtil.getStrUrl(UrlsOfServer.JSON_SINGERS); 
29                String strResult =connServerForResult(strUrl); 
30                //获得多个Singer 
31               parseJsonMulti(strResult); 
32            } 
33        }); 
34    } 
35    private String connServerForResult(String strUrl) { 
36        // HttpGet对象 
37        HttpGet httpRequest = new HttpGet(strUrl); 
38        String strResult = ""; 
39        try { 
40            // HttpClient对象 
41            HttpClient httpClient = newDefaultHttpClient(); 
42            // 获得HttpResponse对象 
43            HttpResponse httpResponse =httpClient.execute(httpRequest); 
44            if (httpResponse.getStatusLine().getStatusCode()== HttpStatus.SC_OK) { 
45                // 取得返回的数据 
46                strResult =EntityUtils.toString(httpResponse.getEntity()); 
47            } 
48        } catch (ClientProtocolException e) { 
49            tvJson.setText("protocolerror"); 
50            e.printStackTrace(); 
51        } catch (IOException e) { 
52            tvJson.setText("IOerror"); 
53            e.printStackTrace(); 
54        } 
55        return strResult; 
56    } 
57    // 普通Json数据解析 
58    private void parseJson(String strResult) { 
59        try { 
60            JSONObject jsonObj = newJSONObject(strResult).getJSONObject("singer"); 
61            int id =jsonObj.getInt("id"); 
62            String name =jsonObj.getString("name"); 
63            String gender =jsonObj.getString("gender"); 
64            tvJson.setText("ID号"+id + ", 姓名：" + name + ",性别：" + gender); 
65        } catch (JSONException e) { 
66            System.out.println("Json parseerror"); 
67            e.printStackTrace(); 
68        } 
69    } 
70    //解析多个数据的Json
71    private void parseJsonMulti(String strResult) { 
72        try { 
73            JSONArray jsonObjs = newJSONObject(strResult).getJSONArray("singers"); 
74            String s = ""; 
75            for(int i = 0; i &lt;jsonObjs.length() ; i++){ 
76                JSONObject jsonObj =((JSONObject)jsonObjs.opt(i)) 
77               .getJSONObject("singer"); 
78                int id =jsonObj.getInt("id"); 
79                String name =jsonObj.getString("name"); 
80                String gender =jsonObj.getString("gender"); 
81                s +=  "ID号"+id + ", 姓名：" + name + ",性别：" + gender+ "\n" ; 
82            } 
83            tvJson.setText(s); 
84        } catch (JSONException e) { 
85            System.out.println("Jsonsparse error !"); 
86            e.printStackTrace(); 
87        } 
88    } 
89 } 
                                                                                                                     使用总结：
第一步，首先根据网址获取Json格式的字符串，方法如下：
      以下方法的返回值就是JSON格式的字符串：
   String strurl_1 = "http://api.map.baidu.com/telematics/v3/weather?location=北京&amp;output=json&amp;ak=5slgyqGDENN7Sy7pw29IUvrZ";//北京的天气接口。
   public String connServerForResult(String strUrl) {
// HttpGet对象
HttpGet httpRequest = new HttpGet(strUrl);
String strResult = "";
try {

// HttpClient对象
HttpClient httpClient = new DefaultHttpClient();
HttpParams params = httpClient.getParams();
// 请求超时5秒 接受超时5秒
HttpConnectionParams.setConnectionTimeout(params, 5000);
HttpConnectionParams.setSoTimeout(params, 5000);
// 获得HttpResponse对象
HttpResponse httpResponse = httpClient.execute(httpRequest);
if (httpResponse.getStatusLine().getStatusCode() == HttpStatus.SC_OK) {
// 取得返回的数据
strResult = EntityUtils.toString(httpResponse.getEntity());
} else {
return strResult;
}
} catch (ClientProtocolException e) {
System.out.println("protocol error");
e.printStackTrace();
} catch (IOException e) {
System.out.println("IO error");
e.printStackTrace();
}
return strResult;
   }

第二步，通过上面的方法已经获得了JSON格式的字符串，假设StringstrResult=connServerForResult(strurl_1);
        下面虚线中的的字符串就是根据实际运行获得的JSON格式的字符串strResult：
    "error":0,
    "status":"success",
    "date":"2014-04-01",
    "results":[
        {
           "currentCity":"\u5317\u4eac",
            "weather_data":[
                {
                   "date":"\u5468\u4e8c(\u4eca\u5929,\u5b9e\u65f6\uff1a22\u2103)",
                   "dayPictureUrl":"http:\/\/www.baidu.com\/aladdin\/img\/new_weath\/bigicon\/7.png",
                   "nightPictureUrl":"http:\/\/api.map.baidu.com\/images\/weather\/night\/yin.png",
                   "weather":"\u973e\u8f6c\u9634",
                   "wind":"\u5fae\u98ce",
                   "temperature":"23 ~ 11\u2103",
                   "dayPictureurl":"http:\/\/api.map.baidu.com\/images\/weather\/day\/mai.png"
                },
                {
                   "date":"\u5468\u4e09",
                   "dayPictureUrl":"http:\/\/api.map.baidu.com\/images\/weather\/day\/yin.png",
                   "nightPictureUrl":"http:\/\/api.map.baidu.com\/images\/weather\/night\/yin.png",
                   "weather":"\u9634",
                   "wind":"\u5fae\u98ce",
                   "temperature":"21 ~ 11\u2103"
                },
                {
                   "date":"\u5468\u56db",
                   "dayPictureUrl":"http:\/\/api.map.baidu.com\/images\/weather\/day\/qing.png",
                   "nightPictureUrl":"http:\/\/api.map.baidu.com\/images\/weather\/night\/qing.png",
                   "weather":"\u6674",
                   "wind":"\u5317\u98ce3-4\u7ea7",
                   "temperature":"21 ~ 10\u2103"
                },
                {
                   "date":"\u5468\u4e94",
                   "dayPictureUrl":"http:\/\/api.map.baidu.com\/images\/weather\/day\/qing.png",
                   "nightPictureUrl":"http:\/\/api.map.baidu.com\/images\/weather\/night\/qing.png",
                   "weather":"\u6674",
                   "wind":"\u5fae\u98ce",
                   "temperature":"24 ~ 9\u2103"
                }
            ]
        }
    ]
}

第三步：现在已经获取到了JSON格式的字符串，字符串的信息是北京市4天的天气信息，下面就是对它的解析方法。
       为方便起见，我们假设要获取第一天的气温、天气、日期的信息，即"23 ~ 11\u2103"、"\u973e\u8f6c\u9634"、"\u5468        \u4e8c(\u4eca\u5929,\u5b9e\u65f6\uff1a22\u2103)",那么我们要这么做：

               try {
JSONObject jsonObj1 = new JSONObject(strResult);
JSONArray array1=jsonObj1.getJSONArray("results");
JSONObject array1_2=array1.getJSONObject(0);
JSONArray array1_3=array1_2.getJSONArray("weather_data");
////////////////////////////////////////////////////////////////////////////////////////////////////////////
                                
String weather_0=array1_3.getJSONObject(0).getString("weather");//今天天气
String date_0=array1_3.getJSONObject(0).getString("date");//今天星期
String temp_0 =array1_3.getJSONObject(0).getString("temperature");//今天气温
 ///////////////////////////////////////////////////////////////////////////////////////////////////////////

                  } catch(JSONException e) {
System.out.println("Json parse error");
e.printStackTrace();
 }
第四步:通过第三步已经能够获取第一天的天气信息了，如果要获取第二天或者第三天的信息，只需要把上面斜杠（/）之间的代码中的0改为1或者2就可以了，如果要获取第四天的信息，就改为3，这是因为数组下标是从0开始的。
        现在你或许会奇怪，为什么上面的信息里面没有汉字，都是些带有反斜杠的字符呢？不要担心，也不要作任何修改，那是国际统一标识码，每一个码对应一个汉字或者其他国家的什么语言，计算机会自动读取出其对应的汉字的。例如System.out.println("\u5fae");显示的一定是一个汉字，无需深究，也没必要知道。

第五步:下面来详细说明一下第三步是如何解析JSON的，其原理是什么？如果不是数组又该如何解析？ 
        1、首先，请看，上面的JSON格式的字符串整个都是在一对大括号{}里面的，那么这整个大括号里面的全部内容就是一个
        JSONObject对象，那么就通过JSONObject jsonObj1 = new JSONObject(strResult)来获取这个JSONObject对象，其中
        strResult等于那整个大括号包括大括号里面的全部内容的字符串。
        2、想必你已经看到最外层大括号里面有"error"，"status"，"date"，"results"等数据。如果要读取"error"中的值，可以        这么做，String error=jsonObj1.getString("error");至于"status"和"date"中的值的读取方法跟它一样，类比一下就行        了。我们发现"results"的值是一个数组，这是因为JSON中中括号[]就是数组的标志。如果要读取这个数组，你可以这么做
        ，JSONArrayarray1=jsonObj1.getJSONArray("results") 。而你发现数组"results"里面有一对大括号{}，把所有数据都       包含在里面了，嗯，那么这个大括号本身就是这个数组的第0个元素，而这个数组元素是一个JSONObject对象，通过            JSONObject array1_2=array1.getJSONObject(0)来获取。而你又发现array1_2这个JSONObject对象里面有 "currentCity"和
      "weather_data"两个元素，那么通过String currentCity=array1_2.getString("currentCity")来读取"currentCity"的值，
      通过JSONArrayarray1_3=array1_2.getJSONArray("weather_data")来读取这个"weather_data"数组。
       3、此时，你看到，"weather_data"数组里面有4对大括号，那么这4对大括号就代表这个数组中的4个元素，每个元素又是一      个JSONObject对象（这是因为JSONObject对象是以大括号为标志的，大括号里面是键—值对），可以通过JSONObject              ob1=array1_3.getJSONObject(0)来获取，而其中的"weather"的值，则通过String weather_0=ob1.getString("weather")来获     取
  ，我在第三步中没有分开写，而是写在一起了，即Stringweather_0=array1_3.getJSONObject(0).getString("weather");
     但本质上都是一样的。
     4、其实不管是对JSON数组的读取还是对普通对象的读取，其本质上没有大的区别，你只需要记住一点，一对大括号{}就是一个JSONObject对象！一对中括号[]就是一个JSONArray数组！一对大括号即一个JSONObject对象中可以包含多个普通键值对、JSONObject对象和JSONArray数组！一对中括号即一个JSONArray数组中也可以包含多个普通键值对、JSONObject对象和JSONArray数组。
     
   
 

 boost锁的概述：
boost库中提供了mutex类与lock类，通过组合可以轻易的构建读写锁与互斥锁。
        举个通俗的例子，如果共享资源是一个自动锁住的房间，互斥体是钥匙，进入房间必须取钥匙，离开房间应该还钥匙。这就对应着互斥体的lock（取钥匙）和unlock（还钥匙）。
       动态分配内存存在类似的情况。如果忘记delete，会导致内存泄漏。它是如何解决的？在栈上分配对象，要一个特点，那就是离开作用域后，对象肯定要调用析构方法。利用这个特点，可以使用对象对指针封装，在对象的析构方法中进行delete，就保证了一定会执行delete。这就是智能指针。因为智能指针，不是针对一个特定类型的指针，因此把智能指针设计为类的模版，根据模版实参特例化一个模板类。同样道理，也可以使用相同技术，对互斥体封装。这就是
 lock类模版。在lock的构造方法调用互斥体的lock方法，在lock的析构方法调用互斥体的unlock方法。mutex是对象类，lock是模板类。 
 mutex对象类：
      mutex类主要有两种：boost::mutex，boost::shared_mutex，其中mutex有lock和unlock方法，shared_mutex除了提供lock和unlock方法外，还有shared_lock和shared_unlock方法。因此，boost::mutex为独占互斥类，boost::shared_mutex为共享互斥类。
 
lock模板类：
      boost::unique_lock&lt;T&gt;，boost::shared_lock&lt;T&gt;，其中unique_lock为独占锁，shared_lock为共享锁。unique_lock&lt;T&gt;中的T可以为mutex类中的任意一种，如果为shared_mutex，那么boost::unique_lock&lt;boost::shared_mutex&gt;类的对象构造函数构造时，会自动调用shared_mutex的shared_lock方法，析构函数里，会自动调用shared_mutex的shared_unlock方法。如果是boost::
 unique_lock&lt;boost::mutex&gt;，则分别自动调用lock和unlock方法。
boost::shared_lock&lt;T&gt;中的T只能是shared_mutex类。
 
读写锁的实现：
typedefboost::shared_lock&lt;boost::shared_mutex&gt; readLock;
typedef boost::unique_lock&lt;boost::shared_mutex&gt; writeLock;
 
boost::shared_mutex rwmutex;
 
void readOnly( )
{
       readLock  rdlock( rwmutex );
       /// do something
}
 
void writeOnly( )
{
       writeLock  wtlock( rwmutex );
       /// do something
}
      对同一个rwmutex，线程可以同时有多个readLock，这些readLock会阻塞任意一个企图获得writeLock的线程，直到所有的readLock对象都析构。如果writeLock首先获得了rwmutex，那么它会阻塞任意一个企图在rwmutex上获得readLock或者writeLock的线程。boost::shared_lock使用要小心，千万不要同一个线程多次进入。
 
互斥锁的实现：
typedef boost::unique_lock&lt;boost::mutex&gt; exclusiveLock;
 
递归式的互斥量：
      boost::recursive_mutex提供一个递归式的互斥量。对于一个实例最多允许一个线程拥有其锁定，如果一个线程已经锁定一个boost::recursive_mutex实例，那么这个线程可以多次锁定这个实例。
boost::mutex::scoped_lock
boost::mutexio_mutex;
void foo( )
{
       {
               boost::mutex::scoped_lock lock( io_mutex);         /// 锁定
       } /// 解锁
}
 

       C++静态成员变量和静态成员函数使用总结：
一．静态成员变量： 
        类体中的数据成员的声明前加上static关键字，该数据成员就成为了该类的静态数据成员。和其他数据成员一样，静态数据成员也遵守public/protected/private访问规则。同时，静态数据成员还具有以下特点： 

1.静态数据成员的定义。 
        静态数据成员实际上是类域中的全局变量。所以，静态数据成员的定义(初始化)不应该被放在头文件中。 
其定义方式与全局变量相同。举例如下： 
xxx.h文件 
class base{ 
private: 
static const int _i;//声明，标准c++支持有序类型在类体中初始化,但vc6不支持。 
}; 

xxx.cpp文件 
const int base::_i=10;//定义(初始化)时不受private和protected访问限制. 
备注：不要在头文件中定义(初始化)静态数据成员。在大多数的情况下，这样做会引起重复定义这样的错误。即使加上#ifndef #define #endif或者#pragma once也不行。 
2.静态数据成员被 类 的所有对象所共享，包括该类派生类的对象。即派生类对象与基类对象共享基类的静态数据成员。举例如下： 
class base{ 
public : 
static int _num;//声明 
}; 
int base::_num=0;//静态数据成员的真正定义 

class derived:public base{ 
}; 

main() 
{ 
base a; 
derived b; 
a._num++; 
cout&lt;&lt;"base class static data number _numis"&lt;&lt;a._num&lt;&lt;endl; 
b._num++; 
cout&lt;&lt;"derived class static data number _numis"&lt;&lt;b._num&lt;&lt;endl; 
} // 结果为1,2;可见派生类与基类共用一个静态数据成员。 

3.静态数据成员可以成为成员函数的可选参数，而普通数据成员则不可以。举例如下： 
class base{ 
public : 
static int _staticVar; 
int _var; 
void foo1(int i=_staticVar);//正确,_staticVar为静态数据成员 
void foo2(int i=_var);//错误,_var为普通数据成员 
}; 

4.静态数据成员的类型可以是所属类的类型，而普通数据成员则不可以。普通数据成员的只能声明为所属类类型的 指针或引用。举例如下： 

class base{ 
public : 
static base _object1;//正确，静态数据成员 
base _object2;//错误 
base *pObject;//正确，指针 
base &amp;mObject;//正确，引用 
}; 

5.这个特性，我不知道是属于标准c++中的特性，还是vc6自己的特性。 
静态数据成员的值在const成员函数中可以被合法的改变。举例如下： 

class base{ 
public: 
base(){_i=0;_val=0;} 

mutable int _i; 
static int _staticVal; 
int _val; 
void test() const{//const 成员函数 

_i++;//正确，mutable数据成员 
_staticVal++;//正确，static数据成员 
_val++;//错误 
} 
}; 
int base::_staticVal=0; 
二．静态成员函数：

1.静态成员函数的地址可用普通函数指针储存，而普通成员函数地址需要用 类成员函数指针来储存。举例如下： 
class base{ 
static int func1(); 
int func2(); 
}; 

int (*pf1)()=&amp;base::func1;//普通的函数指针 
int (base::*pf2)()=&amp;base::func2;//成员函数指针 

2.静态成员函数不可以调用类的非静态成员。因为静态成员函数不含this指针。 
3.静态成员函数不可以同时声明为 virtual、const、volatile函数。举例如下： 
class base{ 
virtual static void func1();//错误 
static void func2() const;//错误 
static void func3() volatile;//错误 
}; 
          静态函数不包含有编译器提供的隐藏的this指针，它在类没有实例化的时候就存在，所以可以直接用 （类名：：函数）  来调用，并且由于没有this指针，所以也就没有特定的成员变量供它用，因为类没有实例化前，这些类成员变量不存在，系统也没有分配空间给这些变量，且没有this指针，也无法调用这些成员变量，所以他只能使用那些类没有实例化前就已经存在的静态变量。最后要说的一点是，静态成员是可以独立访问的，也就是说，无须创建任何对象实例就可以访问。
         普通成员函数即非静态函数因为new时传递了一个默认的this指针，所以意味着每一个对象都有一组自己的成员变量，这就意味着他可以使用这些成员变量，同时也可以使用静态成员变量，因为这些变量在对象还没new出来之前就已经存在。
         普通成员函数要通过对象调用，所以要求首先建立一个对象；而静态成员函数可不建立对象就可以被使用。因此，与类的非静态数据成员无关的成员函数，虽然可以被定义为非静态函数，但是如果定义为静态函数的话在使用上会更加方便。另外，如果类的成员函数想作为回调函数来使用，一般情况下只能将它定义为静态成员才行。
 备注：类在实例化的时候，是通过new关键字来进行的，new时会默认提供一个隐藏的this指针，该指针的作用只要是用来访问实例对象的成员变量的。
 

          C++虚函数浅析：
          C++中的虚函数的作用主要是实现了多态的机制。关于多态，简而言之就是用父类型的指针指向其子类的实例，然后通过父类的指针调用实际子类的成员函数。这种技术可以让父类的指针有“多种形态”，这是一种泛型技术。所谓泛型技术，说白了就是试图使用不变的代码来实现可变的算法。比如：模板技术，RTTI技术，虚函数技术，要么是试图做到在编译时绑定，要么试图做到运行时绑定。
虚函数表
         对C++ 了解的人都应该知道虚函数（Virtual Function）是通过一张虚函数表（Virtual Table）来实现的。简称为V-Table。在这个表中，主是要一个类的虚函数的地址表，这张表解决了继承、覆盖的问题，保证其容真实反应实际的函数。这样，在有虚函数的类的实例中这个表被分配在了这个实例的内存中，所以，当我们用父类的指针来操作一个子类的时候，这张虚函数表就显得由为重要了，它就像一个地图一样，指明了实际所应该调用的函数。
         这里我们着重看一下这张虚函数表。C++的编译器应该是保证虚函数表的指针存在于对象实例中最前面的位置（这是为了保证取到虚函数表的有最高的性能——如果有多层继承或是多重继承的情况下）。 这意味着我们通过对象实例的地址得到这张虚函数表，然后就可以遍历其中函数指针，并调用相应的函数。
现在举个例子：
假设我们有这样的一个类：
 class Base{
     public:
            virtual void f(){ cout &lt;&lt; "Base::f" &lt;&lt; endl; }
            virtual void g(){ cout &lt;&lt; "Base::g" &lt;&lt; endl; }
            virtual void h(){ cout &lt;&lt; "Base::h" &lt;&lt; endl; }
 };
 
按照上面的说法，我们可以通过Base的实例来得到虚函数表。 下面是实际例程：
          typedef void(*Fun)(void);
             Baseb;
             Fun pFun = NULL;
            cout&lt;&lt; "虚函数表地址：" &lt;&lt; (int*)(&amp;b) &lt;&lt; endl;
            cout&lt;&lt; "虚函数表 — 第一个函数地址：" &lt;&lt; (int*)*(int*)(&amp;b)&lt;&lt; endl;
            //Invoke the first virtual function 
            pFun= (Fun)*((int*)*(int*)(&amp;b));
            pFun();
      实际运行经果如下：(WindowsXP+VS2003,  Linux 2.6.22 + GCC 4.1.3)
虚函数表地址：0012FED4
虚函数表 — 第一个函数地址：0044F148
Base::f
      通过这个示例，我们可以看到，我们可以通过强行把&amp;b转成int *，取得虚函数表的地址，然后，再次取址就可以得到第一个虚函数的地址了，也就是Base::f()，这在上面的程序中得到了验证（把int*强制转成了函数指针）。通过这个示例，我们就可以知道如果要调用Base::g()和Base::h()，其代码如下：
            (Fun)*((int*)*(int*)(&amp;b)+0);  // Base::f()
            (Fun)*((int*)*(int*)(&amp;b)+1);  //Base::g()
            (Fun)*((int*)*(int*)(&amp;b)+2);  //Base::h()
如下图所示：



注意：在上面这个图中，我在虚函数表的最后多加了一个结点，这是虚函数表的结束结点，就像字符串的结束符“/0”一样，其标志了虚函数表的结束。这个结束标志的值在不同的编译器下是不同的。在WinXP+VS2003下，这个值是NULL。而在Ubuntu 7.10 +Linux 2.6.22 + GCC 4.1.3下，这个值是如果1，表示还有下一个虚函数表，如果值是0，表示是最后一个虚函数表。
      下面，我将分别说明“无覆盖”和“有覆盖”时的虚函数表的样子。没有覆盖父类的虚函数是毫无意义的。我之所以要讲述没有覆盖的情况，主要目的是为了给一个对比。在比较之下，我们可以更加清楚地知道其内部的具体实现。
 一般继承（无虚函数覆盖）：
     下面，再让我们来看看继承时的虚函数表是什么样的。假设有如下图所示的一个继承关系：
  

 请注意，在这个继承关系中，子类没有重载任何父类的函数。那么，在派生类的实例中，其虚函数表如下所示：
 
对于实例：Derive d; 的虚函数表如下：
 我们可以看到下面几点：
1）虚函数按照其声明顺序放于表中。
2）父类的虚函数在子类的虚函数前面。
 
我相信聪明的你一定可以参考前面的那个程序，来编写一段程序来验证。
 一般继承（有虚函数覆盖）：
 覆盖父类的虚函数是很显然的事情，不然，虚函数就变得毫无意义。下面，我们来看一下，如果子类中有虚函数重载了父类的虚函数，会是一个什么样子？假设，我们有下面这样的一个继承关系。
 
 

为了让大家看到被继承过后的效果，在这个类的设计中，我只覆盖了父类的一个函数：f()。那么，对于派生类的实例，其虚函数表会是下面的一个样子：
 
 
     我们从表中可以看到下面几点，
1）覆盖的f()函数被放到了虚表中原来父类虚函数的位置。
2）没有被覆盖的函数依旧。
这样，我们就可以看到对于下面这样的程序，
             Base *b = new Derive();
            b-&gt;f();
     由b所指的内存中的虚函数表的f()的位置已经被Derive::f()函数地址所取代，于是在实际调用发生时，是Derive::f()被调用了。这就实现了多态。
多重继承（无虚函数覆盖）：
下面，再让我们来看看多重继承中的情况，假设有下面这样一个类的继承关系。注意：子类并没有覆盖父类的函数。
  


 对于子类实例中的虚函数表，是下图这个样子：
 
我们可以看到：
1）  每个父类都有自己的虚表。
2）  子类的成员函数被放到了第一个父类的表中。（所谓的第一个父类是按照声明顺序来判断的）
这样做就是为了解决不同的父类类型的指针指向同一个子类实例，而能够调用到实际的函数。
多重继承（有虚函数覆盖）：
 下面我们再来看看，如果发生虚函数覆盖的情况。
 下图中，我们在子类中覆盖了父类的f()函数。
  
下面是对于子类实例中的虚函数表的图：
  
 我们可以看见，三个父类虚函数表中的f()的位置被替换成了子类的函数指针。这样，我们就可以任一静态类型的父类来指向子类，并调用子类的f()了。如：
            Derive d;
            Base1*b1 = &amp;d;
            Base2*b2 = &amp;d;
            Base3*b3 = &amp;d;
            b1-&gt;f(); //Derive::f()
            b2-&gt;f(); //Derive::f()
            b3-&gt;f(); //Derive::f()
            b1-&gt;g(); //Base1::g()
            b2-&gt;g(); //Base2::g()
            b3-&gt;g(); //Base3::g()
 安全性：
    一、通过父类型的指针访问子类自己的虚函数
我们知道，子类没有重载父类的虚函数是一件毫无意义的事情。因为多态也是要基于函数重载的。虽然在上面的图中我们可以看到Base1的虚表中有Derive的虚函数，但我们根本不可能使用下面的语句来调用子类的自有虚函数：
          Base1*b1 = new Derive();
            b1-&gt;f1();  //编译出错
 任何妄图使用父类指针想调用子类中的未覆盖父类的成员函数的行为都会被编译器视为非法，所以，这样的程序根本无法编译通过。但在运行时，我们可以通过指针的方式访问虚函数表来达到违反C++语义的行为。（关于这方面的尝试，通过阅读后面附录的代码，相信你可以做到这一点）
   二、访问non-public的虚函数
另外，如果父类的虚函数是private或是protected的，但这些非public的虚函数同样会存在于虚函数表中，所以，我们同样可以使用访问虚函数表的方式来访问这些non-public的虚函数，这是很容易做到的。
 如：
 class Base {
    private:
            virtual void f(){ cout &lt;&lt; "Base::f" &lt;&lt; endl; }
 
};
 
class Derive: public Base{
 
};
 
typedef void(*Fun)(void);
 
void main() {
    Derived;
    Fun  pFun= (Fun)*((int*)*(int*)(&amp;d)+0);
    pFun();
}
  
附录一：VC中查看虚函数表
 我们可以在VC的IDE环境中的Debug状态下展开类的实例就可以看到虚函数表了（并不是很完整的）
 附录 二：例程
下面是一个关于多重继承的虚函数表访问的例程：
 

#include 
using namespace std;
class Base1 {
public:
            virtual void f(){ cout &lt;&lt; "Base1::f" &lt;&lt; endl; }
            virtual void g(){ cout &lt;&lt; "Base1::g" &lt;&lt; endl; }
            virtual void h(){ cout &lt;&lt; "Base1::h" &lt;&lt; endl; }
 
};
 
class Base2 {
public:
            virtual void f(){ cout &lt;&lt; "Base2::f" &lt;&lt; endl; }
            virtual void g(){ cout &lt;&lt; "Base2::g" &lt;&lt; endl; }
            virtual void h(){ cout &lt;&lt; "Base2::h" &lt;&lt; endl; }
};
 
class Base3 {
public:
            virtual void f(){ cout &lt;&lt; "Base3::f" &lt;&lt; endl; }
            virtual void g(){ cout &lt;&lt; "Base3::g" &lt;&lt; endl; }
            virtual void h(){ cout &lt;&lt; "Base3::h" &lt;&lt; endl; }
};
  
class Derive: public Base1, public Base2, public Base3 {
public:
            virtual void f(){ cout &lt;&lt; "Derive::f" &lt;&lt; endl; }
            virtual void g1(){ cout &lt;&lt; "Derive::g1" &lt;&lt; endl; }
};
  
typedef void(*Fun)(void);
 
int main()
{
            FunpFun = NULL;
 
            Derived;
            int**pVtab = (int**)&amp;d;
 
            //Base1'svtable
            //pFun= (Fun)*((int*)*(int*)((int*)&amp;d+0)+0);
            pFun= (Fun)pVtab[0][0];
            pFun();
 
            //pFun= (Fun)*((int*)*(int*)((int*)&amp;d+0)+1);
            pFun= (Fun)pVtab[0][1];
            pFun();
 
            //pFun= (Fun)*((int*)*(int*)((int*)&amp;d+0)+2);
            pFun= (Fun)pVtab[0][2];
            pFun();
 
            //Derive'svtable
            //pFun= (Fun)*((int*)*(int*)((int*)&amp;d+0)+3);
            pFun= (Fun)pVtab[0][3];
            pFun();
 
            //Thetail of the vtable
            pFun= (Fun)pVtab[0][4];
            cout&lt;&lt;pFun&lt;&lt;endl;
  
            //Base2'svtable
            //pFun= (Fun)*((int*)*(int*)((int*)&amp;d+1)+0);
            pFun= (Fun)pVtab[1][0];
            pFun();
 
            //pFun= (Fun)*((int*)*(int*)((int*)&amp;d+1)+1);
            pFun= (Fun)pVtab[1][1];
            pFun();
 
            pFun= (Fun)pVtab[1][2];
            pFun();
 
            //Thetail of the vtable
            pFun= (Fun)pVtab[1][3];
            cout&lt;&lt;pFun&lt;&lt;endl;
  
            //Base3'svtable
            //pFun= (Fun)*((int*)*(int*)((int*)&amp;d+1)+0);
            pFun= (Fun)pVtab[2][0];
            pFun();
 
            //pFun= (Fun)*((int*)*(int*)((int*)&amp;d+1)+1);
            pFun= (Fun)pVtab[2][1];
            pFun();
 
            pFun= (Fun)pVtab[2][2];
            pFun();
 
            //Thetail of the vtable
            pFun= (Fun)pVtab[2][3];
            cout&lt;&lt;pFun&lt;&lt;endl;
 
            return 0;
}
 
 

apache_hadoop源码，下载：

http://archive.apache.org/dist/
Hadoop 工具下载：
http://hadoop.apache.org/

Hadoop大数据最新最全资料下载地址：

http://download.csdn.net/album/detail/3047
Hadoop大数据最新经典资料下载地址：

http://download.csdn.net/detail/fanyun_01/9476856
欢迎IT爱好者加入我们的交流群：62775887

MySQL触发器语法详解:
         触发器 trigger是一种特殊的存储过程，他在插入（inset）、删除（delete）或修改（update）特定表中的数据时触发执行，它比数据本身标准的功能更精细和更复杂的数据控制能力。触发器不是由程序调用，而是由某个事件来触发的。在有数据修改时自动强制执行其业务规则，经常用于加强数据的完整性约束和业务规则等。触发器可以查询其他表，而且包含复制的sql语句。触发器也可用于强制引用完整性。触发器可以强制比用check约束定义的约束更为复杂的约束。
(一).CREATE TRIGGER语法
        CREATE TRIGGER trigger_nametrigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt;
        触发程序是与表有关的命名数据库对象，当表上出现特定事件时，将激活该对象。
        触发程序与命名为tbl_name的表相关。tbl_name必须引用永久性表。不能将触发程序与TEMPORARY表或视图关联起来。
        trigger_time是触发程序的动作时间。它可以是BEFORE或AFTER，以指明触发程序是在激活它的语句之前或之后触发。
        trigger_event指明了激活触发程序的语句的类型。trigger_event可以是下述值之一：
        (1).INSERT：将新行插入表时激活触发程序，例如，通过INSERT、LOAD DATA和REPLACE
语句。
       (2).UPDATE：更改某一行时激活触发程序，例如，通过UPDATE语句。
       (3).DELETE：从表中删除某一行时激活触发程序，例如，通过DELETE和REPLACE语句。
请注意，trigger_event与以表操作方式激活触发程序的SQL语句并不很类似，这点很重要。例如，关于INSERT的BEFORE触发程序不仅能被INSERT语句激活，也能被LOAD DATA语句激活。可能会造成混淆的例子之一是INSERT INTO .. ON DUPLICATE UPDATE ...语法：BEFORE INSERT触发程序对于每一行将激活，后跟AFTER INSERT触发程序，或BEFORE UPDATE和AFTER  UPDATE触发程序，具体情况取决于行上是否有重复键。
      对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。例如，对于某一表，不能有两个BEFORE UPDATE触发程序。但可以有1个BEFORE UPDATE触发程序和1个BEFORE  INSERT触发程序，或1个BEFOREUPDATE触发程序和1个AFTER UPDATE触发程序。trigger_stmt是当触发程序激活时执行的语句。如果你打算执行多个语句，可使用BEGIN ... END复合语句结构。这样，就能使用存储子程序中允许的相同语句
(二).DROP TRIGGER语法
     DROP TRIGGER[schema_name.]trigger_name舍弃触发程序。方案名称（schema_name）是可选的。如果省略了schema（方案），将从当前方案中舍弃触发程序。
     注释：从MySQL 5.0.10之前的MySQL版本升级到5.0.10或更高版本时（包括所有的MySQL5.1版本），必须在升级之前舍弃所有的触发程序，并在随后重新创建它们，否则，在升级之后DROP TRIGGER不工作。DROP TRIGGER语句需要SUPER权限。
(三).使用触发程序
      在本节中，介绍了在MySQL 5.1中使用触发程序的方法，并介绍了在使用触发程序方面的限制。
      触发程序是与表有关的命名数据库对象，当表上出现特定事件时，将激活该对象。在某些触发程序的用法中，可用于检查插入到表中的值，或对更新涉及的值进行计算。 
      触发程序与表相关，当对表执行INSERT、DELETE或UPDATE语句时，将激活触发程序。可以将触发程序设置为在执行语句之前或之后激活。例如，可以在从表中删除每一行之前，或在更新了,每一行后激活触发程序。要想创建触发程序或舍弃触发程序，可使用CREATE TRIGGER或DROP TRIGGER语句.触发程序不能调用将数据返回客户端的存储程序，也不能使用采用CALL语句的动态SQL（允许存储程序通过参数将数据返回触发程序）。
      触发程序不能使用以显式或隐式方式开始或结束事务的语句，如START TRANSACTION、
COMMIT或ROLLBACK。
     使用OLD和NEW关键字，能够访问受触发程序影响的行中的列（OLD和NEW不区分大小写）。
     在INSERT触发程序中，仅能使用NEW.col_name，没有旧行。在DELETE触发程序中，仅能使用OLD.col_name，没有新行。在UPDATE触发程序中，可以使用OLD.col_name来引用更新前的某一行的列，也能使用NEW.col_name来引用更新后的行中的列。
用OLD命名的列是只读的。你可以引用它，但不能更改它。对于用NEW命名的列，如果具有SELECT权限，可引用它。在BEFORE触发程序中，如果你具有UPDATE权限，可使用“SET NEW.col_name = value”更改它的值。这意味着，你可以使用触发程序来更改将要插入到新行中的值，或用于更新行的值。在BEFORE触发程序中，AUTO_INCREMENT列的NEW值为0，不是实际插入新记录时将自动生成的序列号。
     通过使用BEGIN ...END结构，能够定义执行多条语句的触发程序。在BEGIN块中，还能使用存储子程序中允许的其他语法，如条件和循环等。但是，正如存储子程序那样，定义执行多条语句的触发程序时，如果使用mysql程序来输入触发程序，需要重新定义语句分隔符，以便能够在触发程序定义中使用字符“;”。在下面的示例中，演示了这些要点。在该示例中，定义了1个UPDATE触发程序，用于检查更新每一行时将使用的新值，并更改值，使之位于0～100的范围内。它必须是BEFORE触发程序，这是因为，需要在将值用于更新行之前对其进行检查：
mysql&gt; delimiter //
mysql&gt; CREATE TRIGGER upd_check BEFORE UPDATE ON account
      -&gt; FOR EACH ROW
      -&gt; BEGIN
      -&gt; IF NEW.amount &lt; 0 THEN
      -&gt; SET NEW.amount = 0;
      -&gt; ELSEIF NEW.amount &gt; 100 THEN
      -&gt;  SET NEW.amount = 100;
      -&gt;  END IF;
      -&gt; END;//
mysql&gt; delimiter ;
      较为简单的方法是，单独定义存储程序，然后使用简单的CALL语句从触发程序调用存储程序。如果你打算从数个触发程序内部调用相同的子程序，该方法也很有帮助。在触发程序的执行过程中，MySQL处理错误的方式如下：
     (1)如果BEFORE触发程序失败，不执行相应行上的操作。
     (2)仅当BEFORE触发程序（如果有的话）和行操作均已成功执行，才执行AFTER触发程序。
     (3) 如果在BEFORE或AFTER触发程序的执行过程中出现错误，将导致调用触发程序的整个语句的失败。
     (4)对于事务性表，如果触发程序失败（以及由此导致的整个语句的失败），该语句所执行的所有更改将回滚。对于非事务性表，不能执行这类回滚，因而，即使语句失败，失败之前所作的任何更改依然有效。
例一：
mysql&gt; CREATE TABLE account (acct_num INT, amount DECIMAL(10,2));
mysql&gt; CREATE TRIGGER ins_sum BEFORE INSERT ON account  FOR EACH ROW SET @sum = @sum + NEW.amount;

 1.删除0字节文件

代码如下:
find -type f -size 0 -exec rm -rf {} \;
2.查看进程
按内存从大到小排列
代码如下:
ps -e -o "%C : %p : %z : %a"|sort-k5 -nr
3.按cpu利用率从大到小排列
代码如下:
ps -e -o "%C : %p : %z : %a"|sort-nr
4.打印说cache里的URL
代码如下:
grep -r -a jpg /data/cache/* | strings |grep "http:" | awk -F'http:' '{print "http:"$2;}'
5.查看http的并发请求数及其TCP连接状态：
代码如下:
netstat -n | awk '/^tcp/ {++S[$NF]} END{for(a in S) print a, S[a]}'
6. 在sed -i'/Root/s/no/yes/' /etc/ssh/sshd_config   sed在sshd_config这个文件里Root的一行，匹配Root一行，将no替换成yes.
# sed -i '/Root/s/no/yes/'/etc/ssh/sshd_config 
7.如何杀掉mysql进程：
代码如下:
ps aux |grep mysql |grep -v grep |awk'{print $2}' |xargs kill -9 (从中了解到awk的用途)
killall -TERM mysqld
kill -9 `cat /usr/local/apache2/logs/httpd.pid` 试试查杀进程PID
8.显示运行3级别开启的服务:
代码如下:
ls /etc/rc3.d/S* |cut -c 15- (从中了解到cut的用途，截取数据)
9.如何在编写SHELL显示多个信息，用EOF代码如下:
cat &lt;&lt; EOF
+--------------------------------------------------------------+
| === Welcome to Tunoff services === |
+--------------------------------------------------------------+
EOF
10.for 的巧用(如给mysql建软链接)：
代码如下:
cd /usr/local/mysql/bin
for i in *
do ln /usr/local/mysql/bin/$i /usr/bin/$i
done
11. 取IP地址：
代码如下:
ifconfig eth0 |grep "inet addr:"|awk '{print $2}'|cut -c 6- 
或者
ifconfig | grep 'inet addr:'| grep -v '127.0.0.1' | cut -d: -f2 | awk '{ print$1}'
12.内存的大小:
代码如下:
free -m |grep "Mem" | awk '{print$2}'
13.查看连接某服务端口最多的的IP地址：
代码如下:
netstat -an -t | grep ":80" |grep ESTABLISHED | awk '{printf "%s %s\n",$5,$6}' | sort
14.查看Apache的并发请求数及其TCP连接状态：
代码如下:
netstat -n | awk '/^tcp/ {++S[$NF]} END{for(a in S) print a, S[a]}'
15.因为同事要统计一下服务器下面所有的jpg的文件的大小,写了个shell给他来统计.原来用xargs实现,但他一次处理一部分,搞的有多个总和....,下面的命令就能解决：
代码如下:
find / -name *.jpg -exec wc -c {} \;|awk'{print $1}'|awk '{a+=$1}END{print a}'

CPU的数量（多核算多个CPU，cat /proc/cpuinfo |grep -c processor）越多，系统负载越低，每秒能处理的请求数也越多。
16.CPU负载   ：
# cat /proc/loadavg
检查前三个输出值是否超过了系统逻辑CPU的4倍。  
18.CPU负载  ：
 #mpstat 1 1
检查%idle是否过低(比如小于5%)
19.内存空间 ：
 #free
检查free值是否过低   也可以用 # cat /proc/meminfo
20.swap空间 ：  
# free
检查swap used值是否过高   如果swap used值过高，进一步检查swap动作是否频繁：
# vmstat 1 5
观察si和so值是否较大
21.磁盘空间  ： 
# df -h
检查是否有分区使用率(Use%)过高(比如超过90%)   如发现某个分区空间接近用尽，可以进入该分区的挂载点，用以下命令找出占用空间最多的文件或目录：
代码如下:
# du -cks * | sort -rn | head -n 10
22.磁盘I/O负载  # iostat -x 1 2：
检查I/O使用率(%util)是否超过100%
rrqm/s队列中每秒钟合并的读请求数量 
wrqm/s队列中每秒钟合并的写请求数量 
r/s每秒钟完成的读请求数量 
w/s每秒钟完成的写请求数量 
rsec/s每秒钟读取的扇区数量 
wsec/s每秒钟写入的扇区数量 
avgrq-sz平均请求扇区的大小 
avgqu-sz平均请求队列的长度 
await平均每次请求的等待时间 
svctm平均每次请求的服务时间 
util设备的利用率
23.网络负载：  
 #sar -n DEV
检查网络流量(rxbyt/s, txbyt/s)是否过高
24.网络错误 ：  
# netstat -i
检查是否有网络错误(drop fifo colls carrier)   也可以用命令：# cat/proc/net/dev
25.网络连接数目 ：
# netstat -an | grep -E “^(tcp)” | cut -c68- | sort | uniq -c | sort -n
26.进程总数  ：
 # psaux | wc -l
检查进程个数是否正常 (比如超过250)
27.可运行进程数目 ： 
# vmwtat 1 5
列给出的是可运行进程的数目，检查其是否超过系统逻辑CPU的4倍
28.进程  ： 
# top -id 1
观察是否有异常进程出现
29.网络状态   检查DNS, 网关等是否可以正常连通：
30.用户：   
# who | wc -l
检查登录用户是否过多 (比如超过50个)   也可以用命令：# uptime
31.系统日志 ： 
 #cat /var/log/rflogview/*errors
检查是否有异常错误记录   也可以搜寻一些异常关键字，例如：
代码如下:
# grep -i error /var/log/messages
# grep -i fail /var/log/messages
32.核心日志  ：
 # dmesg
检查是否有异常错误记录
33.系统时间：   
# date
检查系统时间是否正确
34.打开文件数目：   
# lsof | wc -l
检查打开文件总数是否过多
35.日志：   
# logwatch –print   配置/etc/log.d/logwatch.conf，将 Mailto 设置为自己的email 地址，启动mail服务(sendmail或者postfix)，这样就可以每天收到日志报告了。
缺省logwatch只报告昨天的日志，可以用# logwatch –print –range all 获得所有的日志分析结果。
可以用# logwatch –print –detail high 获得更具体的日志分析结果(而不仅仅是出错日志)。
36.杀掉80端口相关的进程：
代码如下:
lsof -i :80|grep -v "PID"|awk'{print "kill -9",$2}'|sh
37.清除僵死进程：
代码如下:
ps -eal | awk '{ if ($2 == "Z"){print $4}}' | kill -9
38.tcpdump抓包 ，用来防止80端口被人攻击时可以分析数据：
代码如下:
# tcpdump -c 10000 -i eth0 -n dst port 80&gt; /root/pkts
39.然后检查IP的重复数 并从小到大排序 注意 "-t\ +0"   中间是两个空格：
代码如下:
# less pkts | awk {'printf$3"\n"'} | cut -d. -f 1-4 | sort | uniq -c | awk {'printf $1""$2"\n"'} | sort -n -t\ +0
40.查看有多少个活动的php-cgi进程：
代码如下:
netstat -anp | grep php-cgi | grep ^tcp |wc -l
chkconfig --list | awk '{if ($5=="3:on") print $1}'
41.kudzu查看网卡型号：
代码如下:
kudzu --probe --class=network
42.常用正则表达式：
匹配中文字符的正则表达式： [\u4e00-\u9fa5]
评注：匹配中文还真是个头疼的事，有了这个表达式就好办了
匹配双字节字符(包括汉字在内)：[^\x00-\xff]
评注：可以用来计算字符串的长度（一个双字节字符长度计2，ASCII字符计1）
匹配空白行的正则表达式：\n\s*\r
评注：可以用来删除空白行
匹配HTML标记的正则表达式：&lt;(\S*?)[^&gt;]*&gt;.*?&lt;/\1&gt;|&lt;.*? /&gt;
评注：网上流传的版本太糟糕，上面这个也仅仅能匹配部分，对于复杂的嵌套标记依旧无能为力
匹配首尾空白字符的正则表达式：^\s*|\s*$
评注：可以用来删除行首行尾的空白字符(包括空格、制表符、换页符等等)，非常有用的表达式
匹配Email地址的正则表达式：\w+([-+.]\w+)*@\w+([-.]\w+)*\.\w+([-.]\w+)*
评注：表单验证时很实用
匹配网址URL的正则表达式：[a-zA-z]+://[^\s]*
评注：网上流传的版本功能很有限，上面这个基本可以满足需求
匹配帐号是否合法(字母开头，允许5-16字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]{4,15}$
评注：表单验证时很实用
匹配国内电话号码：\d{3}-\d{8}|\d{4}-\d{7}
评注：匹配形式如 0511-4405222 或 021-87888822
匹配腾讯QQ号：[1-9][0-9]{4,}
评注：腾讯QQ号从10000开始
匹配中国邮政编码：[1-9]\d{5}(?!\d)
评注：中国邮政编码为6位数字
匹配身份证：\d{15}|\d{18}
评注：中国的身份证为15位或18位
匹配ip地址：\d+\.\d+\.\d+\.\d+
评注：提取ip地址时有用
匹配特定数字：
代码如下:
^[1-9]\d*$　 　 //匹配正整数
^-[1-9]\d*$ 　 //匹配负整数
^-?[1-9]\d*$　　 //匹配整数
^[1-9]\d*|0$　 //匹配非负整数（正整数 + 0）
^-[1-9]\d*|0$　　 //匹配非正整数（负整数 + 0）
^[1-9]\d*\.\d*|0\.\d*[1-9]\d*$　　 //匹配正浮点数
^-([1-9]\d*\.\d*|0\.\d*[1-9]\d*)$　 //匹配负浮点数
^-?([1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0)$　 //匹配浮点数
^[1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0$　　 //匹配非负浮点数（正浮点数+ 0）
^(-([1-9]\d*\.\d*|0\.\d*[1-9]\d*))|0?\.0+|0$　　//匹配非正浮点数（负浮点数+ 0）
评注：处理大量数据时有用，具体应用时注意修正
匹配特定字符串：
代码如下:
^[A-Za-z]+$　　//匹配由26个英文字母组成的字符串
^[A-Z]+$　　//匹配由26个英文字母的大写组成的字符串
^[a-z]+$　　//匹配由26个英文字母的小写组成的字符串
^[A-Za-z0-9]+$　　//匹配由数字和26个英文字母组成的字符串
^\w+$　　//匹配由数字、26个英文字母或者下划线组成的字符串
43.     有文件file:
a)   查询file里面空行的所在行号
awk‘{if($0~/^$/)printNR}’ file
or
grep -n ^$ file |awk ‘BEGIN{FS=”:”}{print $1}’
b)   查询file以abc结尾的行
grep abc$ file
c)   打印出file文件第1到第3行
sed -n ’1,3p’ file
head -3 file
44.     如何将本地80端口的请求转发到8080端口，当前主机IP为192.168.2.1:
-APREROUTING -d 124.42.60.109 -p tcp -m tcp –dport 80 -j DNAT–to-destination 10.0.0.18:9000
45.     crontab:
在11月份内，每天的早上6点到12点中，每隔2小时执行一次/usr/bin/httpd.sh怎么实现
0 6-12/2 * 11*/usr/bin/httpd.sh
46.     编写个shell脚本将/usr/local/test 目录下大于100K的文件转移到/tmp目录下:
#!/bin/bash
for file in `ls /root`
do
       if [ -f $file ]; then
             if[ `ls -l $file|awk '{print $5}'` -gt 10000 ];  then
                    mv$file /tmp/
             fi
       fi
done
47.     read 命令5秒后自动退出 :
[root@localhost bin]# read -t 5 
48.     自动ftp上传 :
#!/bin/sh 
ftp -n&lt;&lt;END_FTP 
open 192.168.1.4 
user codfei duibuqi //用户名codfei 密码duibuqi 
binary 
prompt off //关闭提示 
mput test //上传test 
close 
bye 
END_FTP 
自动ssh登陆从A到B然后再到c 
#!/usr/bin/expect -f 
set timeout 30 
spawn ssh codfei@B 
expect "password:" 
send "pppppp/r" 
expect "]*" 
send "ssh codfei@C/r" 
expect "password:" 
send "pppppp/r" 
interact 
#打印第一个域 
[root@localhost bin]# cat 3 
eqeqedadasdD 
eqeqdadfdfDD 
fdsfdsfQWEDD 
DSADASDSADSA 
[root@localhost bin]# 
[root@localhost bin]# 
[root@localhost bin]# awk -F "" '{print $1}' 3 
e 
e 
f 
D 
实现字符串翻转 
[root@localhost bin]# cat 8 
qweqewqedadaddas 
[root@localhost bin]# rev 8 
saddadadeqweqewq 
49.利用iptables对应简单攻击
 
netstat -an | grep -v LISTEN | awk ?{print$5}‘ |grep -v 127.0.0.1|grep -v 本机ip|sed 
―s/::ffff://g‖|awk ?BEGIN { FS=‖:‖ } { Num[$1]++ } END { for(i in Num) if(Num&gt;8)
{ print i} }’ |grep‘[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}’|  xargs -i[]
iptables -I INPUT -s [] -j DROP 
Num&gt;8
部分设定值为阀值这条句子会自动将
netstat -an 中查到的来自同一的超过一定量的连接的列入禁止范围。本机
ip
改成你的服务器的
ip
地址
选择性的删除某些行
# 删除所有空白行 类似于 "grep '.'"
awk NF
awk '/./'
# 删除重复连续的行 模拟 "uniq"
awk 'a !~ $0; {a=$0}'
# 删除重复的、非连续的行
awk '! a[$0]++'                     # 最简练
awk '!($0 in a) {a[$0];print}'      # 最有效
查询系统状态的指令集
cat 文件名        一屏查看文件内容
more 文件名        分页查看文件内容
less 文件名        可控分页查看文件内容
grep -l -r 字符串 路径    显示包含字符串的文件名
grep -L -r 字符串 路径    显示不包含字符串的文件名
lsof -p           进程号例如lsof  -p 2428查看进程打开的文件
lsof abc.txt       显示开启文件abc.txt的进程
lsof -i :22        显示22端口现在运行什么程序
lsof -c nsd        显示nsd进程现在打开的文件
nohup 程序 &amp;    在后台运行程序退出登录后并不结束程序
strace -f -F -o outfile &lt;cmd&gt;    详细显示程序的运行信息
arping IP地址        根据IP查网卡地址
nmblookup -A IP地址    根据IP查电脑名
linux删除特殊文件名的文件
    假设Linux系统中有一个文件名叫―-ee‖如果我们想对它进行操作例如要删除
它按照一般的删除方法在命令行中输入rm -ee命令界面会提示我们是―无效选
项‖(invalid option)原来由于文件名的第一个字符为―-‖Linux把文件名当作选项
了我们可以使用―–‖符号来解决这个问题输入―rm — -ee‖命令便可顺利删除名为
―-ee‖的文件。如果是其他特殊字符的话可以在特殊字符前加一个―‖符号或者用双
引号把整个文件名括起来。
    如/usr/lcoal/目录下有个--exclude文件通过命令rm -- --exclude
删除此文件
一句话快速查找PHP木马的方法
find ./ -name "*.php" -type f-print0|xargs -0 egrep 
"(phpspy|c99sh|milw0rm|eval\(base64_decode|eval\(gzinflate\(base64_decode|eval\(gzinf
late\(str_rot13\(base64_decode|spider_bc)"|awk-F: '{print $1}'|sort|uniq
如何删去重复行并保持顺序不变
awk '{ if (!seen[$0]++) { print $0; } }'$file_path
perl -lne 'print unless $seen{$_}++ '$file_path
50. 查看某文件的一部分
如果你只想看文件的前5 行可以使用head命令
如head -5 /etc/passwd
如果你想查看文件的后10行可以使用tail命令
如tail -10 /etc/passwd
查看文件中间一段可以使用sed命令
如:sed –n '5,10p' /etc/passwd 
这样你就可以只查看文件的第5行到第10行
51.将file.txt 里的123改为456方法 1 
sed 's/123/456/g' file.txt &gt;file.txt.new 修改的保存到其它文件 
sed -i 's/123/456/g' file.txt 直接修改原文件  
方法2 vi file.txt  输入命令 :%s/123/456/g
注意如果替换的文件有特殊符号如/就要用来取消。
例sed -i 's//usr/local/apache2/htdocs//var/www/html/g' 
/usr/local/apache2/conf/httpd.conf 如果只是下原有的行后添加就用&amp; 例
sed -i 's/DirectoryIndex index.htmlindex.html.var/&amp; index.htm index.php /g' /usr/local/apache2/conf/httpd.conf
 


Linux目录和Windows目录有着很大的不同，Linux目录类似一个树，最顶层是其根目录，如下图：
                                                  

                                                     



/bin 二进制可执行命令
/dev 设备特殊文件
/etc 系统管理和配置文件
/etc/rc.d 启动的配置文件和脚本
/home 用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示
/lib 标准程序设计库，又叫动态链接共享库，作用类似windows里的.dll文件
/sbin 超级管理命令，这里存放的是系统管理员使用的管理程序
/tmp 公共的临时文件存储点
/root 系统管理员的主目录
/mnt 系统提供这个目录是让用户临时挂载其他的文件系统
/lost+found这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里
/proc 虚拟的目录，是系统内存的映射。可直接访问这个目录来获取系统信息。
/var 某些大文件的溢出区，比方说各种服务的日志文件
/usr 最庞大的目录，要用到的应用程序和文件几乎都在这个目录，其中包含：
/usr/x11R6 存放x window的目录
/usr/bin 众多的应用程序
/usr/sbin 超级用户的一些管理程序
/usr/doc linux文档
/usr/include linux下开发和编译应用程序所需要的头文件
/usr/lib 常用的动态链接库和软件包的配置文件
/usr/man 帮助文档
/usr/src 源代码，linux内核的源代码就放在/usr/src/linux里
/usr/local/bin 本地增加的命令
/usr/local/lib 本地增加的库根文件系统
    一般：ls查看：蓝色：表示目录 青色：表示链接
 黑色：表示文件
    通常情况下，根文件系统所占空间一般应该比较小，因为其中的绝大部分文件都不需要经常改动，而且包括严格的文件和一个小的不经常改变的文件系统不容易损坏。
除了可能的一个叫/vmlinuz标准的系统引导映像之外，根目录一般不含任何文件。所有其他文件在根文件系统的子目录中。
1./bin目录
/ b i n目录包含了引导启动所需的命令或普通用户可能用的命令(可能在引导启动后)。这些命令都是二进制文件的可执行程序(
 b i n是b i n a r y - -二进制的简称)，多是系统中重要的系统文件。
2./sbin目录
/ s b i n目录类似/bin，也用于存储二进制文件。因为其中的大部分文件多是系统管理员使用的基本的系统程序，所以虽然普通用户必要且允许时可以使用，但一般不给普通用户使用。
3./etc目录：
/ e t c目录存放着各种系统配置文件，其中包括了用户信息文件/ e t c / p a s s w d，系统初始化文件/
 e t c / r c等。l i n u x正是*这些文件才得以正常地运行。
/etc文件系统
（1）          /etc 目录包含各种系统配置文件，下面说明其中的一些。其他的你应该知道它们属于哪个程序，并阅读该程序的m a n页。许多网络配置文件也在/etc 中。
1. /etc/rc或/etc/rc.d或/etc/rc?.d
启动、或改变运行级时运行的脚本或脚本的目录。
2. /etc/passwd
用户数据库，其中的域给出了用户名、真实姓名、用户起始目录、加密口令和用户的其
他信息。
3. /etc/fdprm
软盘参数表，用以说明不同的软盘格式。可用setfdprm 进行设置。更多的信息见s e t f d p r m
的帮助页。
4. /etc/fstab
指定启动时需要自动安装的文件系统列表。也包括用swapon -a启用的s w a p区的信息。
5. /etc/group
类似/etc/passwd ，但说明的不是用户信息而是组的信息。包括组的各种数据。
6. /etc/inittab
init 的配置文件。
7. /etc/issue
包括用户在登录提示符前的输出信息。通常包括系统的一段短说明或欢迎信息。具体内容由系统管理员确定。
8. /etc/magic
“f i l e”的配置文件。包含不同文件格式的说明，“f i l e”基于它猜测文件类型。
9. /etc/motd
m o t d是message of the day的缩写，用户成功登录后自动输出。内容由系统管理员确定。常用于通告信息，如计划关机时间的警告等。
10. /etc/mtab
当前安装的文件系统列表。由脚本( s c r i t p )初始化，并由mount 命令自动更新。当需要一个当前安装的文件系统的列表时使用(例如df 命令)。
11. /etc/shadow
在安装了影子( s h a d o w )口令软件的系统上的影子口令文件。影子口令文件将/ e t c / p a s s wd文件中的加密口令移动到/ e t c / s h a d o w中，而后者只对超级用户( r o o t)可读。这使破译口令更困难，以此增加系统的安全性。
12. /etc/login.defs
l o g i n命令的配置文件。
13. /etc/printcap
类似/etc/termcap ，但针对打印机。语法不同。
14. /etc/profile 、/ e t c / c s h . l o g i n、/etc/csh.cshrc
登录或启动时b o u r n e或c shells执行的文件。这允许系统管理员为所有用户建立全局缺省环境。
15. /etc/securetty
确认安全终端，即哪个终端允许超级用户( r o o t )登录。一般只列出虚拟控制台，这样就不可能(至少很困难)通过调制解调器( m o d e m )或网络闯入系统并得到超级用户特权。
16. /etc/shells
列出可以使用的s h e l l。chsh 命令允许用户在本文件指定范围内改变登录的s h e l l。提供一台机器f t p服务的服务进程ftpd 检查用户s h e l l是否列在/etc/shells 文件中，如果不是，将不允许该用户登录。
17. /etc/termcap
终端性能数据库。说明不同的终端用什么“转义序列”控制。写程序时不直接输出转义序列(这样只能工作于特定品牌的终端)，而是从/etc/termcap 中查找要做的工作的正确序列。
这样，多数的程序可以在多数终端上运行。
4. /root目录
/root 目录是超级用户的目录。
5./lib目录
/ l i b目录是根文件系统上的程序所需的共享库，存放了根文件系统程序运行所需的共享文件。这些文件包含了可被许多程序共享的代码，以避免每个程序都包含有相同的子程序的副本，故可以使得可执行文件变得更小，节省空间。
6./lib/modules 目录
/lib/modules 目录包含系统核心可加载各种模块，尤其是那些在恢复损坏的系统时重新引导系统所需的模块(例如网络和文件系统驱动)。
7./dev目录
/ d e v目录存放了设备文件，即设备驱动程序，用户通过这些文件访问外部设备。比如，用户可以通过访问/ d e v / m o u s e来访问鼠标的输入，就像访问其他文件一样。
8./tmp目录
/tmp 目录存放程序在运行时产生的信息和数据。但在引导启动后，运行的程序最好使用/ v a r / t m p来代替/tmp，因为前者可能拥有一个更大的磁盘空间。
9./boot目录
/ b o o t目录存放引导加载器(bootstrap loader)使用的文件，如l
 i lo，核心映像也经常放在这里，而不是放在根目录中。但是如果有许多核心映像，这个目录就可能变得很大，这时使用单独的文件系统会更好一些。还有一点要注意的是，要确保核心映像必须在i d e硬盘的前1
 0 2 4柱面内。
10./mnt目录
/ m n t目录是系统管理员临时安装( m o u n t )文件系统的安装点。程序并不自动支持安装到/mnt。/mnt下面可以分为许多子目录，例如/mnt/dosa可能是使用m
 s d o s文件系统的软驱，而/mnt/exta可能是使用e x t 2文件系统的软驱，/mnt/cdrom光驱等等。
11./proc, /usr,/var,/home目录
其他文件系统的安装点。
/dev文件系统
/dev 目录包括所有设备的设备文件。设备文件用特定的约定命名，这在设备列表中说明。
设备文件在安装时由系统产生，以后可以用/dev/makedev 描述。/ d e v / m a k e d e v.local是
系统管理员为本地设备文件(或连接)写的描述文稿(即如一些非标准设备驱动不是标准
makedev 的一部分)。下面简要介绍/ d e v下一些常用文件。
1. /dev/console
系统控制台，也就是直接和系统连接的监视器。
2. /dev/hd
i d e硬盘驱动程序接口。如： / d e v / h d a指的是第一个硬盘， h a d 1则是指/ d e v / h da的第一个
分区。如系统中有其他的硬盘，则依次为/ d e v / h d b、/ d e v / h d c、. . . . ..；如有多个分区则依次为
h d a 1、h d a 2 . . . . . .
3. /dev/sd
s c s i磁盘驱动程序接口。如有系统有s c s i硬盘，就不会访问/ d e v / h a d，而会访问/ d e v / sd a。
4. /dev/fd
软驱设备驱动程序。如： / d e v / f d 0指系统的第一个软盘，也就是通常所说的a：盘，
/ d e v / f d 1指第二个软盘，. . . . . .而/ d e v / f d 1 h 1 4 40则表示访问驱动器1中的4 . 5高密盘。
5. /dev/st
s c s i磁带驱动器驱动程序。
6. /dev/tty
提供虚拟控制台支持。如： / d e v / t t y 1指的是系统的第一个虚拟控制台， / d e v / t t y2则是系统
的第二个虚拟控制台。
7. /dev/pty
提供远程登陆伪终端支持。在进行te l n e t登录时就要用到/ d e v / p t y设备。
8. /dev/ttys
计算机串行接口，对于d o s来说就是“ c o m 1”口。
9. /dev/cua
计算机串行接口，与调制解调器一起使用的设备。
10. /dev/null
“黑洞”，所有写入该设备的信息都将消失。例如：当想要将屏幕上的输出信息隐藏起来时，只要将输出信息输入到/ d e v / n u l l中即可。
/usr文件系统
/usr 是个很重要的目录，通常这一文件系统很大，因为所有程序安装在这里。/usr 里的
所有文件一般来自l i n u x发行版( d i s t r i b u t i o n)；本地安装的程序和其他东西在/usr/local下，因为这样可以在升级新版系统或新发行版时无须重新安装全部程序。/usr目录下的许多内容是可选的，但这些功能会使用户使用系统更加有效。/ u s r可容纳许多大型的软件包和它们的配置文件。下面列出一些重要的目录(一些不太重要的目录被省略了)。
1. /usr/x11r6
包含x wi n d o w系统的所有可执行程序、配置文件和支持文件。为简化x的开发和安装，x的文件没有集成到系统中。x wi n d o w系统是一个功能强大的图形环境，提供了大量的图形工具程序。用户如果对microsoft wi n d o w s或m a c h i n t o s h比较熟悉的话，就不会对x win d o w系统感到束手无策了。
2. /usr/x386
类似/ u s r / x 11r6 ，但是是专门给x 11 release 5的。
3. /usr/bin
集中了几乎所有用户命令，是系统的软件库。另有些命令在/bin 或/usr/local/bin 中。
4. /usr/sbin
包括了根文件系统不必要的系统管理命令，例如多数服务程序。
5. /usr/man、/ u s r / i n f o、/ u s r / d o c
这些目录包含所有手册页、g n u信息文档和各种其他文档文件。每个联机手册的“节”都有两个子目录。例如： / u s r / m a n / m a n 1中包含联机手册第一节的源码(没有格式化的原始文件)，/ u s r / ma n / c a t 1包含第一节已格式化的内容。l联机手册分为以下九节：内部命令、系统调用、库函数、设备、文件格式、游戏、宏软件包、系统管理和核心程序。
6. /usr/include
包含了c语言的头文件，这些文件多以. h结尾，用来描述c语言程序中用到的数据结构、子过程和常量。为了保持一致性，这实际上应该放在/usr/lib 下，但习惯上一直沿用了这个名字。
7. /usr/lib
包含了程序或子系统的不变的数据文件，包括一些s i t e - w i d e配置文件。名字l i b来源于库(library); 编程的原始库也存在/usr/lib 里。当编译程序时，程序便会和其中的库进行连接。也有许多程序把配置文件存入其中。
8. /usr/local
本地安装的软件和其他文件放在这里。这与/ u s r很相似。用户可能会在这发现一些比较大的软件包，如t e x、e m a c s等。
/var文件系统
/var 包含系统一般运行时要改变的数据。通常这些数据所在的目录的大小是要经常变化或扩充的。原来/ v a r目录中有些内容是在/ u s r中的，但为了保持/ u s r目录的相对稳定，就把那些需要经常改变的目录放到/ v a r中了。每个系统是特定的，即不通过网络与其他计算机共享。下面列出一些重要的目录(一些不太重要的目录省略了)。
1. /var/catman
包括了格式化过的帮助( m a n )页。帮助页的源文件一般存在/ u s r / m a n / m a n中；有些m an页可能有预格式化的版本，存在/ u s r / m a n / c a t中。而其他的m a n页在第一次看时都需要格式化，格式化完的版本存在/var/man 中，这样其他人再看相同的页时就无须等待格式化了。(/var/catman 经常被清除，就像清除临时目录一样。)
2. /var/lib
存放系统正常运行时要改变的文件。
3. /var/local
存放/usr/local 中安装的程序的可变数据(即系统管理员安装的程序)。注意，如果必要，即使本地安装的程序也会使用其他/var 目录，例如/var/lock 。
4. /var/lock
锁定文件。许多程序遵循在/var/lock 中产生一个锁定文件的约定，以用来支持他们正在使用某个特定的设备或文件。其他程序注意到这个锁定文件时，就不会再使用这个设备或文件。
5. /var/log
各种程序的日志( l o g )文件，尤其是login (/var/log/wtmp log纪录所有到系统的登录和注销) 和syslog(/var/log/messages 纪录存储所有核心和系统程序信息)。/var/log里的文件经常不确定地增长，应该定期清除。
6. /var/run
保存在下一次系统引导前有效的关于系统的信息文件。例如， /var/run/utmp 包含当前登录的用户的信息。
7. /var/spool
放置“假脱机( s p o o l )”程序的目录，如m a i l、n e w s、打印队列和其他队列工作的目录。每个不同的s p o o l在/var/spool 下有自己的子目录，例如，用户的邮箱就存放在/var/spool/mail中。
8. /var/tmp
比/tmp 允许更大的或需要存在较长时间的临时文件。注意系统管理员可能不允许/var/tmp 有很旧的文件。
/proc文件系统
/proc 文件系统是一个伪的文件系统，就是说它是一个实际上不存在的目录，因而这是一
个非常特殊的目录。它并不存在于某个磁盘上，而是由核心在内存中产生。这个目录用于提
供关于系统的信息。下面说明一些最重要的文件和目录(/proc 文件系统在proc man页中有更详
细的说明)。
1. /proc/x
关于进程x的信息目录，这一x是这一进程的标识号。每个进程在/proc 下有一个名为自
己进程号的目录。
2. /proc/cpuinfo
存放处理器( c p u )的信息，如c p u的类型、制造商、型号和性能等。
3. /proc/devices
当前运行的核心配置的设备驱动的列表。
4. /proc/dma
显示当前使用的d m a通道。
5. /proc/filesystems
核心配置的文件系统信息。
6. /proc/interrupts
显示被占用的中断信息和占用者的信息，以及被占用的数量。
7. /proc/ioports
当前使用的i / o端口。
8. /proc/kcore
系统物理内存映像。与物理内存大小完全一样，然而实际上没有占用这么多内存；它仅仅是在程序访问它时才被创建。(注意：除非你把它拷贝到什么地方，否则/proc 下没有任何东西占用任何磁盘空间。)
9. /proc/kmsg
核心输出的消息。也会被送到s y s l o g。
10. /proc/ksyms
核心符号表。
11. /proc/loadavg
系统“平均负载”； 3个没有意义的指示器指出系统当前的工作量。
12. /proc/meminfo
各种存储器使用信息，包括物理内存和交换分区( s w a p )。
13. /proc/modules
存放当前加载了哪些核心模块信息。
14. /proc/net
网络协议状态信息。
15. /proc/self
存放到查看/proc 的程序的进程目录的符号连接。当2个进程查看/proc 时，这将会是不同的连接。这主要便于程序得到它自己的进程目录。
16. /proc/stat
系统的不同状态，例如，系统启动后页面发生错误的次数。
17. /proc/uptime
系统启动的时间长度。
18. /proc/version
核心版本

        对于Linux系统来说，无论是中央处理器、内存、磁盘驱动器、键盘、鼠标，还是用户等都是文件，Linux系统管理的命令是它正常运行的核心。熟悉了Linux常用的文件处理命令以后，这一讲介绍对系统和用户进行管理的命令。 

df 

1.作用
     df命令用来检查文件系统的磁盘空间占用情况，使用权限是所有用户。 

2.格式
df [options] 

3.主要参数
－s：对每个Names参数只给出占用的数据块总数。
－a：递归地显示指定目录中各文件及子目录中各文件占用的数据块数。若既不指定－s，也不指定－a，则只显示Names中的每一个目录及其中的各子目录所占的磁盘块数。
－k：以1024字节为单位列出磁盘空间使用情况。
－x：跳过在不同文件系统上的目录不予统计。
－l：计算所有的文件大小，对硬链接文件则计算多次。
－i：显示inode信息而非块使用量。
－h：以容易理解的格式印出文件系统大小，例如136KB、254MB、21GB。
－P：使用POSIX输出格式。
－T：显示文件系统类型。 

4.说明
       df命令被广泛地用来生成文件系统的使用统计数据，它能显示系统中所有的文件系统的信息，包括总容量、可用的空闲空间、目前的安装点等。 

       超级权限用户使用df命令时会发现这样的情况：某个分区的容量超过了100％。这是因为Linux系统为超级用户保留了10％的空间，由其单独支配。也就是说，对于超级用户而言，他所见到的硬盘容量将是110％。这样的安排对于系统管理而言是有好处的，当硬盘被使用的容量接近100％时系统管理员还可以正常工作。 

5.应用实例
     Linux支持的文件系统非常多，包括JFS、ReiserFS、ext、ext2、ext3、ISO9660、XFS、Minx、vfat、MSDOS等。使用df -T命令查看磁盘空间时还可以得到文件系统的信息： 








＃df－T
文件系统 类型    
容量  
已用  可用 
已用% 
挂载点
/dev/hda7        reiserfs 5.2G  1.6G  3.7G  30%  /
/dev/hda1        vfat     2.4G  1.6G  827M  66%  /windows/C
/dev/hda5        vfat     3.0G  1.7G  1.3G  57%  /windows/D
/dev/hda9        vfat     3.0G  2.4G  566M  82%  /windows/E
/dev/hda10       NTFS     3.2G  573M  2.6G  18%  /windows/F
/dev/hda11       vfat     1.6G  1.5G   23M  99%  /windows/G







      从上面除了可以看到磁盘空间的容量、使用情况外，分区的文件系统类型、挂载点等信息也一览无遗。

top 

1.作用
      top命令用来显示执行中的程序进程，使用权限是所有用户。 

2.格式
top [－] [d delay] [q] [c] [S] [s] [i][n] 

3.主要参数 
d：指定更新的间隔，以秒计算。
q：没有任何延迟的更新。如果使用者有超级用户，则top命令将会以最高的优先序执行。 
c：显示进程完整的路径与名称。
S：累积模式，会将己完成或消失的子行程的CPU时间累积起来。 
s：安全模式。
i：不显示任何闲置(Idle)或无用(Zombie)的行程。 
n：显示更新的次数，完成后将会退出top。 

4.说明
      top命令是Linux系统管理的一个主要命令，通过它可以获得许多信息。这里我们结合图1来说明它给出的信息。 
图1 top命令的显示 
      在图1中，第一行表示的项目依次为当前时间、系统启动时间、当前系统登录用户数目、平均负载。第二行显示的是所有启动的进程、目前运行的、挂起(Sleeping)的和无用(Zombie)的进程。第三行显示的是目前CPU的使用情况，包括系统占用的比例、用户使用比例、闲置(Idle)比例。第四行显示物理内存的使用情况，包括总的可以使用的内存、已用内存、空闲内存、缓冲区占用的内存。第五行显示交换分区使用情况，包括总的交换分区、使用的、空闲的和用于高速缓存的大小。第六行显示的项目最多，下面列出了详细解释。
PID（Process ID）：进程标示号。
USER：进程所有者的用户名。
PR：进程的优先级别。
NI：进程的优先级别数值。
VIRT：进程占用的虚拟内存值。
RES：进程占用的物理内存值。
SHR：进程使用的共享内存值。
S：进程的状态，其中S表示休眠，R表示正在运行，Z表示僵死状态，N表示该进程优先值是负数。
%CPU：该进程占用的CPU使用率。
%MEM：该进程占用的物理内存和总内存的百分比。
TIME＋：该进程启动后占用的总的CPU时间。
Command：进程启动的启动命令名称，如果这一行显示不下，进程会有一个完整的命令行。
top命令使用过程中，还可以使用一些交互的命令来完成其它参数的功能。这些命令是通过快捷键启动的。
&lt;空格&gt;：立刻刷新。
P：根据CPU使用大小进行排序。
T：根据时间、累计时间排序。
q：退出top命令。
m：切换显示内存信息。
t：切换显示进程和CPU状态信息。
c：切换显示命令名称和完整命令行。
M：根据使用内存大小进行排序。
W：将当前设置写入~/.toprc文件中。这是写top配置文件的推荐方法。 

       可以看到，top命令是一个功能十分强大的监控系统的工具，对于系统管理员而言尤其重要。但是，它的缺点是会消耗很多系统资源。 

5.应用实例
       使用top命令可以监视指定用户，缺省情况是监视所有用户的进程。如果想查看指定用户的情况，在终端中按“U”键，然后输入用户名，系统就会切换为指定用户的进程运行界面。 

free 

1.作用
       free命令用来显示内存的使用情况，使用权限是所有用户。 

2.格式
      free [－b|－k|－m] [－o] [－s delay] [－t] [－V] 

3.主要参数
－b －k －m：分别以字节（KB、MB）为单位显示内存使用情况。
－sdelay：显示每隔多少秒数来显示一次内存使用情况。
－t：显示内存总和列。
－o：不显示缓冲区调节列。 

4.应用实例
        free命令是用来查看内存使用情况的主要命令。和top命令相比，它的优点是使用简单，并且只占用很少的系统资源。通过－S参数可以使用free命令不间断地监视有多少内存在使用，这样可以把它当作一个方便实时监控器。
＃free －b －s5 

       使用这个命令后终端会连续不断地报告内存使用情况（以字节为单位），每5秒更新一次。 
quota 

1.作用
        quota命令用来显示磁盘使用情况和限制情况，使用权限超级用户。 

2.格式
        quota [－g][－u][－v][－p] 用户名 组名 

3.参数
－g：显示用户所在组的磁盘使用限制。
－u：显示用户的磁盘使用限制。
－v：显示没有分配空间的文件系统的分配情况。
－p：显示简化信息。 

4.应用实例
         在企业应用中磁盘配额非常重要，普通用户要学会看懂自己的磁盘使用情况。要查询自己的磁盘配额可以使用下面命令（下例中用户账号是caojh)： 








＃quota caojh
Disk quotas for user caojh(uid 502): 
Filesystem blocks quota limit grace files quota limit grace 
/dev/hda3   58      200000 400000   41   500    1000





         以上显示ID号为502的caojh账号，文件个数设置为500～1000个，硬盘空间限制设置为200MB～400MB。一旦磁盘配额要用完时，就需要删除一些垃圾文件或向系统管理员请求追加配额。 
at 
1.作用
        at命令用来在指定时刻执行指定的命令序列。 
2.格式
        at [－V] [－q x] [－f file] [－m] time 
3.主要参数
－V：显示标准错误输出。
－q：许多队列输出。 
－f：从文件中读取作业。
－m：执行完作业后发送电子邮件到用户。
time：设定作业执行的时间。time格式有严格的要求，由小时、分钟、日期和时间的偏移量组成，其中日期的格式为MM.DD.YY，MM是分钟，DD是日期，YY是指年份。偏移量的格式为时间＋偏移量，单位是minutes、hours和days。 
4.应用实例
＃at －f data 15:30 +2 days 
上面命令表示让系统在两天后的17：30执行文件data中指明的作业。 
lp 
1.作用
lp是打印文件的命令，使用权限是所有用户。 
2.格式
lp [－c][－d][－m][－number][－title][-p] 
3.主要参数
－c：先拷贝文件再打印。
－d：打印队列文件。
－m：打印结束后发送电子邮件到用户。
－number：打印份数。
－title：打印标题。
－p：设定打印的优先级别，最高为100。 
4.应用实例
（1）使用lp命令打印多个文件
＃lp 23 4
request id is 11 (3 file(s)) 

其中2、3、4分别是文件名；“requestid is 11 (3 file(s)) ”表示这是第11个打印命令，依次打印这三个文件。 
（2）设定打印优先级别
＃lp lp-d LaserJet -p 90 /etc/aliases 
通过添加“-p90”，规定了打印作业的优先级为90。它将在优先级低于90的打印作业之前打印，包括没有设置优先级的作业，缺省优先级是50。 
下一篇我们将介绍html"&gt;useradd、groupadd、kill等命令。

useradd 
1.作用
useradd命令用来建立用户帐号和创建用户的起始目录，使用权限是超级用户。 

2.格式
useradd [－d home] [－s shell] [－c comment] [－m [－ktemplate]] [－f inactive] [－eexpire ] [－p passwd] [－r]name 

3.主要参数
－c：加上备注文字，备注文字保存在passwd的备注栏中。　
－d：指定用户登入时的启始目录。
－D：变更预设值。
－e：指定账号的有效期限，缺省表示永久有效。
－f：指定在密码过期后多少天即关闭该账号。
－g：指定用户所属的群组。
－G：指定用户所属的附加群组。
－m：自动建立用户的登入目录。
－M：不要自动建立用户的登入目录。
－n：取消建立以用户名称为名的群组。
－r：建立系统账号。
－s：指定用户登入后所使用的shell。
－u：指定用户ID号。 
示例：
useradd testuser   创建用户testuser
passwd testuser   给已创建的用户testuser设置密码 （说明：新创建的用户会在/home下创建一个用户目录testuser）
usermod --help   修改用户这个命令的相关参数
userdel testuser  删除用户testuser
rm -rf testuser   删除用户testuser所在目录
4.说明
useradd可用来建立用户账号，它和adduser命令是相同的。账号建好之后，再用passwd设定账号的密码。使用useradd命令所建立的账号，实际上是保存在/etc/passwd文本文件中。 

5.应用实例
建立一个新用户账户，并设置ID：
＃useradd caojh －u 544 

需要说明的是，设定ID值时尽量要大于500，以免冲突。因为Linux安装后会建立一些特殊用户，一般0到499之间的值留给bin、mail这样的系统账号。 

groupadd 

1.作用
groupadd命令用于将新组加入系统。 

2.格式
groupadd [－g gid] [－o]] [－r] [－f] groupname 

3.主要参数
－g gid：指定组ID号。
－o：允许组ID号，不必惟一。
－r：加入组ID号，低于499系统账号。
－f：加入已经有的组时，发展程序退出。 

4.应用实例
建立一个新组，并设置组ID加入系统：
＃groupadd －g 344 cjh 

此时在/etc/passwd文件中产生一个组ID（GID）是344的项目。 

kill 

1.作用
kill命令用来中止一个进程。 

2.格式
kill [ －s signal | －p ] [ －a ] pid ... 
kill －l [ signal ] 

3.参数
－s：指定发送的信号。
－p：模拟发送信号。
－l：指定信号的名称列表。
pid：要中止进程的ID号。
Signal：表示信号。 

4.说明
进程是Linux系统中一个非常重要的概念。Linux是一个多任务的操作系统，系统上经常同时运行着多个进程。我们不关心这些进程究竟是如何分配的，或者是内核如何管理分配时间片的，所关心的是如何去控制这些进程，让它们能够很好地为用户服务。 

Linux操作系统包括三种不同类型的进程，每种进程都有自己的特点和属性。交互进程是由一个Shell启动的进程。交互进程既可以在前台运行，也可以在后台运行。批处理进程和终端没有联系，是一个进程序列。监控进程（也称系统守护进程）时Linux系统启动时启动的进程，并在后台运行。例如，httpd是著名的Apache服务器的监控进程。 

kill命令的工作原理是，向Linux系统的内核发送一个系统操作信号和某个程序的进程标识号，然后系统内核就可以对进程标识号指定的进程进行操作。比如在top命令中，我们看到系统运行许多进程，有时就需要使用kill中止某些进程来提高系统资源。在讲解安装和登陆命令时，曾提到系统多个虚拟控制台的作用是当一个程序出错造成系统死锁时，可以切换到其它虚拟控制台工作关闭这个程序。此时使用的命令就是kill，因为kill是大多数Shell内部命令可以直接调用的。 

5.应用实例
（1）强行中止（经常使用杀掉）一个进程标识号为324的进程：
＃kill －9 324 

（2）解除Linux系统的死锁
在Linux中有时会发生这样一种情况：一个程序崩溃，并且处于死锁的状态。此时一般不用重新启动计算机，只需要中止(或者说是关闭)这个有问题的程序即可。当kill处于X-Window界面时，主要的程序(除了崩溃的程序之外)一般都已经正常启动了。此时打开一个终端，在那里中止有问题的程序。比如，如果Mozilla浏览器程序出现了锁死的情况，可以使用kill命令来中止所有包含有Mozolla浏览器的程序。首先用top命令查处该程序的PID，然后使用kill命令停止这个程序：
＃kill －SIGKILL XXX
其中，XXX是包含有Mozolla浏览器的程序的进程标识号。 

（3）使用命令回收内存
我们知道内存对于系统是非常重要的，回收内存可以提高系统资源。kill命令可以及时地中止一些“越轨”的程序或很长时间没有相应的程序。例如，使用top命令发现一个无用 (Zombie) 的进程，此时可以使用下面命令：
＃kill －9 XXX
其中，XXX是无用的进程标识号。 

然后使用下面命令：
＃free 
此时会发现可用内存容量增加了。 

（4）killall命令
Linux下还提供了一个killall命令，可以直接使用进程的名字而不是进程标识号，例如：
＃ killall -HUP inetd 

crontab 

1.作用
使用crontab命令可以修改crontab配置文件，然后该配置由cron公用程序在适当的时间执行，该命令使用权限是所有用户。 

2.格式
crontab [ －u user ] 文件 
crontab [ －u user ] { －l | －r | －e } 

3.主要参数
－e：执行文字编辑器来设定时程表，内定的文字编辑器是vi。 
－r：删除目前的时程表。
－l：列出目前的时程表。 

crontab文件的格式为“M H D m d cmd”。其中，M代表分钟（0～59），H代表小时（0～23），D代表天（1～31），m代表月（1～12），d代表一星期内的天（0～6，0为星期天）。cmd表示要运行的程序，它被送入sh执行，这个Shell只有USER、HOME、SHELL三个环境变量。 

4.说明
和at命令相比，crontab命令适合完成固定周期的任务。 

5.应用实例
设置一个定时、定期的系统提示：
[cao @www cao]#crontab －e
此时系统会打开一个vi编辑器。 

如果输入以下内容：35 17 * * 5 wall "Tomorrow is Saturday Iwill go CS"，然后存盘退出。这时在/var/spool/cron/目录下会生产一个cao的文件，内容如下： 








# DO NOT EDIT THIS FILE － edit the master and reinstall.
# (/tmp/crontab.2707 installed on Thu Jan  1 22:01:51 2004)
# (Cron version －－ $Id: crontab.c,v 2.13 1994/01/17 03:20:37 vixie Exp $)
35 17 * * 5 wall "Tomorrow is Saturday I will play CS "







这样每个星期五17：35系统就会弹出一个终端，提醒星期六可以打打CS了！显示结果见图3所示。 


一个定时、定期的系统提示 


1.联合使用kill和top命令观察系统性能的变化 

首先启动一个终端运行top命令，然后再启动一个终端使用kill命令，见图4所示。 


图4 观察kill命令对top终端的影响 

这时利用上面介绍的kill命令来中止一些程序： 
＃killSIGKILL XXX 

然后再看top命令终端的变化，包括内存容量、CPU使用率、系统负载等。注意，有些进程是不能中止的，不过学习Linux命令时可以试试，看看系统有什么反应。 

2.使用at和halt命令定时关机
首先设定关机时间是17:35，输入下面代码： 








＃at 17:35 
warning: commands will be executed using (in order) a) $SHELL b) login shell c) /bin/sh
at&gt;halt `-i －p
at&gt; &lt;EOT&gt;
job 6 at 2004－01－01 17:35







此时实际上就已经进入Linux系统的Shell，并且编写一个最简单程序：halt －i －p。上面Shell中的文本结束符号表示按“Ctrl＋D”组合键关闭命令，提交任务退出Shell。“Job 6 at 2004－01－01 17:35”表示系统接受第6个at命令，在“2004－01－01 17:35”时执行命令：先把所有网络相关的装置停止，关闭系统后关闭电源。 

3.用crontab命令实现每天定时的病毒扫描
前面已经介绍了一个简单的crontab命令操作，这里看一些更重要的操作。 

（1）建立一个文件，文件名称自己设定，假设为caoproject：
＃crontab －e 

（2）文件内容如下：
05 09 * * * antivir
用vi编辑后存盘退出。antivir是一个查杀Linux病毒的软件，当然需要时先安装在系统中。 

（3）使用crontab命令添加到任务列表中：
＃crontab caoproject
这样系统内所有用户在每天的9点05分会自动进行病毒扫描。 

4.用kill使修改的配置文件马上生效
Windows用户一般都知道，重要配置文件修改后往往都要重新启动计算机才能使修改生效。而Linux由于采用了模块化设计，可以自己根据需要实时设定服务。这里以网络服务inetd为例介绍一些操作技巧。 

inetd是一个监听守护进程，监听与提供互联网服务进程（如rlogin、telnet、ftp、rsh）进行连接的要求，并扩展所需的服务进程。默认情况下，inetd监听的这些daemon均列于/etc /inetd.conf文件中。编辑/etc/inetd.conf文件，可以改变inetd启动服务器守护进程的选项，然后驱使inetd以SIGHUP（signal 1）向当前的inet

一.Linux虚拟机常用命令
# virsh list                     //查看已打开虚拟机列表
# virsh list --all                //查看所有虚拟机列表
# virsh version                //查看virsh版本号
# virsh start node1            //启动node1虚拟机
# virsh shutdown node1      //关机node1虚拟机
# virsh destroy node1         //强制关机node1虚拟机
# virsh dumpxml node1 &gt; node1.xml //导出node1虚拟机配置文件
# virsh undefine node1       //取消node1定义
# virsh define node1.xml     //重新定义node1
# virsh autostart node1       //设置开机自启动node1
# virt-clone -o node1 -n node1-clone-f  /data/images/node1-clone.img //克隆虚拟机
使用命令安装新的虚拟机：可根据需要调整选项
virt-install \
--name node1 \
--noautoconsole \
--ram 512 \
--arch=x86_64 \
--vcpus=1 \
--os-type=linux \
--os-variant=rhel6 \
--hvm \
--accelerate \
--disk path=/data/images/node1.img \
--network bridge=br0 \
--locationnfs:192.168.100.1:/var/ftp/pub/iso/RedHat/6.4 \
--extra-args="ks=http://192.168.100.1/rhel-ks.cfg ip=192.168.100.10 netmask=255.255.255.0 gateway=192.168.100.254 dns=192.168.100.2 noipv6"
二.使用LVM方式管理虚拟主机磁盘
1.创建LV
# fdisk -l | grep /dev/sda6                 //创建分区
/dev/sda6           6170      39163  265015296  8e  Linux LVM
PV  --&gt; VG --&gt; LV
# pvcreate /dev/sda6                       //创建PV
# vgcreate vg_data /dev/sda6               //创建VG
# lvcreate -L 10G -n lv_kvm_node1vg_data      //创建LV
 2.使用创建的LV安装Guest
# virt-install \
--name kvm_node1 \
--noautoconsole \
--ram 1024 \
--arch=x86_64 \
--vcpus=1 \
--os-type=linux \
--os-variant=rhel6 \
--hvm \
--accelerate \
--disk path=/dev/vg_data/lv_kvm_node1\              //安装在刚创建的LV中
--network bridge=br0 \
--locationnfs:192.168.100.1:/var/ftp/pub/iso/RedHat/6.4 \
--extra-args="ks=http://192.168.100.1/rhel-ks.cfg ip=192.168.100.10 netmask=255.255.255.0 gateway=192.168.100.254 dns=192.168.100.2 noipv6"
 3.设置模板虚拟机，去掉一些个性信息（在刚装好的虚拟机kvm_node1上操作）
# touch  /.unconfigured
 4.对已安装好lv_kvm_node1的生成快照(快照大小只要为被快照的逻辑卷的15~20%就可以了)
# lvcreate -s -n kvm_snap1 -L 2G /dev/vg_data/lv_kvm_node1
 5.将快照定义到virt-manager
# vim /etc/libvirt/qemu/kvm_node1.xml       //默认配置文件位置
# virsh dumpxml kvm_node1 &gt; /root/kvm_snap1.xml  //也可导出配置文件
修改配置文件kvm_snap1.xml中名字,UUID,磁盘位置，mac地址
# virsh define /root/kvm_snap1.xml 这样就能使用快照的虚拟机做实验，当这个快照虚拟坏了再快照一个就能恢复到刚安装好的状态


Linux网关配置：


一、网络拓扑



二、配置网络

1．A,GW1,GW2,B的网络配置

A:

eth0:

IPADDR :192.168.1.2

NETMASK:255.255.255.0

GATEWAY:192.168.1.1

GW1:

eth0:

IPADDR :172.16.113.173

NETMASK:255.255.255.0

GATEWAY:172.16.113.9

eth1:

IPADDR :192.168.1.1

NETMASK:255.255.255.0

B:

eth0:

IPADDR :192.168.0.2

NETMASK:255.255.255.0

GATEWAY:192.168.0.1

GW2:

eth0:

IPADDR :172.16.113.163

NETMASK:255.255.255.0

GATEWAY:172.16.113.9

eth1:

IPADDR :192.168.0.1

NETMASK:255.255.255.0

2．GW1设置

(1)开启路由转发功能

# echo 1 &gt; /proc/sys/net/ipv4/ip_forward

或

#vim /etc/sysctl.conf

net.ipv4.ip_forward = 0 --&gt; net.ipv4.ip_forward = 1

(2)添加到192.168.0.0/24网段的路由

# route add -net 192.168.0.0 netmask 255.255.255.0 gw 172.16.113.163

3. GW2设置

(1)开启路由转发功能

# echo 1 &gt; /proc/sys/net/ipv4/ip_forward

或

#vim /etc/sysctl.conf

net.ipv4.ip_forward = 0 --&gt; net.ipv4.ip_forward = 1

(2)添加到192.168.1.0/24网段的路由

# route add -net 192.168.1.0 netmask 255.255.255.0 gw 172.16.113.173

三、测试网关

若A，B能相互ping通，则两个网关也配置成功了.


    
   原文名称：7 tools to fire up Spark's big data engine  

      Spark正在数据处理领域卷起一场风暴。让我们通过本篇文章，看看为Spark的大数据平台起到推波助澜的几个重要工具。



Spark生态系统众生相

       Apache Spark不仅仅让大数据处理起来更快，还让大数据处理起来更简单、功能更强大、更方便。Spark并非只是一项技术，它结合了诸多部分，新的功能和性能改进不断添加进来，每个部分都在不断完善之中。

      本文介绍了Spark生态系统的每个主要部分：每个部分的功能，为什么很重要，是如何发展的，在哪方面不尽如人意，以及可能会往哪个方向发展。

Spark Core



      Spark的核心是恰如其名的Spark Core。除了协调和调度作业外，Spark Core还为Spark中的数据处理提供了基本的抽象机制，名为弹性分布式数据集(RDD)。

      RDD对数据执行两个动作：转换和操作。前者转换数据，并将它们作为刚创新的RDD来提供;后者根据现有的RDD(比如对象数量)来计算结果。

      Spark的速度很快，原因是转换和操作都保存在内存中。操作慢腾腾地评估，这意味着只有需要相关的数据时，才执行操作;然而，很难搞清楚什么在缓慢运行。

      Spark的速度在不断提高。Java的内存管理往往给Spark带来问题，于是Project Tungsten计划避开JVM的内存和垃圾收集子系统，以此提高内存效率。

Spark API



       Spark主要是用Scala编写的，所以Spark的主要API长期以来也支持Scala。不过另外三种使用广泛得多的语言同样得到支持：Java(Spark也依赖它)、Python和R.

       总的来说，你最好选择自己最擅长的那种语言，因为你需要的功能特性很可能在该语言中直接得到支持。只有一个例外：相比之下，SparkR中对机器学习的支持不大给力，目前只有一小批算法可供使用。不过将来这种情况势必会发生变化。

Spark SQL



      千万不要低估了能够对批量数据执行SQL查询的能力或便利。Spark SQL提供了对Spark提供的数据执行SQL查询(并且请求列式DataFrame)的一种通用机制，包括通过ODBC/JDBC连接件进行管道处理的查询。你甚至不需要正规的数据源。Spark 1.6中添加了这一功能：支持以一种得到支持的格式查询扁平文件，就像Apache Drill那样。

       Spark SQL其实并不用于更新数据，因为那与Spark的整个意义相悖。可以将因而生成的数据写回成新的Spark数据源(比如新的Parquet表)，但是UPDATE查询并不得到支持。别指望诸如此类的功能特性很快就会推出;着眼于Spark SQL的改进大多数用于提升其性能，因为它也成了Spark Streaming的基础。

Spark Streaming



       Spark的设计让它得以支持许多处理方法，包括流处理――Spark Streaming因此得名。关于Spark Steaming的传统观点是，它还半生不熟，这意味着只有你不需要瞬间延迟，或者如果你还没有投入到另一种流数据处理解决方案(比如说Apache Storm)，你才会使用它。

      但是Storm在逐渐失去人气;长期使用Storm的推特此后已改用了自己的项目Heron。此外，Spark 2.0承诺会推出一种新的“结构化数据流”模式，以便对实时数据进行交互式Spark SQL查询，包括使用Spark?的机器学习库。至于其性能是否高得足以击败竞争对手仍需拭目以待，不过它值得认真考虑。

MLlib(机器学习)



      机器学习技术素有既神奇，又困难之称。Spark让你可以对Spark中的数据运行许多常见的机器学习算法，从而使这些类型的分析容易得多，也更容易被Spark用户所使用。

      MLlib中的可用算法数量众多，该框架每推出一个修订版，就会随之增多。话虽如此，一些类型的算法还是没有――比如说，涉及深度学习的任何算法。第三方正在利用Spark的人气来填补这一空白;比如说，雅虎可以借助CaffeOnSpark执行深度学习，它通过Spark充分利用了Caffe深度学习系统。

GraphX??(图形计算)



      描绘数百万实体之间的关系通常需要图形，这种数据构件描述了那些实体之间的相互关系。Spark的GraphX?? API让你可以使用Spark的一套方法，对数据执行图形操作，于是构建和转换这类图形的繁重任务卸载到了Spark。GraphX??还包括用于处理数据的几种常见算法，比如PageRank或标签传播(label propagation)。

      从目前来看，GraphX的??一个主要限制是，它最适合静态图形。处理添加了新顶点的图形会严重影响性能。此外，如果你已经在使用一种成熟的图形数据库解决方案，GraphX还??不太可能取代它。

SparkR(Spark上的R)



       R语言为进行统计数值分析和机器学习工作提供了一种环境。Spark在2015年6月添加了支持R的功能，以匹配其支持Python和Scala的功能。

       除了为潜在的Spark开发人员多提供一种语言外，SparkR还让R程序员们可以做之前做不了的许多事情，比如访问超过单一机器的内存容量的数据集，或者同时轻松地使用多个进程或在多个机器上运行分析。

      SparkR还让R程序员可以充分利用Spark中的MLlib机器学习模块，创建一般的线性模型。遗憾的是，并非所有的MLlib功能在SparkR中得到支持，不过Spark每推出一个后续的修订版，都在填补R支持方面的差距。

      作者：布加迪编译来源：51CTO.com



船在大海上航行，需要灯塔的指引。目标就是项目中灯塔。在项目中，目标不但可以指引方向，还可以凝聚人心。

1.把员工团结在目标下面

不善于给工作制定目标的管理者不是优秀的管理者，没有目标的团队也不能称之为团队。一个合适的目标，可以将员工紧紧的凝聚在一起，产生强大的力量。因此，项目经理必须要学会利用这一点，让员工为目标干活，将员工团结在目标下面。

（1）目标是领导力的来源，也是团队的基本特征

无论是对个人，还是对组织，目标的重要性都不言而喻。为了“实现共产主义”这一伟大目标，无数革命先烈抛头颅、洒热血，献出了自己宝贵的生命，由此可见目标的具大作用。

对一个管理者而言，做每一件事，都要先制定目标，不然你就是领导力再强，工作也不会有成效。事实上，制定合适的工作目标是管理者领导力的重要表现。李开复先生曾经提出，管理者的领导力来源于九大因素，其中首要的就是要明确愿景，制定工作目标。

目标也是一个团队的基本特征。管理学中对团队的一般定义是：“团队是由员工和管理层组成的一个共同体，它合理利用每一个成员的知识和技能协同工作，解决问题，达到共同的目标。团队的构成要素总结为5P，分别为目标、人、定位、权限、计划。”由此可见，要成为一个团队，首先是要有一个共同的目标，否则就不能算做团队，顶多只能叫做“一群人”。

项目也是这样，没有明确的目标，项目经理的领导力也就成了无源之水，项目组也会成为一群“乌合之众”。

（2）领导员工要拉不要推

桌子上放着一根绳子，怎样才能让绳子向前移动呢？你如果向前推，绳子就会拧成一团，它就不是原来的绳子了，最好的办法是把绳子向前面拉。

一个人工作的动力，同样可以分为推力和拉力。项目经理批评、督促员工，这是一种推力，它是一种外在的力；激励员工、给员工制定目标，这是一种拉力，拉力则是一种自我驱动的内在力量。其实人就像绳子一样，是柔软的，这种柔软来自于人内在的思想和感情。对于这种柔软的东西，“拉”显然是一种更加明智的做法。

项目中如果员工的主要动力是推力的话，其工作也就没有什么积极性，项目经理也会觉得自己非常辛苦，却没有成效。因此，项目经理要想轻松一点的话，最后的办法就是改变思路，不要在后面推员工，而是要在前面给员工施加拉力。

拉力主要有两种，一是来自于员工的自我激励，二是靠管理者的激励。为员工制定合适的工作目标实际上是引发员工的自我激励的一种方法，如果员工连要达到什么目标都不知道，那他又拿什么来激励自己呢？

（3）让员工为目标干活

在一个具有超强个人魅力的项目经理的团队中，员工干活的动力很可能是来自项目经理的吸引力，也就是为项目经理而干活。如果换一个项目经理的话，员工的干劲就可能会一落千丈。这样的项目经理就好比是一块磁铁，员工就好比是被磁铁吸引的螺丝钉，现在把磁铁拿走了，螺丝钉们自然就会像一盘散沙。

阿里巴巴公司的创始人马云曾经说过：“不要让你的同事为你干活，而让我们的同事为我们的目标干活，共同努力，团结在一个共同的目标下面，要比团结在你一个企业家底下容易得多。所以首先要说服大家认同共同的理想，而不是让大家来为你干活。”

马云也是这样做的。虽然他本人具有非凡的个人魅力，但他更加重视目标的作用。从阿里巴巴创业之初，马云就定下了做中国最大的电子商务公司的目标，碰到千难万险也从不放弃。阿里巴巴能有今天的成就，可以说跟当初的目标是密不可分的。

一个企业需要目标，一个项目同样需要目标。事实上，项目比企业更加具有目标性，一个项目组也正是因为要完成某一目标而成立的，在招标文件或者合同中往往也会明确写着这些目标。项目经理要做的，就是认真评估这些目标，根据实际情况进行必要的调整和细化，让目标代替项目经理，成为将员工吸引在一起的磁铁，将项目组紧紧团结在一起。

2.项目目标的制定

对公司领导而言，项目目标就是对项目经理考核的指标；对项目经理而言，项目目标是愿景、是动力、是手段、更是项目团队的信用，所以项目经理必须谨慎制定项目目标。

制定一个合适的项目目标并不是一件轻松、愉快的事情， 需要经过多方的沟通、博弈，并获得相关方的认可。项目经理要做到有理有据、不卑不亢，力争发挥其主导作用，而不能一味做老好人，委曲求全，否则可能会让整个团队受到伤害。

（1）制定项目目标的三种方式

相对于公司或者部门，一个项目的目标是比较容易确定的，这是由项目的天然特性决定的。为什么要有项目？是为了做一件事情，做好这件事情就是项目的目标。具体的来说，项目有三大目标，即成本、时间和质量。其中，质量和时间目标是来自于客户的要求，一般在招标文件或合同中有明确的规定，而成本目标则来自于是公司，是领导要求最多花多少钱来做好这件事。一般来说，成本目标是三大目标最难以确定的，也往往是项目经理与公司之间最容易产生争议的地方。

项目目标一般在项目章程或项目任务书或项目立项书中进行确定。项目目标的制定有自上而下、自下而上以及双方共同制定三种方式，这几种方式也体现了公司之间文化氛围的差异。

l自上而下

即项目目标由上司制定，直接下发给项目组。领导在制定目标时，总是希望越高越好，他们相信有压力才会有动力，员工才会拼命的往前赶。他们常见的做法是质量和进度要求直接与合同保持一致，而成本限制往往是合同金额按一定比例计算出来的。这一类型的工作公司往往管理层过于强势和精明，氛围比较紧张，员工的压力比较交大。

l自下而上

由项目组制定后，交给公司领导层进行审批。这种公司的管理层一般比较温和，公司气氛也比较缓和。

l双方共同制定

一般由公司或项目组先制定一份初始的目标，然后根据项目的实际情况，公司和项目组一起进行分析讨论确定。这种公司的氛围一般比较民主化，能够具体问题具体分析，尊重员工的意见。

当然还有的一些运作比较粗放的公司，根本就不会去制定明确的项目目标，也没有项目章程或项目任务书之类的文件。一般直接按照合同要求开展工作，有问题时双方进行口头沟通。

（2）制定项目目标应注意的两大问题

根据管理学中的目标管理理论，一项目标应该符合SMART原则。其中S（Specific）是指要具体明确，尽可能量化为具体数据；M（Measurable）是指可测量的，要把目标转化为指标，指标可以按照一定标准进行评价；A（Attainable）是指可达成的，要根据组织的资源、人员技能和管理流程配备程度来设计目标，保证目标是可以达成的；R（Relevant）是指相关的，即各项目标之间有关联，相互支持，符合实际；T（Time-based）是指目标要有完成时间期限，便于监控评价。

这五项要求看似简单，但要做到其实并不容易。结合软件公司中经常存在的弊病，我觉得有两点必须加以特别的重视，第一是要重申SMART中的A，即目标必须是现实可行的；第二要补充一点，目标必须获取所有人相关人员的认同。

l目标必须是现实可行的

所谓现实可行，也就是说目标是综合了范围、成本、时间、质量、人力资源各方因素权衡的结果，是可以实现的，而不是拍脑瓜的结果。自上而下的方式中最容易出现的误区，就是领导直接规定目标，还美其名曰“不看过程，只看结果”。可以说这种方式非常简单粗暴，只管公司利润，不管员工死活。要知道一个不现实的目标，不但不能像磁铁一样吸引员工为之努力工作，反而可能会大大打击员工的积极性。

因此在制定目标的过程中，项目经理必须争取自己的话语权，认真分析目标的可行性，充分考虑项目中可能存在的问题和风险，并就双方差异主动与领导进行沟通，力求达成一致。试想，假如等到项目开展到后半段，项目经理才发现原来的目标有很多不切实际的地方，比如工作量与想象的有太大出入，质量要求过于苛刻，费用预算完全是拍脑袋，如果这个时候项目经理才提出目标不科学，将责任推给公司领导，只会让领导更加觉得项目经理无能、没有担当。

有些人担心，这样“讨价还价”领导会不会嫌我烦？确实有个别领导不喜欢员工提出异议，觉得他在讲条件、不听指挥，但大部分领导还是乐于与项目经理讨论这些问题的，因为这说明项目经理对项目有全面系统的思考，重要细节都已经考虑过了，这样做事领导会更加放心。公司最怕的不是挑剔的项目经理，而是没有想法的项目经理。

对于质量和进度目标，由于涉及到客户方的直接利益，如果经过评估确实需要变更，还必须与客户和监理等方面进行沟通，确保各方的认识一致。

l目标必须获得所有人认同

一个目标可以要想将员工团结起来，把大家凝聚在一起，还必须获得项目团队的认同。

项目中经常出现这样的情况，项目经理和上级领导确定目标后，项目经理将目标通知给大家，也不理会员工的意见。等到目标眼看无法完成的时候，项目经理对员工 “兴师问罪”，员工会理直气壮的说，我当就说过这个目标不可能实现的。这是一个项目目标没有获得员工认同的典型例子，项目经理等于复制了公司对项目“拍脑袋”的决策模式，是一种不负责任的做法。要想得大家的认同，最好的办法就是让员工直接参与评估和制定项目目标。

（3）团队目标与个人目标

在一个组织中目标可以分为团队目标和个人目标两个层次。团队目标确定以后，项目经理需要根据具体的工作任务、团队成员的构成情况，对目标进行分解，最后形成每个员工的个人目标，从而实现个人目标与团队目标的统一。



图 项目目标的层次性

项目经理要关注团队目标，同样也需要关注个人目标。团队目标会影响整个团队的士气，个人目标同样会影响个人的动力。因此项目经理必须也要认真考虑如何与员工一起，对员工进行准确定位，制定恰当的目标，安排合适的工作，以保证员工发挥最佳状态，保持最强的战斗力。

3.弗洛姆“期望理论”的启发

斯大林所曾说过：“伟大的目标产生伟大的动力。”那么一个具体目标对员工究生能产生多大的动力呢？

著名心理学家弗洛姆曾提出一种激励员工的理论，认为“某一活动对于调动某一人的积极性，激发出人的内部潜力的激励的强度，取决于达成目标后对于满足个人的需要的价值的大小与他根据以往的经验进行判断能导致该结果的概率”，用公式来表示就是：

目标的激励力=目标的价值×能实现目标的概率

根据这一理论，我们可以得到以下几点启发：

（1）目标要适当

从公式中我们可以看到，目标的激励力与目标的价值是成正比的，因此在给员工安排工作任务时，可以尽量将目标设定得适当的高一点，让其更具有挑战性。

美国行为学家吉格勒认为：“设定一个高目标就等于达到了目标的一部分。”由此可见，给员工设置一个具有挑战性的目标是非常有必要的。适当的提高目标，可以激发员工的工作热情，增强其抗压能力，而且当工作目标完成时，会更加具有成就感。

然而必须注意的是，上述公式中的两大自变量，即目标的价值、能实现目标的概率两者这间本身也存在一定的函数关系。当目标较低时，实现概率为1，也就是说，员工信心十足，认为完全可以实现；当目标逐渐升高，超过某个点时，实现的概率又逐降低。两者的变化关系如下图所示：



图 预期实现概率与目标价值之间的关系

相应的，目标的激励力与目标价值之间的关系也可以用下图来表示：



图 目标的激励力与目标价值之间的关系

因此目标不能定得过高，否则“预期的实现概率”将会迅速减小，到达某一临界点后，目标对员工的激励力也会随之降低。

图中有A、B两个点，其中A点是能够完全实的最大目标值，但项目经理给员工安排工作时的最佳点还不在这里，因为此时“目标的激励力”这一变量，要到B点才能达到最大值，这也就是为什么管理专家们建议安排工作时要让员工“跳一跳能够得着”的原因。

（2）改变个人对概率的预期

上面的图只是简单的数学模拟，实际上预期实现概率是一个主观的因素，它与目标的难度固然存在较强的关联，但其本身又受其它诸多因素的影响。项目经理利用这一点，可以通过对员工的引导，提升他对实现概率的预期，从而提高其工作的动力。

要提升员工对实现概率预期，实际上就是提升员工对工作的信心。要实现这一点，项目经理可以做很多工作，例如：

l把客户的肯定、领导的表扬及时传递给员工；

l使员工对目标保持谨慎乐观。如果过于乐观，就会觉得太容易，放松工作要求；

l让工作井井有条，员工看到一切都有条不紊的进行，会觉得一切都在计划之中，其信心自然大大增强；

l加强项目控制，让员工感受到目标正在一步步靠近；

l让员工及时看到整个团队的工作成果；

l项目经理要展示自己的能力、活力和信心，员工首先要对项目经理充满信心，才能对工作充满信心；

l帮助员工克服困难，实现目标。

4.目标是一种承诺

为什么要让目标获得团队的认同？一个重要原因就是获得员工的承诺。当员工接受并认可工作安排的时候，其实就是做出了一种承诺：我承诺按照目标的要求，及时保质保量的完成任务。一个人一旦做出承诺，就会想办法去实现自己的承诺，这是一种内在的力量，可以促使其自主自发的完成任务，达到目标。

转自：http://developer.51cto.com/art/201306/397594.htm

一、 
查看端口和防火墙状态：#netstat -lnt |grep 80
1)        开启80端口命令：/sbin/iptables -I INPUT -p tcp --dport 80 -j ACCEPT
2)        保存配置命令：/etc/rc.d/init.d/iptables save
3)        重启服务命令：/etc/rc.d/init.d/iptables restart
4)        查看已经开放的端口： /etc/init.d/iptables status
* 打开指令  
iptables -A INPUT -p tcp -s 192.168.245.223--dport 3306 -j ACCEPT  
iptables -A INPUT -p tcp -s 192.168.245.223--dport 80 -j ACCEPT  
* 关闭指令  
iptables -D INPUT -p tcp -s xxx.xxx.xxx.xxx--dport 3306 -j ACCEPT
/etc/sysconfig/network 包括主机基本网络信息，用于系统启动
/etc/sysconfig/network-script/ 此目录下是系统启动最初始化网络的信息
/etc/sysconfig/network-script/ifcfg-eth0 网络配置信息
/etc/xinetd.conf 定义了由超级进程XINETD启动的网络服务
/etc/protocols 设定了主机使用的协议以及各个协议的协议号
/etc/services 设定了主机的不同端口的网络服务
/etc/sysconfig/iptables 防火墙配置信息
防火墙配置命令
关闭
/etc/rc.d/init.d/iptables stop
开启
/etc/rc.d/init.d/iptables start
查看当前配置：iptables -L
redhat :
chkconfig --level 2345 iptables off
service iptables stop
但是不推荐关闭防火墙
1) 重启后生效
开启： chkconfig iptables on
关闭： chkconfig iptables off
2) 即时生效，重启后失效
开启： service iptables start
关闭： service iptables stop
需要说明的是对于Linux下的其它服务都可以用以上命令执行开启和关闭操作。
在开启了防火墙时，做如下设置，开启相关端口，
修改/etc/sysconfig/iptables 文件，添加以下内容：
-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT
-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
将网卡禁用
　　ifconfig eth0 down
将网卡启用
　　ifconfig eth0 up
修改Ip地址
方法一
ifconfig eth0 1.2.3.4 netmask 255.0.0.0 up
重新启动网络
service network restart
方法二：纯修改配置文件修改配置
cd  /etc/sysconfig/network-scripts
vi ifcfg-eth0    #拿我机子举例，我只有一块网卡，就是eth0
==============================   
DEVICE=eth0
BOOTPROTO=static    #这里将auto改为static即自动改为静态
BROADCAST=192.168.2.255  #这里修改为你设置的局域网广播地址（可以不写这行）
HWADDR=00:0C:29:1D:9F:22 #这里是硬件地址（可以不写这行）
IPADDR=192.168.2.173  #这里写上你要设置的IP地址。
NETMASK=255.255.255.0 #掩码
NETWORK=192.168.2.0 #网络号
ONBOOT=yes  #开机即启动网卡。
TYPE=Ethernet #这是类型，当然也可以不写这一行。
=======================
DNS修改
编辑文件vi /etc/resolv.conf 
 /* 使用route 命令配置路由表 */
//添加到主机路由
# route add –host 192.168.168.110 dev eth0:1
# route add –host 192.168.168.119 gw 192.168.168.1
//添加到网络的路由
# route add –net IP netmask MASK eth0
# route add –net IP netmask MASK gw IP
# route add –net IP/24 eth1
//添加默认网关
# route add default gw IP
//删除路由
# route del –host 192.168.168.110 dev eth0:1
/* 常用命令 */
# traceroute www.baidu.com
# ping www.baidu.com
//显示网络接口状态信息
# netstat –I
//显示所有监控的服务器的Socket和正在使用Socket的程序信息
# netstat –lpe
//显示内核路由表信息
# netstat –r
# netstat –nr
//显示TCP/UDP传输协议的连接状态
# netstat –t
# netstat –u
//更改主机名
# hostname myhost
//查看ARP缓存
# arp
//添加
# arp –s IP MAC
//删除
# arp –d IP
 /* 运行级别与网络服务 */
//查看当前运行级别
# runlevel
//运行级别的切换
# init
# telinit
 

 linux 基本命令整理：
 ls     (list 显示当前目录下文件和目录 ls -l 详细显示 =ll )
[root@linux ~]# ls [-aAdfFhilRS] 目录名称 
[root@linux ~]# ls [--color={none,auto,always}] 目录名称 
[root@linux ~]# ls [--full-time] 目录名称 
参数： 
-a ：全部的档案，连同隐藏档( 开头为 . 的档案) 一起列出来～ 
-A ：全部的档案，连同隐藏档，但不包括 . 与 .. 这两个目录，一起列出来～ 
-d ：仅列出目录本身，而不是列出目录内的档案数据 
-f ：直接列出结果，而不进行排序 (ls 预设会以档名排序！) 
-F ：根据档案、目录等信息，给予附加数据结构，例如： 
*：代表可执行档； /：代表目录； =：代表 socket 档案； |：代表 FIFO 档案； 
-h ：将档案容量以人类较易读的方式(例如 GB, KB 等等)列出来； 
-i ：列出 inode 位置，而非列出档案属性； 
-l ：长数据串行出，包含档案的属性等等数据； 
-n ：列出 UID 与 GID 而非使用者与群组的名称 (UID与GID会在账号管理提到！) 
-r ：将排序结果反向输出，例如：原本档名由小到大，反向则为由大到小； 
-R ：连同子目录内容一起列出来； 
-S ：以档案容量大小排序！ 
-t ：依时间排序 
--color=never ：不要依据档案特性给予颜色显示； 
--color=always ：显示颜色 
--color=auto ：让系统自行依据设定来判断是否给予颜色 
--full-time ：以完整时间模式 (包含年、月、日、时、分) 输出 
--time={atime,ctime} ：输出 access 时间或改变权限属性时间 (ctime) 
而非内容变更时间 (modification time)      
cat 由第一行开始显示档案内容 
 [root@linux ~]# cat [-AEnTv] 
参数： 
-A ：相当于 -vET 的整合参数，可列出一些特殊字符～ 
-E ：将结尾的断行字符 $ 显示出来； 
-n ：打印出行号； 
-T ：将 [tab] 按键以 ^I 显示出来； 
-v ：列出一些看不出来的特殊字符      
tac 从最后一行开始显示，可以看出 tac 是 cat 的倒着写！
nl 显示的时候，顺道输出行号！ 
 
[root@linux ~]# nl [-bnw] 档案 
参数： 
-b ：指定行号指定的方式，主要有两种： 
-b a ：表示不论是否为空行，也同样列出行号； 
-b t ：如果有空行，空的那一行不要列出行号； 
-n ：列出行号表示的方法，主要有三种： 
-n ln ：行号在屏幕的最左方显示； 
-n rn ：行号在自己字段的最右方显示，且不加 0 ； 
-n rz ：行号在自己字段的最右方显示，且加 0 ； 
-w ：行号字段的占用的位数。      

more 一页一页的显示档案内容 
  空格键 (space)：代表向下翻一页；
 Enter ：代表向下翻『一行』；
 /字符串：代表在这个显示的内容当中，向下搜寻『字符串』；
 :f ：立刻显示出文件名以及目前显示的行数；
 q ：代表立刻离开 more ，不再显示该档案内容。 

less 与 more 类似，但是比 more 更好的是，他可以往前翻页！
空格键 ：向下翻动一页； 
[pagedown]：向下翻动一页； 
[pageup] ：向上翻动一页； 
/字符串 ：向下搜寻『字符串』的功能； 
?字符串 ：向上搜寻『字符串』的功能； 
n ：重复前一个搜寻 (与 / 或 ? 有关！) 
N ：反向的重复前一个搜寻 (与 / 或 ? 有关！) 
q ：离开 less 这个程序； 

head 只看头几行 
 
[root@linux ~]# head [-n number] 档案 
参数： 
-n ：后面接数字，代表显示几行的意思      

tail 只看尾巴几行   tail -200f logfile2 ( 显示日志最后 200 行 )
od 以二进制的方式读取档案内容！ 
 
[root@linux ~]# od [-t TYPE] 档案 
参数： 
-t ：后面可以接各种『类型 (TYPE)』的输出，例如： 
a ：利用预设的字符来输出； 
c ：使用 ASCII 字符来输出 
d[size] ：利用十进制(decimal)来输出数据，每个整数占用 size bytes ； 
f[size] ：利用浮点数值(floating)来输出数据，每个数占用 size bytes ； 
o[size] ：利用八进位(octal)来输出数据，每个整数占用 size bytes ； 
x[size] ：利用十六进制(hexadecimal)来输出数据，每个整数占用 size bytes ；      


 chmod  ( chmod +R filename增加文件读写执行权限,+R 可读,+W 可写,+X 可执行
        ( chmod 777 filename 增加文件读写执行权限的另一种方式,
                            7=&gt; 对应8进制的 111 可读可写可执行)
        
 chown  ( chown -R haowen .将当前目录下所有文件和目录权限赋给haowen 
          ,-R 包括子目录)
 chgrp -R mysql . (把当前文件夹变更到mysql群组,mysql是已经有的群组)变更文件或目录的所属群组。

 umask 档案预设权限：
 umask 指定的是『该默认值需要减掉的权限！』

chattr (设定档案隐藏属性) 
lsattr (显示档案隐藏属性) 
 
 find   ( find ./ -name file1 -print ,从当前目录向下查找名为file1 的文件)
 mkdir  ( mkdir  dir1 ,新建目录 dir1 ) 
 
mkdir [-mp] 目录名称 
参数： 
-m ：设定档案的权限喔！直接设定，不需要看预设权限 (umask) 的脸色～ 
-p ：帮助你直接将所需要的目录递归建立起来！      
  
[root@linux ~]# rmdir [-p] 目录名称 
参数： 
-p ：连同上层『空的』目录也一起删除      

 pwd   Print Working Directory  ( pwd  ,显示当前路径 ) pwd-P 显示出确实的路径,而非使用连接(link)路径

 cd     ( cd /usr/local/   进入目录/usr/local/ , cd ../ 返回到上一级目录  
                          ./ 当前目录 ../父目录 - 代表前一个工作目录 ~代表[目前使用者身份]所在的家目录  ~account代表account这个使用者的家目录)针对 cd 的使用方法，如果仅输入 cd 时，代表的就是『 cd           ~ 』
 
 mv     ( mv file1  /home/haowen/ ,将文件移动到目录/home/haowen/下 
                                 ,相当于 window 剪切 )
        ( mv file1 filenew1 ,将文件名改为filenew1 )
 
[root@linux ~]# mv [-fiu] source destination 
[root@linux ~]# mv [options] source1 source2 source3 .... directory 
参数： 
-f ：force 强制的意思，强制直接移动而不询问； 
-i ：若目标档案 (destination) 已经存在时，就会询问是否覆盖！ 
-u ：若目标档案已经存在，且 source 比较新，才会更新 (update)      

 cp     ( cp file1 /home/haowen/  ,将文件复制copy到目录/home/haowen/下  
          cp -r dir1/home/haowen/ 
          cp file1 ./file2 复制文件并改名)
 
[root@linux ~]# cp [-adfilprsu] 来源档(source) 目的檔(destination) 
[root@linux ~]# cp [options] source1 source2 source3 .... directory 
参数： 
-a ：相当于 -pdr 的意思； 
-d ：若来源文件为连结文件的属性(link file)，则复制连结文件属性而非档案本身； 
-f ：为强制 (force) 的意思，若有重复或其它疑问时，不会询问使用者，而强制复制； 
-i ：若目的檔(destination)已经存在时，在覆盖时会先询问是否真的动作！ 
-l ：进行硬式连结 (hard link) 的连结档建立，而非复制档案本身； 
-p ：连同档案的属性一起复制过去，而非使用预设属性； 
-r ：递归持续复制，用于目录的复制行为； 
-s ：复制成为符号连结文件 (symbolic link)，亦即『快捷方式』档案； 
-u ：若 destination 比 source 旧才更新 destination ！      

 rm     ( rm file1 ,rm -r dir1,rm -rf dir2 删除文件或目录, f不提示输入y 
 
[root@linux ~]# rm [-fir] 档案或目录 
参数： 
-f ：就是 force 的意思，强制移除； 
-i ：互动模式，在删除前会询问使用者是否动作 
-r ：递归删除啊！最常用在目录的删除了      

touch 建立一个空的档案,将某个档案日期修订为目前 (mtime 与 atime) 
 
[root@linux ~]# touch [-acdmt] 档案 
参数： 
-a ：仅修订 access time； 
-c ：仅修改时间，而不建立档案；
-d ：后面可以接日期，也可以使用 --date="日期或时间" 
-m ：仅修改 mtime ； 
-t ：后面可以接时间，格式为[YYMMDDhhmm]      
    
file 如果你想要知道某个档案的基本数据，例如是属于 ASCII 或者是 data 档案，或者是 binary ，且其中有没有使用到动态函式库 (share library) 等等的信息，就可以利用 file 这个指令来检阅喔！ 

which (寻找『执行档』) 这个指令是根据『PATH』这个环境变量所规范的路径，去搜寻『执行档』的档名
 
[root@linux ~]# which [-a] command 
参数： 
-a ：将所有可以找到的指令均列出，而不止第一个被找到的指令名称      

whereis (从数据库寻找特定档案)
 
[root@linux ~]# whereis [-bmsu] 档案或目录名 
参数： 
-b :只找 binary 的档案 
-m :只找在说明文件 manual 路径下的档案 
-s :只找 source 来源档案 
-u :没有说明档的档案！      

功能说明：计算字数。
语 　 法：wc[-clw][--help][--version][文件名]
补充说明：利用wc指令我们可以计算文件的Byte数、字数、或是列数，若不指定任何文件名称，或是所给予的文件名为"-"，则wc指令会从标准输入设备读取数据。假设不给予其参数，wc指令会一并显示列数、字数和Byte数
参 　 数：
-c 只显示Byte数，亦即字符数；
-l 只显示列数；
-w 只显示字数；
-m 同样显示字符数
--help 在线帮助；
--version 显示此软件的版本信息。


locate 从数据库列出某个档案的完整档名

find ./ -name index.jsp 查找当前目录下名称为index.jsp的文件

 grep   ( grep "mobile=13712345678"  logfile1 ,在logfile1中 
          搜索查找内容"mobile=13712345678" )
 
 ping   ( ping 61.129.78.9 ,ping www.163.com ,测试网络连接是否正常 )
 ifconfig  ( ifconfig ,查看本机 IP地址，子网掩码等 )
 
 ps    ( ps aux 查看系统中已经启动的进程, ps aux| grep programe1 ,
         查看程序1是否正在运行
 kill  ( kill -9  2325 ,杀死进程号为 2325的进程, 
         killall  programe1 ,杀死programe1进程 )
 reboot ( 重启系统 )
 init 0 ( 关机 ,仅 root 用户有权操作 )
 init 6 ( 重启系统 ,仅 root 用户有权操作 )
 
 gzip   ( gzip file1 ,压缩文件 file1 )
 gunzip ( gunzip file1.gz  解压缩文件 file1.gz )
 
 tar -zcvf ( tar -zcvf  dir1.tar.gz ./dir1  ,将当前目录下 dir1目录所有内容
           压缩打包,包名dir1.tar.gz)
 tar -zxvf ( tar -zxvf  dir1.tar.gz ,解开压缩包 )
  
 echo "hello!" &gt;&gt; file1  ( 将"hello" 添加到文件 file1后面, 
                          当 file1 不存在就创建 file1
 
 vi file2       ( vi 编即器新建文件file2)
               ...输入内容 welcome..
               ( 按 i 进入 insert 状态即插入模式 ,按 Esc 退出插入模式
                 在非插入模式下按 dd 删除光标当前行,按 x 删除当前字,
                 按 j,n,l移动光标 )
 :wq  ( 保存退出 ) :q! (不保存退出) 
 
增加环境变量
 
[root@linux ~]# echo $PATH        
[root@linux ~]# PATH="$PATH":/root      
 
env  显示系统的一些环境变量 
set  显示系统的所有变量 

 chmod:
 Linux/Unix 的档案调用权限分为三级 : 档案拥有者、群组、其他。
 利用 chmod 可以藉以控制档案如何被他人所调用。   
 + 表示增加权限、- 表示取消权限、= 表示唯一设定权限。 
　 r 表示可读取，w 表示可写入，x 表示可执行，
 
 1. 将档案 file1.txt 设为所有人皆可读取 : 
　　 chmod ugo+r file1.txt   或  chmod 444 file1.txt
 
 2. 将文件 file2 设为属主可读写执行,Group,other ,只能读
   chmod 744 file2   ( 7=&gt; "111",4=&gt;"100" 二进制 )
   
 3. 将文件 file3 设为属主可读写执行,Group,other ,无权限操作不能读写执行)
   chmod 700 file3   ( 7=&gt; "111",0=&gt;"000"  )
   
   其中a,b,c各为一个数字，分别表示User、Group、及Other的权限。 
　 
　 r=4，w=2，x=1 若要rwx属性则4+2+1=7；若要rw-属性则4+2=6；
                 若要r-x属性则4+1=5
　 　
  tar:
  tar 调用gzip
　　gzip是GNU组织开发的一个压缩程序，.gz结尾的文件就是gzip压缩的结果。
    与gzip相对的解压程序是gunzip。tar中使用-z这个参数来调用gzip。
　　# tar -czf all.tar.gz *.jpg
    
　　这条命令是将所有.jpg的文件打成一个tar包，并且将其用gzip压缩，生成一个
    gzip压缩过的包，包名为all.tar.gz
    
　　# tar -xzf all.tar.gz   这条命令是将上面产生的包解开。
　　
date 显示日期的指令： 

cal 显示日历的指令： 

bc 简单好用的计算器： 

[Tab] 按键   (按两次) 命令补全: 

[Ctrl]-c 按键中断目前程序: 

[Ctrl]-d 按键  (相当于输入 exit) 键盘输入结束: 

info 在线求助  :  
    
who 要看目前有谁在在线:  
    
finger 显示关于系统用户的信息

netstat -a     看网络的联机状态: 

ntsysv 设置服务随系统启动时同时启动
    
shutdown  ,shutdown -h now  惯用的关机指令： 

reboot, halt, poweroff 重新开机，关机： 

--- 系统相关的命令:---
 dmesg : 例如 dmesg | more  显示系统的诊断信息,操作系统版本号,物理内及其它信息
 df : 例如 df -h 显示硬盘空间
 du :   查看目录中各级子目录使用的硬盘空间
 free:  查看系统内存,虚拟内存(交换空间)的大小占用情况
 top: 动态实时查看系统内存,CPU,进程

 hostname 查看主机名:
 
 hostname 新主机名修改主机名(临时的,重启就没了):

man 命令:查看该命令的基础用法 
info 命令:查看该命令的基础用法
ls -l /lib/modules/`uname -r`/kernel/fs 查看Linux 支持的档案系统有哪些
cat /proc/filesystems  查看Linux目前已启用的档案系统

type 查询某个指令是来自于外部指令(指的是其它非 bash 套件所提供的指令) 或是内建在 bash 当中的指令
 
[root@linux ~]# type [-tpa] name 
参数： 
：不加任何参数时，则 type 会显示出那个 name 是外部指令还是 bash 内建的指令！ 
-t ：当加入 -t 参数时，type 会将 name 以底下这些字眼显示出他的意义： 
file ：表示为外部指令； 
alias ：表示该指令为命令别名所设定的名称； 
builtin ：表示该指令为 bash 内建的指令功能； 
-p ：如果后面接的 name 为指令时，会显示完整文件名(外部指令)或显示为内建指令； 
-a ：会将由 PATH 变量定义的路径中，将所有含有 name 的指令都列出来，包含 alias      

myname=pqb 变量的设定
PATH="$PATH":/home/dmtsai/bin  变量的累加
echo $myname 变量的查看
unset myname 变量的取消

在来看看关机，关闭系统使用Shutdown命令，确保用户和系统的资料完整。只有root用户才能使用这个命令。
一般的用户是不允许执行这个命令的。
我们先看看showdown语法：
shutdown [options] when [message]
options:　-r 表示重启，-h表示系统服务停滞(halt)后，立刻关机，-f表示快速重启
when：　为shutdown指定时间。hh:mm：绝对时间，hh指小时，mm指分钟；如08:30，+m:m分钟后执行，
now=+0，也就是立刻执行
message：表示系统的广播信息，一般提示各个用户系统关机或重启，要求用户保存资料后退出。
我们来看看几个例子：
shutdown -h now 立刻关机
shutdown -h 21:30 今天21：30关机
shutdown -h +10 十分钟后关机
shutdown -r now 立刻重启
shutdown -r +10 ‘the system will reboot’ 10分钟后重启，管理员提示用户系统要重启了，便于用户保存工
作中的资料。只有root用户才能使用这个命令。

创建文件
创建文件是指创建一个一般的普通文件，并且这个文件为空，我们可以使
用touch命令来建立一般文件，如下操作：
[root@Linux two]# touch 111.txt

搜索文件
我们先来学习一下如何搜索文件，特别是刚开始学习Linux的时候，自己建立的文件不知道放在哪里了，常有发
生。如果知道文件名，却不知道文件在那个目录下面了，我们就可以使用locate命令来搜索文件。看如下操作
：
[root@Linux one]# locate install.log
/root/install.log
/root/install.log.syslog
看一下，我们一下就搜索了两个与install.log相关的文件，他们都在/root目录下，同时我们感觉到，使用这个命
令搜索文件的速度比较快，其实要使用这个命令，必须配合数据库来使用，因为这个命令是从数据库中来搜索
文件，这个数据库的更新速度是7天更新一次。如下操作：
[root@Linux one]# touch 001.txt
[root@Linux one]# locate 001.txt
发现这个命令找不到新建立的文件，所以我们要使用这个命令搜索文件之前，必须自己更新一下数据库(更新数据库需要root权限)，如下
操作：
[root@Linux one]# updatedb
[root@Linux one]# locate 001.txt
/root/one/001.txt
看看，如果执行updatedb这个命令更新数据库之后，我们就可以找到我们所需要的数据。不过更新数据库的时
间需要一段时间。


locale能看语言环境
保存语言信息的文件在/etc/sysconfig/i18n中。

/sbin/service xinetd restart|start|stop 启动后台服务， 

/sbin/chkconfig --list |more 显示系统服务启动情况，显示了运行级别0到运行级别6的情况.
这些服务都是靠系统脚本init启动的。还有一些不是靠系统脚本启动的而下面会看到一些特殊服务，他们不是
靠init 启动的。是靠xinetd启动的，是一个独立的互联网服务器的服务器是一个超级服务其，可以启动很多的子服
务器。

大家看到 xinetd这个服务只要他是开启的，就可以运行他下面的服务器，它下面的大部分都是关闭的，只
有一个是开启的，如果我们想开启一个服务可以使用chkconfig命令，例如我们想开启 rsync服务，我们可以使
用chkconfig rsync on|off 命令。

mount
在mount命令不使用任何选项和参数的时候将显示当前linux系统中以挂载的文件系统信息。

mount Cttype dev dir
光盘文件系统类型是：iso9660；dev表示需要挂载文件系统的设备名称，光盘驱动器的设备名称是/dev/cdrom; dir表示挂载点，即挂载到的文件目录路径。
首先介绍光盘的挂载方法：
mount -t iso9660 /dev/cdrom /media/cdrom

列出系统中所有存储设备
fdisk -l命令

使用“vfat”文件系统类型表示所有的fat文件系统类型，包括fat16和fat32，ntfs还是使用ntfs表示。
u盘的挂载方法
mount -t vfat /dev/sdb1 /mnt/
mount -t ntfs /dev/sdb1 /mnt/

umount命令用于卸载已经挂载的文件系统，基本格式如：umount dir device

对于光盘文件系统的卸载可以使用，以下两条命令中的任意一条
umount /dev/cdrom
umount /media/cdrom

u盘的卸载
umount /dev/sdb1

eject命令
eject 弹出光盘命令
eject -t 光盘驱动器自动回收

cut 
使用权限：所有使用者 
用法：cut -cnum1-num2 filename 
说明：显示每行从开头算起 num1 到 num2 的文字。 
范例： 
shell&gt;&gt; cat example 
test2 
this is test1 
shell&gt;&gt; cut -c0-6 example 开头算起前 6 个字元 
test2 
this i 

指令名称:ln 
　　使用权限:所有使用者 
　　使用方式:ln [options] source dist,其中 option 的格式为:
　　[-bdfinsvF] [-S backup-suffix] [-V {numbered,existing,simple}] 
　　[--help] [--version] [--] 

　　说明:Linux/Unix 档案系统中,有所谓的连结(link),我们可以将其视为档案的别名,而连结又可分为两种:硬连结(hard link)与软连结(symbolic link),硬连结的意思是一个档案可以有多个名称,而软连结的方式则是产生一个特殊的档案,该档案的内容是指向另一个档案的位置。硬连结是存在同一个档案系统中,而软连结却可以跨越不同的档案系统。 

　　ln source dist 是产生一个连结(dist)到 source,至于使用硬连结或软链结则由参数决定。 

　　不论是硬连结或软链结都不会将原本的档案复制一份,只会占用非常少量的磁碟空间。

　　-f:链结时先将与 dist 同档名的档案删除-d:允许系统管理者硬链结自己的目录-i:在删除与 dist 同档名的档案时先进行询问-n:在进行软连结时,将 dist 视为一般的档案-s:进行软链结(symbolic link)-v:在连结之前显示其档名-b:将在链结时会被覆写或删除的档案进行备份-S SUFFIX:将备份的档案都加上 SUFFIX 的字尾-V METHOD:指定备份的方式--help:显示辅助说明--version:显示版本 
　　范例:
　　将档案 yy 产生一个 symbolic link:zz 
　　ln -s yy zz 
　　将档案 yy 产生一个 hard link:zz 
　　ln yy xx 

名称:at 
　　使用权限:所有使用者 
　　使用方式:at -V [-q queue] [-f file] [-mldbv] TIME 
　　说明:at 可以让使用者指定在 TIME 这个特定时刻执行某个程式或指令,TIME 的格式是 HH:MM其中的 HH 为小时,MM 为分钟,甚至你也可以指定 am, pm, midnight, noon, teatime(就是下午 4 点锺)等口语词。 
　　如果想要指定超过一天内的时间,则可以用 MMDDYY 或者 MM/DD/YY 的格式,其中 MM 是分钟,DD 是第几日,YY 是指年份。另外,使用者甚至也可以使用像是 now + 时间间隔来弹性指定时间,其中的时间间隔可以是 minutes, hours, days, weeks 
　　另外,使用者也可指定 today 或 tomorrow 来表示今天或明天。当指定了时间并按下 enter 之后,at 会进入交谈模式并要求输入指令或程式,当你输入完后按下 ctrl+D 即可完成所有动作,至于执行的结果将会寄回你的帐号中。
　　把计:
　　-V:印出版本编号 
　　-q:使用指定的伫列(Queue)来储存,at 的资料是存放在所谓的 queue 中,使用者可以同时使用多个 queue,而 queue 的编号为 a, b, c... z 以及 A, B, ... Z 共 52 个 
　　-m:即使程式/指令执行完成后没有输出结果, 也要寄封信给使用者 
　　-f file:读入预先写好的命令档。使用者不一定要使用交谈模式来输入,可以先将所有的指定先写入档案后再一次读入 
　　-l:列出所有的指定 (使用者也可以直接使用 atq 而不用 at -l) 
　　-d:删除指定 (使用者也可以直接使用 atrm 而不用 at -d) 
　　-v:列出所有已经完成但尚未删除的指定 
　　例子:
　　三天后的下午 5 点锺执行 /bin/ls:
　　at 5pm + 3 days /bin/ls 

　　三个星期后的下午 5 点锺执行 /bin/ls:
　　at 5pm + 2 weeks /bin/ls 

　　明天的 17:20 执行 /bin/date:
　　at 17:20 tomorrow /bin/date 
　　1999 年的最后一天的最后一分钟印出 the end of world ! 
　　at 23:59 12/31/1999 echo the end of world ! 

名称：cal 
　　使用权限：所有使用者 
　　使用方式：cal [-mjy] [month [year]] 
　　说明： 
　　显示日历。若只有一个参数,则代表年份(1-9999),显示该年的年历。年份必须全部写出：``cal 89\ 将不会是显示 1989 年的年历。使用两个参数,则表示月份及年份。若没有参数则显示这个月的月历。 
　　1752 年 9 月第 3 日起改用西洋新历,因这时大部份的国家都采用新历,有 10 天被去除,所以该月份的月历有些不同。在此之前为西洋旧历。 
　　匡兜: 
　　-m:以星期一为每周的第一天方式显示。 
　　-j:以凯撒历显示,即以一月一日起的天数显示。 
　　-y:显示今年年历。 
　　范例： 
　　cal:显示本月的月历。
　　[root@mylinux /root]# date 
　　Tue Aug 15 08:00:18 CST 2000 
　　[root@mylinux /root]# cal 
　　...

　　cal 2001:显示公元 2001 年年历。
　　[root@mylinux /root]# cal 2001 
　　...

    cal 5 2001:显示公元 2001 年 5 月月历。
　　[root@mylinux /root]# cal 5 2001 


名称:crontab 
　　使用权限:所有使用者 
　　使用方式:
　　crontab [ -u user ] filecrontab [ -u user ] { -l | -r | -e } 
　　说明:
　　crontab 是用来让使用者在固定时间或固定间隔执行程式之用,换句话说,也就是类似使用者的时程表。-u user 是指设定指定 user 的时程表,这个前提是你必须要有其权限(比如说是 root)才能够指定他人的时程表。如果不使用 -u user 的话,就是表示设定自己的时程表。 
　　参数:

　　-e:执行文字编辑器来设定时程表,内定的文字编辑器是 VI,如果你想用别的文字编辑器,则请先设定 VISUAL 环境变数来指定使用那个文字编辑器(比如说 setenv VISUAL joe) 
　　-r:删除目前的时程表 
　　-l:列出目前的时程表 

　　时程表的格式如下:
　　f1 f2 f3 f4 f5 program 

　　其中 f1 是表示分钟,f2 表示小时,f3 表示一个月份中的第几日,f4 表示月份,f5 表示一个星期中的第几天。program 表示要执行的程式。 
　　当 f1 为 * 时表示每分钟都要执行 program,f2 为 * 时表示每小时都要执行程式,其余类推 
　　当 f1 为 a-b 时表示从第 a 分钟到第 b 分钟这段时间内要执行,f2 为 a-b 时表示从第 a 到第 b 小时都要执行,其余类推 
　　当 f1 为 */n 时表示每 n 分钟个时间间隔执行一次,f2 为 */n 表示每 n 小时个时间间隔执行一次,其余类推 
　　当 f1 为 a, b, c,... 时表示第 a, b, c,... 分钟要执行,f2 为 a, b, c,... 时表示第 a, b, c...个小时要执行,其余类推 
　　使用者也可以将所有的设定先存放在档案 file 中,用 crontab file 的方式来设定时程表。 
　　例子:
　　每月每天每小时的第 0 分钟执行一次 /bin/ls:
　　0 7 * * * /bin/ls 

　　在 12 月内, 每天的早上 6 点到 12 点中,每隔 20 分钟执行一次 /usr/bin/backup:
　　0 6-12/3 * 12 * /usr/bin/backup 

　　周一到周五每天下午 5:00 寄一封信给 alex@domain.name:
　　0 17 * * 1-5 mail -s "hi" alex@domain.name &lt;/tmp/maildata 

　　每月每天的午夜 0 点 20 分, 2 点 20 分, 4 点 20 分....执行 echo "haha" 
　　20 0-23/2 * * * echo "haha" 

　　注意:
　　当程式在你所指定的时间执行后,系统会寄一封信给你,显示该程式执行的内容,若是你不希望收到这样的信,请在每一行空一格之后加上 &gt; /dev/null 2&gt;&amp;1 即可。


名称:sleep 
　　使用权限:所有使用者 
　　使用方式:sleep [--help] [--version] number[smhd] 
　　说明:sleep 可以用来将目前动作延迟一段时间 
　　参数说明:
　　--help:显示辅助讯息 
　　--version:显示版本编号 
　　number:时间长度,后面可接 s,m,h 或 d 
　　其中 s 为秒,m 为分钟,h 为小时,d 为日数 
　　例子:
　　显示目前时间后延迟 1 分钟,之后再次显示时间:
　　date;sleep 1m;date 


　　名称： finger 
　　使用权限： 所有使用者 
　　使用方式： finger [options] user[@address] 
　　说明：finger 可以让使用者查询一些其他使用者的资料。
    范例：下列指令可以查询本机管理员的资料： 
　　finger root 

名称：last 
　　使用权限：所有使用者 
　　使用方式：shell&gt;&gt; last [options] 
　　说明：显示系统开机以来获是从每月初登入者的讯息 
　　把计: 
　　-R 省略 hostname 的栏位 
　　-num 展示前 num 个 
　　username 展示 username 的登入讯息 
　　tty 限制登入讯息包含终端机代号 
　　范例： 

　　shell&gt;&gt; last -R -2 

　名称:write 
　　使用权限:所有使用者 
　　使用方式:
　　write user [ttyname] 
　　说明:传讯息给其他使用者 
　　把计:
　　user:预备传讯息的使用者帐号 
　　ttyname:如果使用者同时有两个以上的 tty 连线,可以自行选择合适的 tty 传讯息 
　　例子.1:
　　传讯息给 Rollaend,此时 Rollaend 只有一个连线:
　　write Rollaend 
    接下来就是将讯息打上去,结束请按 ctrl+c 

　　例子.2 :传讯息给 Rollaend,Rollaend 的连线有 pts/2,pts/3:
　　write Rollaend pts/2

　　接下来就是将讯息打上去,结束请按 ctrl+c 
　　注意:若对方设定 mesg n,则此时讯席将无法传给对方 

名称：expr 
　　使用权限：所有使用者 
　　### 字串长度 
　　shell&gt;&gt; expr length "this is a test" 
　　14 

　　### 数字商数 
　　shell&gt;&gt; expr 14 % 9 
　　5 

　　### 从位置处抓取字串 
　　shell&gt;&gt; expr substr "this is a test" 3 5 
　　is is 

　　### 数字串 only the first character 

　　shell&gt;&gt; expr index "testforthegame" e 
　　2 

　　### 字串真实重现 
　　shell&gt;&gt; expr quote thisisatestformela 
　　thisisatestformela 
 把相同类型文件移动到一个文件夹：
find /usr/bin/Molliza/firefox -name "*.so" -exec cp -f {} /usr/lib64/qt-3.3/lib \；



C/C++连接MySql数据库




本文对如何使用MySql的API连接MySql数据库，开发环境为VS2008.
 
一、VS2008工程设置工作
首先，建立一个windows应用程序的工程，将C/C++-&gt;预处理器-&gt;预处理器定义下的_WINDOWS改为_CONSOLE，

将连接器-&gt;系统-&gt;子系统 选择为控制台。

由于我们要使用Mysql的API，并且我们机子上肯定安装了Mysql数据库，所以我们要将工程的头文件路径指向Mysql安装目录的同文件mysql.h所在的位置，将连接库路径指向libmysql.lib所在的路径，
在我的机子上，Mysql 的安装路径为：C:\Program Files\MySQL\MySQL Server 5.1


我们需要把VS2008的工程中的头文件路径和连接库路径指向上面的两个地方：
 
 
将x项目属性页的C/C++-&gt;常规-&gt;附加包含目录指向：C:\Program Files\MySQL\MySQL Server 5.1\include

 
将项目属性页的链接器-&gt;常规-&gt;附加库目录指向:C:\Program Files\MySQL\MySQL Server 5.1\lib\opt.

将链接器-&gt;输入-&gt;附加依赖项中添加libmysql.lib。

如果不设置链接器-&gt;输入-&gt;附加依赖项中添加libmysql.lib，那么会出现如下的错误：




1&gt;------ 已启动全部重新生成: 项目: MySql-Connect, 配置: Debug Win32 ------ 
1&gt;正在删除项目“MySql-Connect”(配置“Debug|Win32”)的中间文件和输出文件 
1&gt;正在编译... 
1&gt;MySql_Connect.cpp 
1&gt;x:\编程练习\c-c++\c\mysql_connect.cpp(35) : warning C4996: 'scanf': This function or variable may be unsafe. Consider using scanf_s instead. To disable deprecation, use _CRT_SECURE_NO_WARNINGS. See online help for details. 
1&gt;        d:\program files\microsoft visual studio 9.0\vc\include\stdio.h(306) : 参见“scanf”的声明 
1&gt;x:\编程练习\c-c++\c\mysql_connect.cpp(72) : warning C4996: 'sprintf': This function or variable may be unsafe. Consider using sprintf_s instead. To disable deprecation, use _CRT_SECURE_NO_WARNINGS. See online help for details. 
1&gt;        d:\program files\microsoft visual studio 9.0\vc\include\stdio.h(366) : 参见“sprintf”的声明 
1&gt;x:\编程练习\c-c++\c\mysql_connect.cpp(86) : warning C4996: 'sprintf': This function or variable may be unsafe. Consider using sprintf_s instead. To disable deprecation, use _CRT_SECURE_NO_WARNINGS. See online help for details. 
1&gt;        d:\program files\microsoft visual studio 9.0\vc\include\stdio.h(366) : 参见“sprintf”的声明 
1&gt;正在编译资源清单... 
1&gt;Microsoft (R) Windows (R) Resource Compiler Version 6.1.6723.1 
1&gt;Copyright (C) Microsoft Corporation.  All rights reserved. 
1&gt;正在链接... 
1&gt;LINK : 没有找到 d:\我的文档\Visual Studio 2008\Projects\MySql-Connect\Debug\MySql-Connect.exe 或上一个增量链接没有生成它；正在执行完全链接 
1&gt;MySql_Connect.obj : error LNK2019: 无法解析的外部符号 _mysql_close@4，该符号在函数 _main 中被引用 
1&gt;MySql_Connect.obj : error LNK2019: 无法解析的外部符号 _mysql_free_result@4，该符号在函数 _main 中被引用 
1&gt;MySql_Connect.obj : error LNK2019: 无法解析的外部符号 _mysql_num_fields@4，该符号在函数 _main 中被引用 
1&gt;MySql_Connect.obj : error LNK2019: 无法解析的外部符号 _mysql_fetch_row@4，该符号在函数 _main 中被引用 
1&gt;MySql_Connect.obj : error LNK2019: 无法解析的外部符号 _mysql_store_result@4，该符号在函数 _main 中被引用 
1&gt;MySql_Connect.obj : error LNK2019: 无法解析的外部符号 _mysql_error@4，该符号在函数 _main 中被引用 
1&gt;MySql_Connect.obj : error LNK2019: 无法解析的外部符号 _mysql_real_query@12，该符号在函数 _main 中被引用 
1&gt;MySql_Connect.obj : error LNK2019: 无法解析的外部符号 _mysql_select_db@8，该符号在函数 _main 中被引用 
1&gt;MySql_Connect.obj : error LNK2019: 无法解析的外部符号 _mysql_real_connect@32，该符号在函数 _main 中被引用 
1&gt;MySql_Connect.obj : error LNK2019: 无法解析的外部符号 _mysql_init@4，该符号在函数 _main 中被引用 
1&gt;d:\我的文档\Visual Studio 2008\Projects\MySql-Connect\Debug\MySql-Connect.exe : fatal error LNK1120: 10 个无法解析的外部命令 
1&gt;生成日志保存在“file://d:\我的文档\Visual Studio 2008\Projects\MySql-Connect\MySql-Connect\Debug\BuildLog.htm” 
1&gt;MySql-Connect - 11 个错误，3 个警告 
========== 全部重新生成: 成功 0 个，失败 1 个，跳过 0 个 ==========




 
二、连接Mysql和从MySql中取出数据的API介绍
2.1 mysql_real_connect()
2.1.1 函数原型：
MYSQL *mysql_real_connect(MYSQL *mysql, const char *host, const char *user, const char *passwd, const char *db, unsigned int port, const char *unix_socket, unsigned int client_flag)
2.1.2 参数说明：
? 第一个参数应该是一个现存MYSQL结构的地址。在调用mysql_real_connect()之前，你必须调用mysql_init()初始化MYSQL结构。见下面的例子。
? host值可以是一个主机名或一个IP地址。如果host是NULL或字符串"localhost"，假定是到本地主机的一个连接。如果OS支持套接字(Unix)或命名管道(Win32)，使用他们而不是TCP/IP与服务器连接。
? user参数包含用户的MySQL登录ID。如果user是NULL，假定是当前用户。在Unix下，它是当前登录名。在Windows ODBC下，必须明确地指定当前用户名字。见16.4 怎样填写ODBC管理程序中各种域。
? passwd参数为user包含口令。如果passwd是NULL，只有在user表中对于有一个空白口令字段的用户的条目将被检查一个匹配。这允许数据库主管设置MySQL权限，使用户获得不同的口令，取决于他们是否已经指定一个口令。注意：不要试图在调用mysql_real_connect()前加密口令；口令加密自动被客户API处理。
? db是数据库名。如果db不是NULL，连接将缺省数据库设置为这个值。
? 如果port不是0，值对于TCP/IP连接将用作端口号。注意host参数决定连接的类型。
? 如果unix_socket不是NULL，字符串指定套接字或应该被使用的命名管道。注意host参数决定连接的类型。
? client_flag值通常是0，但是在很特殊的情况下可以被设置为下列标志的组合：
标志名字 意味着的标志
CLIENT_FOUND_ROWS 返回找到的(匹配的)行数，不是受到影响的行数。
CLIENT_NO_SCHEMA 不允许db_name.tbl_name.col_name语法。这是为了ODBC；如果你使用该语法，导致语法分析器产生一个错误，它是为在一些ODBC程序捕捉错误是有用的。
CLIENT_COMPRESS 使用压缩协议。
CLIENT_ODBC 客户是一个ODBC客户。这使mysqld变得对ODBC更友好。
2.1.3 返回值
如果连接成功，一个 MYSQL*连接句柄。如果连接失败，NULL。对一个成功的连接，返回值与第一个参数值相同，除非你传递NULL给该参数。
2.1.4 错误
CR_CONN_HOST_ERROR
不能连接MySQL服务器。
CR_CONNECTION_ERROR
不能连接本地MySQL服务器。
CR_IPSOCK_ERROR
不能创建一个IP套接字。
CR_OUT_OF_MEMORY
内存溢出。
CR_SOCKET_CREATE_ERROR
不能创建一个Unix套接字。
CR_UNKNOWN_HOST
不能找到主机名的IP地址。
CR_VERSION_ERROR
由于试图使用一个不同协议版本的一个客户库与一个服务器连接导致的一个协议失配。如果你使用一个非常老的客户库连接一个没有使用--old-protocol选项启动的新服务器，这就能发生。
CR_NAMEDPIPEOPEN_ERROR;
不能在 Win32 上创建一个命名管道。
CR_NAMEDPIPEWAIT_ERROR;
不能在 Win32 上等待一个命名管道。
CR_NAMEDPIPESETSTATE_ERROR;
不能在 Win32 上得到一个管道处理器。
2.2  mysql_select_db()
2.2.1 函数原型
int mysql_select_db(MYSQL *mysql, const char *db)
2.2.2 参数说明
使得由db指定的数据库成为 在由mysql指定的连接上的缺省(当前)数据库。在随后的查询中，这个数据库对于不包括一个显式的数据库指定符的表的引用是缺省数据库。
除非连接的用户能被认证允许使用数据库，否则mysql_select_db()失败。
2.2.3 返回值
成功，零。如果发生一个错误，非零。
2.2.4  错误
CR_COMMANDS_OUT_OF_SYNC
命令以一个不适当的次序被执行。
CR_SERVER_GONE_ERROR
MySQL服务器关闭了。
CR_SERVER_LOST
对服务器的连接在查询期间失去。
CR_UNKNOWN_ERROR
发生一个未知的错误。
 
2.3 mysql_real_query
2.3.1 函数原型
int mysql_real_query(MYSQL *mysql, const char *query, unsigned int length)
2.3.2 参数说明
执行由query指向的SQL查询，它应该是一个length个字节的字符串。查询必须由一个单个的SQL语句组成。你不应该在语句后增加一个终止的分号(“;”)或\g。
对于包含二进制数据的查询，你必须使用mysql_real_query()而不是mysql_query()，因为二进制代码数据可能包含“\0”字符，而且，mysql_real_query()比mysql_query()更快，因为它对查询字符串调用strlen()。
2.3.3 返回值
如果查询成功，零。如果发生一个错误，非零。
2.3.4  错误
CR_COMMANDS_OUT_OF_SYNC
命令以一个不适当的次序被执行。
CR_SERVER_GONE_ERROR
MySQL服务器关闭了。
CR_SERVER_LOST
对服务器的连接在查询期间失去。
CR_UNKNOWN_ERROR
发生一个未知的错误。
 
2.4 mysql_store_result
2.4.1 函数原型
MYSQL_RES *mysql_store_result(MYSQL *mysql)
2.4.2 返回值
A MYSQL_RES result structure with the results. NULL (0) if an error occurred.
2.5  mysql_fetch_row()
Description
Retrieves the next row of a result set. When used after mysql_store_result(), mysql_fetch_row() returns NULL when
 there are no more rows to retrieve. When used after mysql_use_result(), mysql_fetch_row() returns NULL when
 there are no more rows to retrieve or if an error occurred.
The number of values in the row is given by mysql_num_fields(result).
 If row holds the return value from a call tomysql_fetch_row(),
 pointers to the values are accessed as row[0] to row[mysql_num_fields(result)-1]. NULL values in the row are indicated by NULL pointers.
The lengths of the field values in the row may be obtained by calling mysql_fetch_lengths().
 Empty fields and fields containing NULL both have length 0; you can distinguish these by checking the pointer for the field value. If the pointer is NULL, the field is NULL;
 otherwise, the field is empty.
Return Values
A MYSQL_ROW structure for the next row. NULL if there are no more rows to retrieve or if an error occurred.
Errors
Note that error is not reset between calls to mysql_fetch_row()


CR_SERVER_LOST
The connection to the server was lost during the query.

CR_UNKNOWN_ERROR
An unknown error occurred.

参考资料：http://dev.mysql.com/doc/refman/5.6/en/index.html
 
三、利用Mysql库提供的API编写连接Mysql和从Mysql中取出数据的代码

   1:  #include &lt;windows.h&gt;
   2:  #include "stdio.h"
   3:  #include "winsock.h" 
   4:  #include "mysql.h" 
   5:   
   6:   
   7:  int main()
   8:  {
   9:   
  10:  MYSQL * con; //= mysql_init((MYSQL*) 0); 
  11:  MYSQL_RES *res;
  12:  MYSQL_ROW row;
  13:   
  14:   
  15:  char tmp[400];
  16:   
  17:  //database configuartion
  18:  char dbuser[30]="root"; 
  19:  char dbpasswd[30]="apple";
  20:  char dbip[30]="localhost";
  21:  char dbname[50]="excel";
  22:  char tablename[50]="test";
  23:  char *query=NULL;
  24:   
  25:   
  26:  int x;
  27:  int y;
  28:  int rt;//return value
  29:  unsigned int t;
  30:   
  31:  int count = 0;
  32:   
  33:   
  34:  printf("input x,y\n");
  35:  scanf("%d,%d",&amp;x,&amp;y);
  36:  fflush(stdin);
  37:  printf("input over\n");
  38:  con = mysql_init((MYSQL*) 0); 
  39:   
  40:   
  41:  if ( con !=NULL &amp;&amp; mysql_real_connect(con,dbip,dbuser,dbpasswd,dbname,3306/*TCP IP端口*/,NULL/*Unix Socket 连接类型*/,0/*运行成ODBC数据库标志*/) ) 
  42:  { 
  43:      if (!mysql_select_db(con,dbname)) 
  44:      { 
  45:          printf("Select successfully the database!\n"); 
  46:          
  47:          con -&gt;reconnect = 1; 
  48:   
  49:          query = "set names \'GBK\'";
  50:          //mysql_query(con,"set names \'GBK\'"); 
  51:          
  52:          rt=mysql_real_query(con,query,strlen(query));
  53:          if (rt)
  54:          {
  55:              printf("Error making query: %s !!!\n",mysql_error(con));
  56:          }
  57:          else
  58:          {
  59:              printf("query %s succeed!\n",query);
  60:          }
  61:          
  62:      }
  63:  }
  64:   
  65:  else
  66:  {
  67:      MessageBoxA(NULL,"Unable to connect the database,check your configuration!","",NULL);
  68:   
  69:  }
  70:   
  71:      //sprintf(tmp,"update %s set 商品=\'%s\',卖出=%d,成交=%d,涨跌=%d,买进=%d,总量=%d,涨幅=%f,时间=\'%s\' where  %s",tablename,goods,sold,deal,fluctuate,buy,total,delta,time,UpdateCon);
  72:      sprintf(tmp,"insert into %s values(%s,%d,%d)",tablename,"null",x,y); //注意如何向具有自增字段的数据库中插入记录
  73:      //MessageBoxA(NULL,tmp,tmp,MB_OK);
  74:      //mysql_query(con,tmp);
  75:   
  76:      rt=mysql_real_query(con,tmp,strlen(tmp));
  77:      if (rt)
  78:      {
  79:          printf("Error making query: %s !!!\n",mysql_error(con));
  80:      }
  81:      else
  82:      {
  83:          printf("%s executed!!!\n",tmp);
  84:      }
  85:      
  86:      sprintf(tmp,"select * from %s",tablename);
  87:      
  88:      rt=mysql_real_query(con,tmp,strlen(tmp));
  89:      if (rt)
  90:      {
  91:          printf("Error making query: %s !!!\n",mysql_error(con));
  92:      } 
  93:      else
  94:      {
  95:          printf("%s executed!!!\n",tmp);
  96:      }
  97:      
  98:      res = mysql_store_result(con);//将结果保存在res结构体中
  99:   
 100:      while(row = mysql_fetch_row(res))  
 101:      {  
 102:          /** 
 103:          * MYSQL_ROW STDCALL mysql_fetch_row(MYSQL_RES *result); 
 104:           * 检索行 
 105:          */  
 106:   
 107:         for(t=0;t&lt;mysql_num_fields(res);t++)  
 108:          {  
 109:              printf("%s  ",row[t]);  
 110:          }  
 111:          printf(".............\n");  
 112:          count ++;
 113:      }  
 114:     printf("number of rows %d\n",count);
 115:      printf("mysql_free_result...\n");  
 116:      mysql_free_result(res);  
 117:   
 118:      mysql_close(con);
 119:      return 0;
 120:   
 121:  }

 
 
四、运行结果

 
五、数据库脚本
/*
Navicat MySQL Data Transfer

Source Server         : localhost
Source Server Version : 50141
Source Host           : localhost:3306
Source Database       : excel

Target Server Type    : MYSQL
Target Server Version : 50141
File Encoding         : 65001

Date: 2011-09-23 10:41:43
*/
 

   1:  SET FOREIGN_KEY_CHECKS=0;
   2:  -- ----------------------------
   3:  -- Table structure for `test`
   4:  -- ----------------------------
   5:  DROP TABLE IF EXISTS `test`;
   6:  CREATE TABLE `test` (
   7:    `x` bigint(4) NOT NULL AUTO_INCREMENT,
   8:    `y` int(4) DEFAULT NULL,
   9:    `z` int(4) DEFAULT NULL,
  10:    PRIMARY KEY (`x`)
  11:  ) ENGINE=InnoDB AUTO_INCREMENT=118 DEFAULT CHARSET=latin1;
  12:   
  13:  -- ----------------------------
  14:  -- Records of test
  15:  -- ----------------------------
  16:  INSERT INTO `test` VALUES ('95', '12432', '4334');
  17:  INSERT INTO `test` VALUES ('96', '213', '321');
  18:  INSERT INTO `test` VALUES ('97', '213', '213');
  19:  INSERT INTO `test` VALUES ('98', '123', '231');
  20:  INSERT INTO `test` VALUES ('99', '321', '231');
  21:  INSERT INTO `test` VALUES ('100', '123', '32132');
  22:  INSERT INTO `test` VALUES ('101', '777', '32213');
  23:  INSERT INTO `test` VALUES ('102', '123', '213');
  24:  INSERT INTO `test` VALUES ('103', '21', '321');
  25:  INSERT INTO `test` VALUES ('104', '324', '432');
  26:  INSERT INTO `test` VALUES ('105', '132', '231');
  27:  INSERT INTO `test` VALUES ('106', '324', '342');
  28:  INSERT INTO `test` VALUES ('107', '23', '23');
  29:  INSERT INTO `test` VALUES ('108', '12', '21');
  30:  INSERT INTO `test` VALUES ('109', '231', '321');
  31:  INSERT INTO `test` VALUES ('110', '123', '231');
  32:  INSERT INTO `test` VALUES ('111', '123', '231');
  33:  INSERT INTO `test` VALUES ('112', '123', '123');
  34:  INSERT INTO `test` VALUES ('113', '312', '231');
  35:  INSERT INTO `test` VALUES ('114', '312', '321');
  36:  INSERT INTO `test` VALUES ('115', '23', '3');
  37:  INSERT INTO `test` VALUES ('116', '213', '312');
  38:  INSERT INTO `test` VALUES ('117', '2', '3');
  39:   
  40:  -- ----------------------------
  41:  -- Table structure for `xqdata`
  42:  -- ----------------------------
  43:  DROP TABLE IF EXISTS `xqdata`;
  44:  CREATE TABLE `xqdata` (
  45:    `代码` varchar(20) NOT NULL DEFAULT '',
  46:    `商品` varchar(20) CHARACTER SET utf8 DEFAULT NULL,
  47:    `卖出` bigint(20) DEFAULT NULL,
  48:    `成交` bigint(20) DEFAULT NULL,
  49:    `涨跌` bigint(20) DEFAULT NULL,
  50:    `买进` bigint(20) DEFAULT NULL,
  51:    `总量` bigint(20) DEFAULT NULL,
  52:    `涨幅` double DEFAULT NULL,
  53:    `时间` time DEFAULT NULL,
  54:    PRIMARY KEY (`代码`)
  55:  ) ENGINE=InnoDB DEFAULT CHARSET=latin1;
  56:   
  57:  -- ----------------------------
  58:  -- Records of xqdata
  59:  -- ----------------------------
  60:  INSERT INTO `xqdata` VALUES ('FITX*1', '商品', '34', '43', '23', '34', '0', '1.4', '13:23:08');




一、面向对象基本概念：类、对象、封装、继承、多态、打包器
（1）   封装：就是把数据和行为结合在一起形成统一的整体，并对对象使用者隐藏数据的实现过程。
（2）   继承：Java继承是使用已存在的类的定义作为基础建立新类的技术，继承避免了对一般类和特殊类之间共同特征进行的重复描述。
多态：多态指同一个实体同时具有多种形式。
类与对象关系：类是描述具有相同特征的一类事物的基本原型，定义了这类事物所拥有的数据特征以及可以执行的操作；对象是类的实例，是类的具体化
java类是面向对象封装概念的基本体现，java类封装了抽象概念的数据（属性）与行为（方法）。
java类的基本构成：属性、方法、构造方法
属性是是类对抽象概念数据特征的描述，方法是类对概念行为的描述，构造方法在类的对象实例化时对类对象进行初始化。
（3）   重载与覆写（重写）
重载：方法的重载是多态性的体现，重载方法具有相同的方法名称，但方法参数列表不同（参数类型或数目不同），重载为相似功能提供了不同的实现
重写：重写是指在子类中覆盖父类方法的实现，对父类方法进行重新定义，当父类引用指向子类对象并调用重写方法时，将调用子类方法的实现。子类函数的访问修饰权限不能低于父类的。
（4）   super与this
super代表当前类的父类（超类），子类的构造函数如果要引用super的话，必须把super放在函数的首位；当子类变量与父类变量重名时，使用super调用父类变量
this代表当前对象，this使用：this.属性，this.方法，this（）；使用this来区别重名的局部变量与成员变量；使用this在一个构造函数中调用其他的重载构造函数
java修饰符：访问权限修饰符、final、static、abstract
访问权限修饰符：public、protected、private、default
public 成员对所有类可见
private 成员仅类内部可见
protected相同包中的类可以访问（包访问权限）；基类通过protected把基类中的成员的访问权限赋予派生类不是所有类（派生类访问权限）。
default如果一个类的成员没有任何权限修饰，那么它门就是缺省包访问权限
类仅能用public或默认权限修饰
（5）   final修饰符：
       final修饰成员变量：该变量为常量；修饰方法：该方法不能够在子类中被重写；修饰类：该类不能被继承
（6）   static修饰符：
       修饰成员变量：该变量为静态变量（类变量），属于类本身，所有该类对象公用该变量；
       修饰方法：该方法为静态方法，在静态方法中不能使用非静态成员变量或方法，因为在静态方法调用时可能还没有对象被创建，没有对象也就无法获取其成员。静态成员函数中也不能使用this或者super，因为它们是和类的对象相关联的
       静态内部类：静态内部类可以对照静态成员变量来理解
abstract修饰符：
      定义抽象类、方法
     抽象方法 没有方法体｛｝，仅有方法声明
抽象类中可以没有抽象方法，有抽象方法的类一定是抽象类
（7）   Java中定义类：
class 类名
用Java语法定义人类：
public class Person
{
}
（8）  定义对象：
对象声明：类名对象名；
对象创建对象名 =  new类名（）； new作用：分配内存空间。也可以合写为：类名对象名
 = new类名();
（9）   Java基本类型打包器
每一个基本类型 都有一个与之对应的类，这个类就称为对象包装器。 
例如，int 的对象包装器就是 Integer。 
其余的还有： 
   Long、Double、Float、Short、Byte、Character... 
   它能提供丰富的方法！ 
   注意对象包装器是不可以变的，一旦构造了包装器，就不容许更改其中的值了！ 
自动打包 解包 
申明了一个list，List的add方法： 
boolean add(Object obj); 
如果我们只是要在list中加入一些简单的int数据，还得 
list.add(new Integer(10)); 
麻烦吗？ 
要是可以list.add(10);就好了！ 
java se 5.0 可以了，他能将list.add(10);自动转成list.add(new Integer(10)); 
这个过程就是自动打包的过程！ 
取数据的时候，以前得： 
int a = list.get(i).intValue(); 
现在： 
int a = list.get(i); 
自动完成的这个过程就是自动解包。 
API【Integer】 
int intValue(); 
static String toString(int i ); 
static int parseInt(String s); 
static Integer valueOf(String s); 
    基本数据(Primitive)类型的自动装箱(autoboxing)、拆箱(unboxing)是自J2SE 5.0开始提供的功能。虽然为您打包基本数据类型提供了方便，但提供方便的同时表示隐藏了细节，建议在能够区分基本数据类型与对象的差别时再使用。 
     autoboxing和unboxing 
     在Java中，所有要处理的东西几乎都是对象 (Object)，例如之前所使用的Scanner是对象，字符串(String)也是对象，之后还会看到更多的对象。然而基本(Primitive)数据类型不是对象，也就是您使用int、double、boolean等定义的变量，以及您在程序中直接写下的字面常量。 
     在前一个小节中已经大致看到了操作对象的方便性，而使用Java有一段时间的人都知道，有时需要将基本数据类型转换为对象。例如使用Map对象要操作put()方法时，需要传入的参数是对象而不是基本数据类型。 
     要使用打包类型(Wrapper Types)才能将基本数据类型包装为对象，前一个小节中您已经知道在J2SE 5.0之前，要使用以下语句才能将int包装为一个Integer对象：Integer integer = new Integer(10); 
     在 J2SE 5.0之后提供了自动装箱的功能，您可以直接使用以下语句来打包基本数据类型：Integer integer = 10; 
     在进行编译时，编译器再自动根据您写下的语句，判断是否进行自动装箱动作。在上例中integer参考的会是Integer类的实例。同样的动作可以适用于 boolean、byte、short、char、long、float、double等基本数据类型，分别会使用对应的打包类型(Wrapper Types)Boolean、Byte、Short、Character、Long、Float或Double。下面直接使用自动装箱功能来改写范例 4.4。 
     范例4.5   AutoBoxDemo.java 
     public class AutoBoxDemo { 
         public static voidmain(String[] args) { 
            Integer data1 = 10; 
            Integer data2 =20;            
             // 转为double值再除以3 
            System.out.println(data1.doubleValue() / 3); 
             // 进行两个值的比较 
            System.out.println(data1.compareTo(data2)); 
         } 
     } 
     程序看来简洁了许多，data1与data2在运行时就是Integer的实例，可以直接进行对象操作。执行的结果如下： 
     3.3333333333333335 
     –1 
     自动装箱运用的方法还可以如下： 
     int i = 10; 
     Integer integer = i; 
     也可以使用更一般化的java.lang.Number类来自动装箱。例如： 
     Number number = 3.14f; 
     3.14f会先被自动装箱为Float，然后指定给number。 
     从J2SE 5.0开始可以自动装箱，也可以自动拆箱(unboxing)，也就是将对象中的基本数据形态信息从对象中自动取出。例如下面这样写是可以的： 
     Integer fooInteger = 10; 
     int fooPrimitive = fooInteger; 
     fooInteger引用至自动装箱为Integer的实例后，如果被指定给一个int类型的变量fooPrimitive，则会自动变为int类型再指定给fooPrimitive。在运算时，也可以进行自动装箱与拆箱。例如： 
     Integer i = 10; 
     System.out.println(i + 10); 
     System.out.println(i++); 
     上例中会显示20与10，编译器会自动进行自动装箱与拆箱，也就是10会先被装箱，然后在i + 10时会先拆箱，进行加法运算；i++该行也是先拆箱再进行递增运算。再来看一个例子： 
     Boolean boo = true; 
     System.out.println(boo &amp;&amp; false); 
     同样的boo原来是Boolean的实例，在进行AND运算时，会先将boo拆箱，再与false进行AND运算，


一、True和False
       Java中不允许讲一个非布尔(boolean)类型的的变量当做布尔值使用，虽然C和C++中可以(True=非零，False=0)，若想在布尔测试中使用一个非布尔值，比如if(a)中，那么必须要加上一个条件表达式将其转化成布尔值，例如if(!=0)
二、If-else
      if-else是控制流程的最基本形式
      if(Boolean-expression)
             statement
       
若if后面扩后内boolean值为True则执行statement,statement是指用分号分开的的单句，或符合语句，复杂句应被扩在大括号中
       
      If(Boolean-expression){
                    ……..;
                    ……..;
       }
使用if-else判断两数字是大于小于或等于
public class IfElse{
       staticint result = 0;
       staticvoid test(int  testval,int  target){
             if(testvlal&gt;target)
                    result = +1;
             else if(testval&lt;target)
                    result = -1;
             else
                    result = 0; //相等
}
public static void main(String [] args){
      test(10,5);
      System.out.println(result);
      test(5,10);
      System.out.println(result);
      test(5,5);
      System.out.println(result);
}
}
输出
1
-1
0
上例中的else if的意思是在else后紧跟一个if语句。
迭代语句
Java流程控制中while,do-while,for用来控制循环,有时将他们划分为迭代语句(iterationstatement)。用大括号扩住需要重复执行的语句，被扩住的语句会一直重复执行，直到起控制作用的布尔表达式（Boolean-expression）得到False的结果为止。
三、while
while的循环格式如下
       
while(Boolean-expression)
             statement;
       
       循环开始时，会判断一遍布尔表达式的值，如果为True执行语句，迭代后会从新判断布尔值，直到为false，循环结束。
       
下面的例子是判断生成的随机数是否符合条件
public class WhileTest{
static Boolean condition(){
      Boolean result=Math.random() &lt; 0.99;
      System.out.print(result+”.”);
       returnresult;
}
public static void main(String [] args){
      while(condition())
             System.out.print(“循环中 ‘while”);
      System.out.print(“退出 ‘’while”);
}
}
}
condition()方法里面用到了Math库里面的static(静态)方法random(),该方法的作用是产生0和1之间（包括0,但不包括1）的一个double值。result的值是通过比较操作符&lt;而得到的，这个操作符将产生boolean类型的结果
四、do-while
do-while的格式如下:
       do
             statement
      while(Boolean-expression);
       
       区别于while循环，do-while循环至少会循环一次，比如循环开始时布尔值就为false,while循环会直接停止循环，而do-while会循环一次。
for
for循环是三种循环中最常用到的迭代形式,这种循环在第一次迭代之前要进行初始化。随后，它会进行条件测试，而且在每一次迭代结束时，进行某种形式的”进步”。for循环的格式如下
for(initialization;Boolean-expression;step)
      statement
初始化(initialization)表达式,布尔表达式(Boolean-expression)，或者进步(step)运算都可以为空。但是表达式后面的”；”不可以不写。每次迭代前会测试布尔表达式。若获得的结果是true,就会执行for语句后面的代码行。每次循环结束，会执行一次进步
for循环常用于执行”计数”任务:
public class ListCharacters{
       publicstatic void main(String [] args){
             for(char c=0;c&lt;128;c++)
                    if(Character.isLowerCase(c))
                           System.out.println(“value: ”+(int)c+” character: ” + c);
}
}
输出:
value: 97 character : a
value: 98 character : b
switch
switch可被归纳为一种选择语句。根据正式表达式的值（也可以被&lt;int型的变量代替），根据表达式的值，swtich语句可以从一系列代码选出一段去执行。
switch的语法如下
switch(integral-selector){
       caseintegrak-value1 : statement;break;
       caseintegrak-value2 : statement;break;
       caseintegrak-value3 : statement;break;
       caseintegrak-value4 : statement;break;
       //….
      default:statement;
}
      integral-selector选择因子是一个能够产生整数值的表达式,switch能将这个表达式的结果与每个integral-value相比较，若发现相同的，就会执行该case后的语句，若没有相符的则会执行default后面的语句。
       其中每一个case后面均有一个break结尾，这会让语句跳转至switch的末尾，这里break是可选的,如果不写break语句会继续执行接下来的case,直到遇到break位置。最后的default后面没有加break因为这里也是switch的末尾，也可以跟default后面加break，不过没有任何意义。
public class VowelsAndConsonants{
       publicstatic void main(String [] args){
             Random rand =new Random(47);
             for(int i=0; i &lt; 100;i++){
                    int c=rand.nextInt(26)+ ‘a’;
                    System.out.print((char)c+ “,” + c + “:”);
                    switch(c){
                          case ‘a’:
                           case ‘e’:
                           case ‘i’:
                           case ‘o’:
                           case ‘u’:System.out.print(“vowel”);break;
                           case ‘y’:
                           case ‘w’:System.out.print(“Sometimes a vowel”);break;
                           default: System.out.print(“consonant”);
                    }
}
}
}
输出
y,121: Sometimes a vowel
这里a为偏移量，因为随机数会产生0-26任意数字，加上a后就会偏移值小写字母中
五、foreach
foreach语法可以不必创建int变量去对由访问项构成的序列进行计数，foreach将自动遍历获得每一项
       
foreach语法如下
for(变量类型 x : 该变量的序列){
      statement
}
用foreach遍历float数组
public class ForEachFloat{
       publicstatic void main(String [] args){
             Random rand = new Random(47);
             float f[] = new float[10];
             for(int i=0;i &lt; 10;i++){
                    f[i]=rand.nextFloat();
}
for(float x : f)
      System.out.println(x);
}
}
输出
0.32454357.
. //共10行
如上例所示
for(float x : f){
       这条语句定义了一个名为x的float型变量,而后将每一个值遍历到x  
}
任何数组或返回值是数组的方法都可以应用于foreach。
public class ForEachString{
      publicn static void main(String [] args){
             for(char c : “Hello Word”.toCharArray()){
                    System.out.print(c+” ”);
}
}
}
输出
H e l l o  W o r d
但foreach不能直接对不存在的一组逻辑数字进行遍历，但for却可以
例如 for(int i=0;i &lt; 100;i++)
如果想直接对数字进行遍历需要创建int型的数组，但可以做个静态方法自动根据传入数字来建立数组
       
例如for(int i : range(10)) 
这种方法虽然可以使得foreach更通用,但是这种方法会降低计算机效率。
六、return
Java中有多个关键词表示无条件分支，他们只是表示这个分支无需任何测试即可发生。这些关键词包括return,break,continue
       
       return关键词有两个作用,一是指定一个方法返回什么值(void方法除外),二是它会导致当前的方法退出，并返回值。可使用return关键词改写if-else中的例子
       publicclass Return{
       staticint test(int  testval,int  target){
             if(testvlal&gt;target)
                    return 1;
             else if(testval&lt;target)
                    return -1;
             else
                    return 0; //相等
}
public static void main(String [] args){
       
      System.out.println(test(10,5););
      System.out.println(test(5,10));
      System.out.println(test(5,5));
}
}
输出
1
-1
0
也可以省略掉else因为return后方法不再执行
       如果在返回void的方法中没有return语句，那么在该方法的结尾处会有一个隐藏的return,因此在方法中并非总是必须有一个return语句。但是，如果一个方法声明它将返回void之外的其他东西，那么必须确保每一条代码路径都将返回一个值。
break和continue
任何迭代语句的主体部分，都可以用break和continue来控制循环的流程.其中，break可以使程序强行退出，不执行循环中剩余的语句。而continue则会停止指定当前的循环，然后退回到循环的起始，开始下一次循环。
public class BreakContnue{
       publicstatic void main(String [] args){
             for(int i = 0;i &lt; 100;i++){
                    if(i == 74) break;
                    if(i % 9 !=0) continue;
                    System.out.print(i+” “);
}
}
System.out.println();
int i = 0;
while(true){
       i++;
       int j= i * 27;
       if(j== 1269) break;
       if(i%10 !=0) continue;
      System.out.print(i+” ”);
}
}
输出
0 9 18 27 36 45 54 63 72
10 20 30 40
在上例的for循环中i的值达到74后就会触发break;结束循环因此i的值永远没法达到100,而每当i不能被9整除时便会触发continue;因为会跳过之后的代码，即输出部分，直接进入下一次循环(i的值增加)，只有能被9整除且小于74的数才会被输出。
后面的便是无限while循环因为括号内条件永远为true，但是循环中有一个break；可以控制循环结束。
还有一种无限循环的形式是for(;;)。编译器将while(true)和for(;;)看做是同一回事。所以用哪个取决于自己的习惯。
标签
在java中标签作用的地方时在迭代语句之前。在标签和迭代之前置入任何语句都是不好的。而在迭代之前设置标签的唯一理由是：我们希望潜逃另一个迭代或者一个开关。这是由于break和continue关键词通常只中断当前循环，但若随标签一起使用，他们就会中断循环，知道标签所在的地方:
标签的语法
label1:
label1:
outer-iteration{
      inner-iteration{
       //….
      break;//(1)
       //…..
      continue;//(2)
       //…..
       breaklabel1;//(3)
       //…..
      continue label1;//(4)
       在(1)中break会中断内层循环回到外部循环，在(2)中continue回结束内层循环的本次循环回到内部循环的起始点。在(3)中，break lable1中因为break后面加了label1所以会直接中断所有循环回到标签处，但不重新进入循环，也就是说直接终止了两个循环，而(4)中continue label1则会直接中断内部与外部循环回到标签处,随后继续循环，但是从外部循环开始。
public class labeledFor{
       publicstatic void main(String [] args){
             int i = 0;
             outer:
             for(;;){
                    inner:
                    for(;i&lt;10;i++){
             System.out.print(“i=”+i);
             if(i == 2){
                    System.out.print(“continue”);
                    continue;
             }
             if(i == 3){
                    System.out.print(“break”);
i++;
                    break;
             }
             if(i == 7){
                    System.out.print(“continue outer”);
                    i++;
continue outer;
             }
             if(i == 8){
                    System.out.print(“breaj outer”);
                    break outer;
             }
             for(int k=0;k &lt; 5;k++){
                    if(k==3){
                           System.out.print(“continue inner”);
                           continue inner;
}
} 
}
}
}
}
输出
i=0
continue inner
i=1
continue inner
i=2
continue
i=3
break
i=4
continue inner
i=5
continue inner
i=6
continue inner
i=7
continue outer
i=8
七、break outer
break会中断for循环，而且在抵达for循环末尾时，递增表达式不会执行，由于break跳过了递增表达，所以在i==3的情况下直接对i执行递增运算。在i==7的情况下,continue outer语句也会跳到循环顶部，而且也跳过递增。所以这里也对i进行了直接递增
如果没有break outer,就无法直接同时终止2个循环，如果想在终止循环的同时退出，可以使用return关键字。
 


         面向对象的三个基本特征是：封装、继承、多态。其中，封装可以隐藏实现细节，使得代码模块化；继承可以扩展已存在的代码模块（类）；它们的目的都是为了——代码重用。而多态则是为了实现另一个目的——接口重用！
                                               

封装：
        封装可以隐藏实现细节，使得代码模块化；封装是把过程和数据包围起来，对数据的访问只能通过已定义的界面。面向对象计算始于这个基本概念，即现实世界可以被描绘成一系列完全自治、封装的对象，这些对象通过一个受保护的接口访问其他对象。在面向对象编程上可理解为：把客观事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏。
继承：
        继承是指这样一种能力：它可以使用现有类的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。其继承的过程，就是从一般到特殊的过程。
通过继承创建的新类称为“子类”或“派生类”。被继承的类称为“基类”、“父类”或“超类”。要实现继承，可以通过“继承”（Inheritance）和“组合”（Composition）来实现。在某些 OOP 语言中，一个子类可以继承多个基类。但是一般情况下，一个子类只能有一个基类，要实现多重继承，可以通过多级继承来实现。
        继承概念的实现方式有三类：实现继承、接口继承和可视继承。
1. 实现继承是指使用基类的属性和方法而无需额外编码的能力；
2. 接口继承是指仅使用属性和方法的名称、但是子类必须提供实现的能力；
3. 可视继承是指子窗体（类）使用基窗体（类）的外观和实现代码的能力。
多态：
       多态性（polymorphisn）是允许你将父对象设置成为和一个或更多的他的子对象相等的技术，赋值之后，父对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。简单的说，就是一句话：允许将子类类型的指针赋值给父类类型的指针。
实现多态，有二种方式，覆盖，重载。覆盖：是指子类重新定义父类的虚函数的做法。重载：是指允许存在多个同名函数，而这些函数的参数表不同（或许参数个数不同，或许参数类型不同，或许两者都不同）。
分析：
“重载”是指在同一个类中相同的返回类型和方法名，但是参数的个数和类型可以不同
“覆盖\重写”是在不同的类中。
        其实，重载的概念并不属于“面向对象编程”，重载的实现是：编译器根据函数不同的参数表，对同名函数的名称做修饰，然后这些同名函数就成了不同的函数（至少对于编译器来说是这样的）。如，有两个同名函数：functionfunc(p:integer):integer;和functionfunc(p:string):integer;。那么编译器做过修饰后的函数名称可能是这样的：int_func、str_func。对于这两个函数的调用，在编译器间就已经确定了，是静态的（记住：是静态）。也就是说，它们的地址在编译期就绑定了（早绑定），因此，重载和多态无关！真正和多态相关的是“覆盖”。当子类重新定义了父类的虚函数后，父类指针根据赋给它的不同的子类指针，动态（记住：是动态！）的调用属于子类的该函数，这样的函数调用在编译期间是无法确定的（调用的子类的虚函数的地址无法给出）。因此，这样的函数地址是在运行期绑定的（晚邦定）。结论就是：重载只是一种语言特性，与多态无关，与面向对象也无关！引用一句Bruce
 Eckel的话：“不要犯傻，如果它不是晚邦定，它就不是多态。”
C++多态机制的实现：
       面向对象有了一个重要的概念就是对象的实例，对象的实例代表一个具体的对象，故其肯定有一个数据结构保存这实例的数据，这一数据包括对象成员变量，如果对象有虚函数方法或存在虚继承的话，则还有相应的虚函数或虚表指针，其他函数指针不包括。虚函数在c++中的实现机制就是用虚表和虚指针，但是具体是怎样的呢？从more effecive c++其中一篇文章里面可以知道：是每个类用了一个虚表，每个类的对象用了一个虚指针。要讲虚函数机制，必须讲继承，因为只有继承才有虚函数的动态绑定功能，先讲下c++继承对象实例内存分配基础知识：
从more effecive c++其中一篇文章里面可以知道：是每个类用了一个虚表，每个类的对象用了一个虚指针。具体的用法如下：
class A
{public:
    virtual void f();
    virtual void g();
private:
    int a
};

class B : public A
{
public:
    void g();
private:
    int b;
};
//A，B的实现省略
因为A有virtual void f（），和g（），所以编译器为A类准备了一个虚表vtableA，内容如下：




 A::f 的地址




A::g 的地址




B因为继承了A，所以编译器也为B准备了一个虚表vtableB，内容如下：




A::f 的地址




B::g 的地址




注意：因为B::ｇ是重写了的，所以B的虚表的g放的是B::g的入口地址，但是f是从上面的A继承下来的，所以f的地址是A::f的入口地址。然后某处有语句 B bB;的时候，编译器分配空间时，除了A的int a，B的成员int b；以外，还分配了一个虚指针vptr，指向B的虚表vtableB，bB的布局如下：




vptr ： 指向B的虚表vtableB




int a： 继承A的成员




int b： B成员




（1）      当如下语句的时候：
A *pa = &amp;bB;
   pa的结构就是A的布局（就是说用pa只能访问的到bB对象的前两项，访问不到第三项int b）
   那么pa-&gt;g()中，编译器知道的是，g是一个声明为virtual的成员函数，而且其入口地址放在表格（无论是vtalbeA表还是vtalbeB表）的第2项，那么编译器编译这条语句的时候就如是转换：call *(pa-&gt;vptr)[1]（C语言的数组索引从0开始哈~）。
这一项放的是B：：g()的入口地址，则就实现了多态。（注意bB的vptr指向的是B的虚表vtableB）
   另外要注意的是，如上的实现并不是唯一的，C++标准只要求用这种机制实现多态，至于虚指针vptr到底放在一个对象布局的哪里，标准没有要求，每个编译器自己决定。我以上的结果是根据g++ 4.3.4经过反汇编分析出来的。
2、两种多态实现机制及其优缺点
       除了c++的这种多态的实现机制之外，还有另外一种实现机制，也是查表，不过是按名称查表，是smalltalk等语言的实现机制。这两种方法的优缺点如下：
 （1）、按照绝对位置查表，这种方法由于编译阶段已经做好了索引和表项(如上面的call *(pa-&gt;vptr[1]） )，所以运行速度比较快;缺点是：当A的virtual成员比较多（比如1000个），而B重写的成员比较少（比如2个），这种时候，B的vtableＢ的剩下的998个表项都是放Ａ中的virtual成员函数的指针，如果这个派生体系比较大的时候，就浪费了很多的空间。
比如：GUI库，以MFC库为例，MFC有很多类，都是一个继承体系；而且很多时候每个类只是1、2个成员函数需要在派生类重写，如果用Ｃ＋＋的虚函数机制，每个类有一个虚表，每个表里面有大量的重复，就会造成空间利用率不高。于是ＭＦＣ的消息映射机制不用虚函数，而用第二种方法来实现多态，那就是：
（２）、按照函数名称查表，这种方案可以避免如上的问题；但是由于要比较名称，有时候要遍历所有的继承结构，时间效率性能不是很高。（关于MFC的消息映射的实现，看下一篇文章）
３、总结：
     如果继承体系的基类的virtual成员不多，而且在派生类要重写的部分占了其中的大多数时候，用C++的虚函数机制是比较好的；但是如果继承体系的基类的virtual成员很多，或者是继承体系比较庞大的时候，而且派生类中需要重写的部分比较少，那就用名称查找表，这样效率会高一些，很多的GUI库都是这样的，比如MFC，QT，PS 其实，自从计算机出现之后，时间和空间就成了永恒的主题，因为两者在98%的情况下都无法协调，此长彼消；这个就是计算机科学中的根本瓶颈之所在。软件科学和算法的发展，就看能不能突破这对时空权衡了。

     对于C++继承访问权限控制这一块，日常开发中，很容易出现混淆，今天想要借这篇文章做一些总结，希望对大家有帮助，C++通过public,protected,private三个关键字来实现类成员（包括成员变量和成员方法）控制访问权限,一般访问权限的大小顺序为：public
 &gt; protected &gt; private &gt; no.
1、公有继承（public）

① 基类的public和protected成员的访问属性在派生类中保持不变，但基类的private成员不可访问。

② 派生类中的成员函数可以直接访问基类中的public和protected成员，但不能访问基类的private成员。

③ 通过派生类的对象只能访问基类的public成员。

2、私有继承（private）

① 基类的public和protected成员都以private身份出现在派生类中，但基类的private成员不可访问。

② 派生类中的成员函数可以直接访问基类中的public和protected成员，但不能访问基类的private成员。

③ 通过派生类的对象不能访问基类中的任何成员。

3、保护继承（protected）

① 基类的public和protected成员都以protected身份出现在派生类中，但基类的private成员不可访问。

② 派生类中的成员函数可以直接访问基类中的public和protected成员，但不能访问基类的private成员。

③ 通过派生类的对象不能访问基类中的任何成员。

protected成员的特点与作用：

        对建立其所在类对象的模块来说（水平访问时），它与private成员性质相同。对于其派上类来说（垂直访问时），它与public成员的性质相同。既实现了数据隐藏，又方便继承，实现了代码重用。

       以上的总结可对照下图1加深理解：




                                                   图1.1  

为方便记忆，可简单总结为：公开不变，保护变保，私继变私，继私则无！




继承是C++的重要属性：
        在C++中有：
        公有继承(public)、私有继承(private)、保护继承(protected)是常用的三种继承方式。
1. 公有继承(public)
公有继承的特点是基类的公有成员和保护成员作为派生类的成员时，它们都保持原有的状态，而基类的私有成员仍然是私有的，不能被这个派生类的子类所访问。
2. 私有继承(private)
私有继承的特点是基类的公有成员和保护成员都作为派生类的私有成员，并且不能被这个派生类的子类所访问。
3. 保护继承(protected)
保护继承的特点是基类的所有公有成员和保护成员都成为派生类的保护成员，并且只能被它的派生类成员函数或友元访问，基类的私有成员仍然是私有的。
下面列出三种不同的继承方式的基类特性和派生类特性。




 


public


protected


private




共有继承


public


protected


不可见




私有继承


private


private


不可见




保护继承


protected


protected


不可见




在上图中：
1）基类成员对派生类都是：共有和保护的成员是可见的，私有的的成员是不可见的。
         2）基类成员对派生类的对象来说：要看基类的成员在派生类中变成了什么类型的成员。如：私有继承时，基类的共有成员和私有成员都变成了派生类中的私有成员，因此对于派生类中的对象来说基类的共有成员和私有成员就是不可见的。
     为了进一步理解三种不同的继承方式在其成员的可见性方面的区别，下面从三种不同角度进行讨论。
对于公有继承方式
(1) 基类成员对其对象的可见性：
公有成员可见，其他不可见。这里保护成员同于私有成员。
(2) 基类成员对派生类的可见性：
公有成员和保护成员可见，而私有成员不可见。这里保护成员同于公有成员。
(3) 基类成员对派生类对象的可见性：
公有成员可见，其他成员不可见。
所以，在公有继承时，派生类的对象可以访问基类中的公有成员；派生类的成员函数可以访问基类中的公有成员和保护成员。这里，一定要区分清楚派生类的对象和派生类中的成员函数对基类的访问是不同的。
对于私有继承方式
(1) 基类成员对其对象的可见性：
公有成员可见，其他成员不可见。
(2) 基类成员对派生类的可见性：
公有成员和保护成员是可见的，而私有成员是不可见的。
(3) 基类成员对派生类对象的可见性：
所有成员都是不可见的。
所以，在私有继承时，基类的成员只能由直接派生类访问，而无法再往下继承。
对于保护继承方式
这种继承方式与私有继承方式的情况相同。两者的区别仅在于对派生类的成员而言，对基类成员有不同的可见性。
上述所说的可见性也就是可访问性。
关于可访问性还有另的一种说法。这种规则中，称派生类的对象对基类访问为水平访问，称派生类的派生类对基类的访问为垂直访问。
一个私有的或保护的派生类不是子类，因为非公共的派生类不能做基类能做的所有的事，就是指在公开场合，但是在类内部可以的
一、引言
在C++中，类是提供封装的逻辑单位，类的每一个对象都包含有描述其自身状态的数据集合，并且通过接收特定的消息来处理这个数据集合。如果程序设计人员能够通过增加、修改或替换指定类的部分内容的方法对该类进行剪裁，就可以适应不同的应用，从而在很大程度上增强了数据封装的价值，而接下来要讨论的继承就完全可以实现这种操作。
二、与继承有关的基本概念
继承是一个进程，通过继承，一个对象可以获得另一个对象的属性（包括函数），并可向其中加入属于自己的一些特征。作为C++语言的一种重要机制，用继承的方法可以自动为一个类提供来自另一个类的操作和数据结构，进而使程序设计人员在一个一般的类的基础上很快建立一个新的类，而不必从零开始设计每个类。
当一个类被其他的类继承时，被继承的类称为基类（可不是鸡肋^_^），又称为父类。
继承其他类属性的类称为派生类，又称为子类。
一般情况下，继承的进程起源于一个基类的定义，基类定义了其所有派生类的公有属性。从本质上讲，基类具有同一类集合中的公共属性，派生类继承了这些属性，并且增加了自己特有的属性。从任何已存在的类继承的实质就是建造新的派生类。
三、单重继承、多重继承与继承链
从一个基类派生的继承称为单继承，换句话说，派生类只有一个直接基类。单继承声明语句的常用格式为：




class 派生类名: 访问控制关键字 基类名
{
  数据成员和成员函数声明
};




与此相对地，从多个基类派生的继承称为多继承或多重继承，也就是说，一个派生类有多个直接基类。在某些面向对象的语言（如Java)中不支持类间的多重继承而只支持单重继承，即一个类至多只能有一个直接父类，因此实现类似的功能需要借助接口等其他机制。而在C++中提供了多重继承的语法支持，使得问题变得简单了许多。多重继承声明语句的常用格式为：




class 派生类名: 访问控制关键字 基类名1, 访问控制关键字 基类名2，...
{
  数据成员和成员函数声明
};




除了多重继承之外，一个派生类继承多个基类还有一种方法，就是把派生类作为基类再次供别的类继承，产生多层次的继承关系。例如类A派生类B，类B派生类C，则称类A是类B的直接基类，类B是类C的直接基类，类A是类C的间接基类。类的层次结构也叫做继承链。还是上面的例子，当建立类C的对象时，类A 的构造函数最先被调用，接下来被调用的是类B的构造函数，最后是类C的构造函数。析构函数的调用顺序正好相反。当一个派生类继承有层次的类时，继承链上的每个派生类必须将它需要的变量传递给它的基类。
四、公有派生和私有派生
在继承声明语句中，访问控制关键字用于说明在基类定义中所声明的成员和成员函数能够在多大范围内被派生类所访问。访问控制关键字可为public, private或protected。如果访问控制关键字为public，则称派生类从基类公有继承，也称公有派生。如果访问控制关键字为 private，则称派生类从基类私有继承，也称私有派生。现在笔者将公有继承和私有继承的具体区别列表如下。
通过上表，我们可以将两种派生的特点总结如下：




基类成员


基类private成员


基类public成员




派生方式


private


public


private


public




派生类成员


不可见


不可见


可见


可见




外部函数


不可见


不可见


不可见


可见




（1）无论哪种派生方式，基类中的private成员在派生类中都是不可见的。也就是说，基类中的private成员不允许外部函数或派生类中的任何成员访问。
（2）public派生与private派生的不同点在于基类中的public成员在派生类中的访问属性：
public派生时，基类中的public成员相当于派生类中的public成员。
private派生时，基类中的public成员相当于派生类中的private成员。
因此，private派生确保基类中的方法只能被派生类的对象的方法间接使用，而不能被外部使用。public派生使派生类对象与外部都可以直接使用基类中的方法，除非这些方法已经被重新定义。
五、保护成员与保护派生
如果想做到基类成员只由有派生血缘关系的成员访问，而不被无血缘关系的对象成员访问，无论用公有派生还是私有派生都无法做到。因为基类成员中的私有成员是别的类（包括派生类）成员不能访问的，而基类中的公有成员在public派生时，不仅可以由派生类对象成员访问，也可以由外部函数访问；而在 private派生时，基类中的公有成员虽然允许派生类对象中的成员访问，不允许外部访问，可是再派生出下一级时，由于基类的所有成员已经被私有化，其它类成员也不可再访问。实现只许有派生血缘关系的对象成员访问的方法，是在基类中使用具有另一种访问属性的成员——protected成员。
protected成员是一种血缘关系内外有别的成员。它对派生对象而言，是公有成员，可以访问；对血缘关系外部而言，与私有成员一样被隐藏。
此外，除了允许使用private与public两种派生方式之外，C++还允许使用protected派生方式。现在将三种访问属性不同的成员经三种派生后在派生类中访问属性的变化情况总结如下表，是对上一表格的增进和补充。




派生方式


基类的public成员


基类的protected成员


基类的private成员


派生方式引起的访问属性变化概括




private派生


变为private成员


变为private成员


不可见


基类中的非私有成员都成为派生类中的私有成员




protected派生


变为protected成员


变为private成员


不可见


基类中的非私有成员在派生类中的访问属性都降一级




public派生


仍为public成员


仍为protected成员


不可见


基类中的非私有成员在派生类中的访问属性保持不变




需要注意的是，基类的private成员无论经过何种派生，在派生类中都是不可见的。
六、友元类和友元函数
（1）友元函数
通常，类的私有成员只能由本类的成员访问，外部函数只能访问类的成员函数，再由成员函数访问类的私有成员。但是，如果在某个类定义中用friend 声明了一个外部函数（也许是其他类的一个成员）后，这个外部函数便可以例外地访问该类的任何私有成员。用friend声明了的外部函数称为这个类的友元函数。
当友元函数是另一个类的成员函数时，应当注意以下几点：
A：友元函数作为一个类的成员函数时，除应当在它所在的类定义中声明之外，还应当在另一个类中声明它的友元关系，声明语句的格式为：
friend 函数类型 函数所在类名：：函数名（参数列表）；
B：友元函数在引用本类对象的私有成员时无需本类对象的引用参数，但在引用生命它是友元的类的对象中的私有成员时必须有友元类对象的引用参数。
C：一个类的成员函数作另一个类的友元函数时，必须先定义，而不是仅仅声明它。
使用友元函数直接访问对象的私有成员，可以免去再调用类的成员函数所需的开销。同时，友元函数作为类的一个接口，对已经设计好的类，只要增加一条声明语句，便可以使用外部函数来补充它的功能，或架起不同类对象之间联系的桥梁。然而，它同时也破坏了对象封装与信息隐藏，使用时需要谨慎小心。
（2）友元类
也可以把一个类而不仅仅是一个函数声明为另一个类的友元类。这时，只需先声明它而不一定需要先定义。
应当注意，友元关系是单向的，并且只在两个类之间有效。即使类X是类Y的友元，类Y是否是类X的友元也要看类X中是否有相应的声明。即友元关系不具有交换性。若类X是类Y的友元，类Y是类Z的友元，也不一定就说明类X是类Z的友元，即友元关系也不具有传递性。
当一个类要和另一个类协同工作时，使一个类成为另一个类的友元类是很有用的。这时友元类中的每一个成员函数都成为了对方的友元函数。
     
c++继承经典例子
#include &lt;iostream.h&gt;
class Base
{
private:
        int b_number;
public:
        Base( ){}
        Base(int i) : b_number (i) { }
        int get_number( ) {return b_number;}
        void print( ) {cout &lt;&lt; b_number &lt;&lt; endl;}        
};
 
class Derived : public Base
{
private:
        int d_number;
public:
// constructor, initializer used to initialize the base part of a Derived object.
        Derived( int i, int j ) : Base(i), d_number(j) { };        
        // a new member function that overrides the print( ) function in Base
        void print( ) 
        {
                cout &lt;&lt; get_number( ) &lt;&lt; " ";        
                // access number through get_number( )
                cout &lt;&lt; d_number &lt;&lt; endl;
        }
};
int main( )
{
        Base a(2);
        Derived b(3, 4);
        cout &lt;&lt; "a is ";
        a.print( );                // print( ) in Base
        cout &lt;&lt; "b is ";
        b.print( );                // print( ) in Derived
        cout &lt;&lt; "base part of b is "; 
        b.Base::print( );                // print( ) in Base
        return 0;
}
 


 没有虚析构函数，继承类没有析构
//Example:  non- virtual destructors for dynamically allocated objects. 
#include &lt;iostream.h&gt;
#include &lt;string.h&gt;
class Thing
{ public:
virtual void what_Am_I( ) {cout &lt;&lt; "I am a Thing./n";}
~Thing(){cout&lt;&lt;"Thing destructor"&lt;&lt;endl;}
};
class Animal : public Thing
{  
public:
virtual void what_Am_I( ) {cout &lt;&lt; "I am an Animal./n";}
~Animal(){cout&lt;&lt;"Animal destructor"&lt;&lt;endl;}
};
void main( )
{
   Thing *t =new Thing;      
   Animal*x = new Animal;
   Thing* array[2];
   array[0] = t;                                // base pointer
   array[1] = x;               
    for (int i=0; i&lt;2; i++)  array-&gt;what_Am_I( ) ;
   delete array[0];
   delete array[1];
   return ;
}
 


纯虚函数，多态#include &lt;iostream.h&gt;
#include &lt;math.h&gt;
class Point
{
private:
        double x;
        double y;
public:
        Point(double i, double j) : x(i), y(j) { } 
        void print( ) const
        { cout &lt;&lt; "(" &lt;&lt; x &lt;&lt; ", " &lt;&lt; y &lt;&lt; ")"; }
};
class Figure
{
private:
        Point center;
public:
        Figure (double i = 0, double j = 0) : center(i, j) { }         
        
Point&amp; location( )
{
return center;
}                  // return an lvalue
   void move(Point p)
{
center = p;
draw( );
}
        virtual void draw( ) = 0; // draw the figure
        virtual void rotate(double) = 0; 
// rotate the figure by an angle                
};
class Circle : public Figure
{
private:
        double radius;
public:
        Circle(double i = 0, double j = 0, double r = 0) : Figure(i, j), radius(r) { }
        void draw( )
        {
                cout &lt;&lt; "A circle with center ";
                location( ).print( );
                cout &lt;&lt; " and radius " &lt;&lt; radius &lt;&lt; endl;
        }
        void rotate(double)
        {
                cout &lt;&lt; "no effect./n";
        }        // must be defined
};
class Square : public Figure
{
private:
        double side;        // length of the side
        double angle;        // the angle between a side and the x-axis
public:
        Square(double i = 0, double j = 0, double d = 0, double a = 0)        : Figure(i, j), side(d), angle(a) { }
   void draw( )
        {
                cout &lt;&lt; "A square with center ";
                location( ).print( );
                cout &lt;&lt; " side length " &lt;&lt; side &lt;&lt; "./n"  
                &lt;&lt; "The angle between one side and the X-axis is " &lt;&lt; angle &lt;&lt; endl;
        }
        void rotate(double a)
        {
               angle += a;
                cout &lt;&lt; "The angle between one side and the X-axis is " &lt;&lt; angle &lt;&lt; endl;
        }
        void vertices( )
        {
                cout &lt;&lt; "The vertices of the square are:/n";
                // calculate coordinates of the vertices of the square
          }
};
int main( )
{
        Circle c(1, 2, 3);
        Square s(4, 5, 6);
   Figure *f = &amp;c, &amp;g = s;
        f -&gt; draw( );
        f -&gt; move(Point(2, 2));
        g.draw( );
          g.rotate(1);
        
s.vertices( );
// Cannot use g here since vertices( ) is not a member of Figure.
        return 0;
}
////////////////////////////////////////////////////////////////////
#include &lt;iostream.h&gt;
#include &lt;string.h&gt;
class Thing
{ 
public:
virtual void what_Am_I( ) {cout &lt;&lt; "I am a Thing./n";}
~Thing(){cout&lt;&lt;"Thing destructor"&lt;&lt;endl;}
};
class Animal : public Thing
{ 
public:
virtual void what_Am_I( ) {cout &lt;&lt; "I am an Animal./n";}
~Animal(){cout&lt;&lt;"Animal destructor"&lt;&lt;endl;}
};
void main( )
{
   Thing t ; 
        Animal x ;
   Thing* array[2];
   array[0] = &amp;t;                        // base pointer
   array[1] = &amp;x;        
          for (int i=0; i&lt;2; i++)  array-&gt;what_Am_I( ) ;
   return ;
}
 

多继承
#include &lt;iostream.h&gt;
class A
{
private:
        int a;
public:
        A(int i) : a(i) { }
        virtual void print( )        {cout &lt;&lt; a &lt;&lt; endl;}
        int get_a( ) {return a;}
};
class B
{
private:
        int b;
public:
        B(int j) : b(j) { }
        void print( )        {cout &lt;&lt; b &lt;&lt; endl;}
        int get_b( ) {return b;}
};
class C : public A, public B
{
        int c;
public:
        C(int i, int j, int k) : A(i), B(j), c(k) { }
        void print( )        {A::print( ); B::print( );}
        // use print( ) with scope resolution
        void get_ab( )        {cout &lt;&lt; get_a( ) &lt;&lt; " " &lt;&lt; get_b( ) &lt;&lt; endl;}
        // use get_a( ) and get_b( ) without scope resolution
};
int main( )
{
        C x(5, 8, 10);
        A* ap = &amp;x;
        B* bp = &amp;x;
        ap -&gt; print( );                // use C::print( );
        bp -&gt; print( );                // use B::print( );
//        bp -&gt; A::print( );                // as if x is inherited from B only,
                                                // cannot access A::print( );
        x.A::print( );                // use A::print( );
        x.get_ab( );
        return 0;
}
 

共同基类的多继承
#include &lt;iostream.h&gt;
class R
{int r;
public:
        R(int anInt){ r = anInt;};
       printOn(){ cout&lt;&lt;"r="&lt;&lt;r&lt;&lt;endl;} ; };
class A : public R
{
int a;
public:
        A(int int1,int int2):R(int2){ a = int1;};};
class B : public R
{
int b;
public:
        B(int int1,int int2):R(int2){ b = int1;};};
class C : public A, public B
{
int c;
public:
C(int int1,int int2, int int3):A(int2,int3), B(int2,int3){ c = int1;}
};

int main( )
{    
  int i;
        R rr(10);      
A aa(20,30);      
B bb (40,50);
        C cc(5, 7, 9);
        rr.printOn();    
aa.printOn();                  //inherits R printOn  
bb.printOn();                   //inherits R printOn
        //cc.printOn();                  //would give error
        return 0;}
 


虚基类 
#include &lt;iostream.h&gt;
class R
{ int r;
public:
        R (int x = 0) : r(x) { }   // constructor in R
        void f( ){ cout&lt;&lt;"r="&lt;&lt;r&lt;&lt;endl;}     
        void printOn(){cout&lt;&lt;"printOn R="&lt;&lt;r&lt;&lt;endl;}
};
class A : public virtual R
{  int a;
public:
        A (int x, int y) : R(x), a(y)  { } // constructor in A
        void f( ){ cout&lt;&lt;"a="&lt;&lt;a&lt;&lt;endl;R::f();}
};
class B : public virtual R
{int b;
public:
        B(int x, int z) : R(x), b(z) { }// constructor in B
        void f( ){ cout&lt;&lt;"b="&lt;&lt;b&lt;&lt;endl;R::f();}
};
class C : public A, public B
{ int c;
public:
// constructor in C, which constructs an R object first
C(int x, int y, int z, int w) : R(x), A(x, y), B(x, z), c(w) { }
        
void f( ){ cout&lt;&lt;"c="&lt;&lt;c&lt;&lt;endl;A::f(); B::f();}
};
void main()
{  R rr(1000);
   A aa(2222,444);
   B bb(3333,111);
   C cc(1212,345,123,45);
   cc.printOn();     //uses R printOn but only 1 R..no ambiguity
   cc.f();                // shows multiple call of the R::f()
}

////////////////////////////////////////
#include &lt;iostream.h&gt;
class R
{ int r;
public:
        R (int x = 0) : r(x) { }   // constructor in R
        void f( ){ cout&lt;&lt;"r="&lt;&lt;r&lt;&lt;endl;}
};
class A : virtual public R
{ int a ;
protected:
        void fA( ){cout&lt;&lt;"a="&lt;&lt;a&lt;&lt;endl;};
public:
        A (int x, int y) : R(x), a(y)  { } // constructor in A
        void f( ) {fA( ); R::f( );}
};
class B : virtual public R
{  int b;
protected:
        void fB( ){cout&lt;&lt;"b="&lt;&lt;b&lt;&lt;endl;};
public:
        B (int x, int y) : R(x), b(y)  { } // constructor in A
        void f( ) {fB( ); R::f( );}
};

class C : public A, public B
{ int c;
protected:
        void fC( ){ cout&lt;&lt;"c="&lt;&lt;c&lt;&lt;endl;};        
public:
C(int x, int y, int z, int w) : R(x), A(x, y), B(x, z), c(w) { }
void f( )
        {  
                   R::f( );                    // acts on R stuff only
                A::fA( );            //acts on A stuff only
                B::fB( );                   // acts on B stuff only
                fC( );                  // acts on C stuff only
        }
};
void main()
{  R rr(1000);
   A aa(2222,444);
   B bb(3333,111);
   C cc(1212,345,123,45);
   cc.f();
}
 

 
私有继承

// Access levels
#include &lt;iostream.h&gt;
class Base
{
private:
        int priv;
protected:
        int prot;
        int get_priv( ) {return priv;}
public:
        int publ;
        Base( );
        Base(int a, int b, int c) : priv(a), prot(b), publ(c) { }
        int get_prot( ) {return prot;}
        int get_publ( ) {return publ;}
};
class Derived1 : private Base        // private inheritance
{
public:
        Derived1 (int a, int b, int c) : Base(a, b, c) { }
        int get1_priv( ) {return get_priv( );}
        // priv not accessible directly
        int get1_prot( ) {return prot;}
      int get1_publ( ) {return publ;}
};
class Leaf1 : public Derived1
{
public:
        Leaf1(int a, int b, int c) : Derived1(a, b, c) { }
        void print( )
        {
                cout &lt;&lt; "Leaf1 members: " &lt;&lt; get1_priv( ) &lt;&lt; " "
//                        &lt;&lt; get_priv( )        // not accessible
                        &lt;&lt; get1_prot( ) &lt;&lt; " "
//                        &lt;&lt; get_prot( )         // not accessible
//                        &lt;&lt; publ         // not accessible
                        &lt;&lt; get1_publ( ) &lt;&lt; endl;
        }  // data members not accessible.  get_ functions in Base not accessible
};
class Derived2 : protected Base // protected inheritance
{
public:
        Derived2 (int a, int b, int c) : Base(a, b, c) { }
};
class Leaf2 : public Derived2
{
public:
        Leaf2(int a, int b, int c) : Derived2(a, b, c) { }
        void print( )
        {
                cout &lt;&lt; "Leaf2 members: " &lt;&lt; get_priv( ) &lt;&lt; " "
//                        &lt;&lt; priv                 // not accessible
                        &lt;&lt; prot &lt;&lt; " "
                        &lt;&lt; publ &lt;&lt; endl;
        }  // public and protected data members accessible.  get_ functions in Base accessible. 
};
class Derived3 : public Base  // public inheritance
{
public:
        Derived3 (int a, int b, int c) : Base(a, b, c) { }
};
class Leaf3 : public Derived3
{
public:
        Leaf3(int a, int b, int c) : Derived3(a, b, c) { }
        void print( )
        {
                cout &lt;&lt; "Leaf3 members: " &lt;&lt; get_priv( ) &lt;&lt; " "
                        &lt;&lt; prot &lt;&lt; " "
                        &lt;&lt; publ &lt;&lt; endl;
        }  // public and protected data members accessible.  get_ functions in Base accessible
};
int main( )
{
        Derived1 d1(1, 2, 3);
        Derived2 d2(4, 5, 6);
        Derived3 d3(7, 8, 9);
//        cout &lt;&lt; d1.publ;                // not accessible
//        cout &lt;&lt; d1.get_priv( );        // not accessible
//        cout &lt;&lt; d2.publ;                // not accessible
//        cout &lt;&lt; d2.get_priv( );        // not accessible
        cout &lt;&lt; d3.publ;                // OK
        cout &lt;&lt; d3.get_prot( );        // OK
        Leaf1 lf1(1, 2, 3);
        Leaf2 lf2(4, 5, 6);
        Leaf3 lf3(7, 8, 9);
//         cout &lt;&lt; lf1.publ &lt;&lt; endl;                    // not accessible
//         cout &lt;&lt; lf2.publ &lt;&lt; endl;                // not accessible
        cout &lt;&lt; lf3.publ &lt;&lt; endl;                 // OK
        return 0;
}
 

 
多级继承
// Point-Circle-Cylinder
#include &lt;iostream.h&gt;
// THE POINT CLASS
class Point
{
friend ostream &amp; operator&lt;&lt;(ostream &amp;,Point &amp;);
public:
        
//  constructor
        Point (double xval =0, double yval=0 )
        { x=xval; y=yval;};  
protected:       // accessed by derived class
        double  x;
        double  y;
};
ostream &amp; operator &lt;&lt; (ostream &amp; os,
                              Point &amp;  apoint)
{
cout &lt;&lt;" Point:X:Y: "&lt;&lt;apoint.x &lt;&lt; "," 
                      &lt;&lt; apoint.y&lt;&lt; "/n";
  return os;  
}
//The Circle class  inherits from class Point
class Circle : public Point
{
friend ostream &amp; operator&lt;&lt;(ostream &amp;,Circle&amp;);
public:
Circle (double r=0,double xval=0,double yval=0) 
                             :Point(xval,yval), radius(r)
{ 
//radius = r;
}
double area()
{ 
return (3.14159* radius *radius);
}
protected:
  double radius;
};

//note casting circle to point
ostream &amp; operator &lt;&lt;(ostream &amp; os, Circle &amp; aCircle)
{
cout&lt;&lt; "Circle:radius:" &lt;&lt; aCircle.radius;
os&lt;&lt; aCircle.x &lt;&lt; "/n"; 
os&lt;&lt; aCircle.y &lt;&lt; "/n";        
return os;      
}
// THE CYLINDER CLASS
class  Cylinder  : public Circle
{
friend ostream &amp; operator &lt;&lt; (ostream &amp; ,Cylinder &amp;);
public:
Cylinder (double hv=0,double rv=0, 
                      double xv=0,double yv=0 )
                           : Circle( xv,yv,rv)
{
height = hv;
}        
double  area ( );
protected:     // may have derived classes
        double  height;
};
double Cylinder :: area ( )
{ // Note that cylinder area uses Circle area
return  2.0* Circle::area() + 2.0*3.14159* radius*height;
}
ostream &amp; operator &lt;&lt; (ostream &amp; os,
                        Cylinder &amp; acylinder)
{ 
cout &lt;&lt; "cylinder dimensions: ";
  cout &lt;&lt; "x: " &lt;&lt;acylinder.x;
  cout &lt;&lt; "  y: " &lt;&lt;acylinder.y ;
  cout &lt;&lt; "  radius: " &lt;&lt;acylinder.radius ;
  cout &lt;&lt; "  height: " &lt;&lt;acylinder.height 
                        &lt;&lt; endl;
  return os; 
}
int main(void)
{
Point p(2,3);
Circle c(7,6,5);
Cylinder cyl(10,11,12,13);
cout &lt;&lt; p;
cout &lt;&lt; c;
cout &lt;&lt; "area of cirle:" &lt;&lt; c.area() &lt;&lt; endl;
cout&lt;&lt; cyl;
cout&lt;&lt;"area of cylinder:"&lt;&lt; cyl.area()&lt;&lt;endl ;
cout&lt;&lt;"area of cylinder base is "  
                 &lt;&lt; cyl.Circle::area() &lt;&lt; endl;
return 0;
}
 

 
protected 访问控制属性在继承的意义
 
//Example of treating derived class object as base class objects. Point------Circle
#include &lt;iostream.h&gt;
// THE POINT CLASS
class Point
{ 
friend ostream &amp; operator&lt;&lt;(ostream &amp;,Circle&amp;);
public:
Point (double xval =0, double yval=0 ) { x=xval; y=yval;};  
public:
void print()
{
cout &lt;&lt;" Point:X:Y: "&lt;&lt;x &lt;&lt; "," &lt;&lt;y&lt;&lt; "/n";
}
protected:       // accessed by derived class
double  x;    double  y;
};
ostream &amp; operator &lt;&lt; (ostream &amp; os, Point &amp;  apoint)
{
cout &lt;&lt;" Point:X:Y: "&lt;&lt;apoint.x &lt;&lt; ","&lt;&lt; apoint.y&lt;&lt; "/n";
  return os;  
}

//The Circle class  inherits from class Point
class Circle : public Point
{
friend ostream &amp; operator&lt;&lt;(ostream &amp;,Circle&amp;);
public:
Circle (double r=0,double xval=0,double yval=0):Point(xval,yval)
{ radius = r;};
void print()
{
cout&lt;&lt; "Circle:radius:" &lt;&lt;radius&lt;&lt;endl;
cout &lt;&lt;" Point:X:Y: "&lt;&lt;x &lt;&lt; "," &lt;&lt;y&lt;&lt; "/n";
}
double area()
{ return (3.14159* radius *radius);};
protected:
double radius;
};
//note casting circle to point
ostream &amp; operator &lt;&lt;(ostream &amp; os, Circle &amp; aCircle)
{
cout&lt;&lt; "Circle:radius:" &lt;&lt; aCircle.radius;
cout&lt;&lt; (Point) aCircle &lt;&lt; "/n";           
return os;      
}

//We will look at a few main programs based on previous class definitions. Casting and assignments
void main (void )
{
Point p(2,3);         cout &lt;&lt;"Point P=  "&lt;&lt; p;
Point pp(0,0);       cout &lt;&lt;"Point PP=  "&lt;&lt; pp;
Circle c(7,6,5);     cout &lt;&lt;"Circle c=  "&lt;&lt; c;        //radius =7
pp = p;             cout &lt;&lt;"Point PP=  "&lt;&lt; pp;    //built in assign =
// a circle is a member of the point class so assign a circle to a point.
pp = c;           //legal; also assignment O.K.
cout &lt;&lt;"Point PP=  "&lt;&lt; pp;
pp= (Point) c;    // but better  use the cast
cout &lt;&lt;"Point PP=  "&lt;&lt; pp;  //note we get only the point part of the Circle
//c = (Circle) pp;   //  illegal Cannot convert 'class Point' to 'class Circle'
//c=pp;                 //illegal assignment not defined
Point*  p;
p = &amp;c;
P-&gt;print();    //call base class print
((Circle*)p)-&gt;print();
Point&amp; r = c;
r.print();
((Circle&amp;)r).print();
}
 


类的兼容性规则 

#include &lt;iostream.h&gt;
class Base
{ 
public:  
void func( ) 
{cout &lt;&lt; "Base class function./n";} 
};
class Derived : public Base
{ 
public:  
void func( ) 
{cout &lt;&lt; "Derived class function./n";}
};
void foo(Base b)
{ b.func( ); }
int main( )
{
   Derived d;
   Base b;
   Base * p = &amp;d;
   Base&amp; br = d;
   b = d;
   b.func( );
   d.func( );
   p -&gt; func( );
   foo(d);
   br.func( );
   return 0;
}
 
 


 
虚析构函数，防止内存泄露 
 
#include &lt;iostream.h&gt;
#include &lt;string.h&gt;
class Base
{
protected:
        int id;
        char * name;
public:
        // default constructor
        Base(int a = 0, char * s = "") : id(a)
        {
                if (!s) 
{ 
name = NULL; 
}
                else
                {
                        name = new char[strlen(s) + 1];
                        strcpy(name, s);
                }
                cout &lt;&lt; "base default constructor/n";
        }
                // copy constructor
        Base(const Base&amp; b) : id(b.id)
        {
                if (!b.name) { name = NULL; }
                else
                { 
                        name = new char[strlen(b.name) + 1];
        strcpy(name, b.name);
}
                    cout &lt;&lt; "base copy constructor/n";
        } 
        // destructor
      ~Base( ) 
        {
            if( name != NULL )        delete [ ] name; 
                cout &lt;&lt; "base destructor/n";
        }
        const Base&amp; operator= (const Base&amp; b);                
friend ostream&amp; operator &lt;&lt; (ostream&amp;, const Base&amp;);
};
const Base&amp; Base:perator= (const Base&amp; b)
{
        if (this != &amp;b)                        // Check if an object is assigned to itself.
        {
             id = b.id;
                delete [ ] name;                //  Destroy the old object.
                if (!b.name) { name = NULL; }
                else
                {
        name = new char[strlen(b.name) + 1];
        strcpy(name, b.name);
                }
        }
            cout &lt;&lt; "base assignment operator/n";
        return *this;
}
ostream&amp; operator &lt;&lt; (ostream&amp; out, const Base&amp; b)
{
        out &lt;&lt; "Base member id = " &lt;&lt; b.id &lt;&lt; endl;
        out &lt;&lt; "Base member name = " &lt;&lt; b.name &lt;&lt; endl;
        
        return out;
}
class Derived : public Base
{
private:
        float f;
        char * label;
public:
        // default constructor
        Derived(int a = 0, char * s = "", float x = 0, char * t = "") : Base(a, s), f(x)
        {
                if (!t) { label = NULL; }
                else
                {
        label = new char [strlen(t) + 1]; 
        strcpy(label, t);
}
                cout &lt;&lt; "derived default constructor/n";
        }
        // copy constructor
        Derived(const Derived&amp; d) : Base(d), f(d.f)
                // d used as an instance of Base
        {
                if(!d.label) { label = NULL; }
                else
                {
                        label = new char [strlen(d.label) + 1];
        strcpy(label, d.label);
}
                cout &lt;&lt; "derived copy constructor/n";
        }
        // destructor
        ~Derived( )          
        {
                delete [ ] label; 
                cout &lt;&lt; "derived destructor/n";
        }
        const Derived&amp; operator= (const Derived&amp; d);
friend ostream&amp; operator &lt;&lt; (ostream&amp;, const Derived&amp;);
};
const Derived&amp; Derived:perator= (const Derived&amp; d)
{
        if (this != &amp;d)
        {
                delete [ ] label;
                Base:perator=(d);        //  Assign the Base part of d to the Base
// part of the object that calls this operator;
f = d.f;
if (!d.label) { label = NULL; }
else
{
        label = new char [strlen(d.label) + 1];
                        strcpy(label, d.label);
                }
                cout &lt;&lt; "derived assignment operator/n";
        }
        return *this;
}
ostream&amp; operator &lt;&lt; (ostream&amp; out, const Derived&amp; d)
{
        out &lt;&lt; (Base)d;                // Convert d to Base object to output Base members.
        out &lt;&lt; "Derived member f = " &lt;&lt; d.f &lt;&lt; endl;
        out &lt;&lt; "Derived member label = " &lt;&lt; d.label &lt;&lt; endl;
        return out;
}
int main( )
{
        Derived d1;
Derived  d2(d1);
        return 0;
}
 以上示例转自：http://blog.csdn.net/zhaori/article/details/1700356

　C++函数重定义、重载、重写
1. 重写 (override): 
       父类与子类之间的多态性。子类重新定义父类中有相同名称和参数的虚函数。
1) 被重写的函数不能是 static 的。必须是 virtual 的 ( 即函数在最原始的基类中被声明为 virtual ) 。
2) 重写函数必须有相同的类型，名称和参数列表 (即相同的函数原型)
3) 重写函数的访问修饰符可以不同。尽管 virtual 是 private 的，派生类中重写改写为 public,protected 也是可以的
2. 重载 (overload): 
       指函数名相同，但是它的参数表列个数或顺序，类型不同。但是不能靠返回类型来判断。
3. 重定义 (redefining): 
       子类重新定义父类中有相同名称的非虚函数 ( 参数列表可以不同 ) 。
4. 重写与重载的区别 (override) PK (overload)
（1）          方法的重写是子类和父类之间的关系，是垂直关系；方法的重载是同一个类中方法之间的关   系，是水平关系。
（2）          重写要求参数列表相同；重载要求参数列表不同。
（3）          重写关系中，调用那个方法体，是根据对象的类型（对象对应存储空间类型）来决定；重载关系，是根据调用时的实参表与形参表来选择方法体的。 
class Base 
{ 
   private: 
      virtual voiddisplay() { cout&lt;&lt;"Base display()"&lt;&lt;endl; }

      voidsay(){  cout&lt;&lt;"Base say()"&lt;&lt;endl;  } 

   public: 
      void exec(){display(); say(); } 
      voidf1(string a)  { cout&lt;&lt;"Base f1(string)"&lt;&lt;endl; }

      void f1(inta) { cout&lt;&lt;"Base f1(int)"&lt;&lt;endl; }   //overload
}; 
class DeriveA:public Base
{ 
   public: 
      voiddisplay() { cout&lt;&lt;"DeriveA display()"&lt;&lt;endl;}   //override

      void f1(inta,int b) { cout&lt;&lt;"DeriveA f1(int,int)"&lt;&lt;endl;}   //redefining
      void say() {cout&lt;&lt;"DeriveA say()"&lt;&lt;endl; }   //redefining

}; 
class DeriveB:public Base 
{ 
   public: 
      void f1(inta) { cout&lt;&lt;"DeriveB f1(int)"&lt;&lt;endl; }  //redefining

}; 
  int main()
{ 
   DeriveA a; 
   Base *b=&amp;a; 
   b-&gt;exec();       //display():version of DeriveAcall(polymorphism)                          
               //say():version of Base called(allways )

  a.exec();        //same result as laststatement 

   a.say(); 
  //a.f1(1);         //error:no matchingfunction, hidden !!

   DeriveB c; 
  c.f1(1);         //version of DeriveBcalled 

} 
        注意：在 C++ 中若基类中有一个函数名被重载，在子类中重定义该函数，则基类的所有版本将被隐藏——即子类只能用子类定义的，基类的不再可用。
 

一．运算符重载的含义与定义方式
       C++已有的运算符只适合处理C++的基本数据类型。
       C++允许重新定义已有的运算符（运算符重载），以便它能处理程序员定义类型（类类型）。
       运算符重载就是赋予已有的运算符多重含义。运算符重载与函数重载类似，是它的特殊类型。
       C++通过重新定义运算符，使它能够用于特定类的对象执行特定的功能。
       通过对+，-，*，/运算符的重新定义，使它们可以完成复数、分数等不同类的对象的加、减、乘、除运算操作。增强了C++语言的扩充能力。
       先创建一个运算符函数，一般定义成类的成员函数或友元函数。
二．重载一个运算符原则：
1．不能改变运算符的初始意义。
2．不能改变运算符的参数数目。如重载运算符+时只用一个操作数是错误的。
3．运算符函数不能包括缺省的参数。
4．绝大部分C++运算符都可以重载，除以下的例外：
.   ::   .*  ?
5．除赋值运算符外，其它运算符函数都可以由派生类继承。
6．运算符重载不改变运算符的优先级和结合性，也不改变运算符的语法结构，即单目、双目运算符只能重载为单目、双目运算符。
7．运算符的重载实际上是函数的重载。编译程序对运算符重载的选择，遵循函数重载的选择原则。当遇到不很明显的运算符时，编译程序将去寻找参数匹配的运算符函数。
8．运算符重载可使程序更简洁，使表达式更直观，增强可读性。但使用不宜过多。
9．重载运算符含义必须清楚：
如有一个类Time,它有三个数据成员时、分、秒
class Time{
public:
   Time() {hours=minutes=seconds=0;}
   Time(int h,int m,int s){ hours=h; minutes=m; seconds=s;}
private:
int hours, minutes, seconds;
};

Time t1(8,10,20), t2(9,15,30), t3;

t3=t1+t2;
这里加法（+）运算用于类Time的对象，就是含义不清的。所以不能给类Time定义重载运算符+。
三．运算符重载函数的两种形式：
成员函数形式和友元函数形式，他们都可访问类中的私有成员。
1．重载为类的成员函数
1）X类中重载一元运算符@
返回类型   X：：operator@（）
      { …  }
不指定参数，因为它已带有一个隐含的this参数，对X类的一个对象obj：
       表达式               C++编译器的解释
      @obj                 operator @（obj）
      obj@                 operator @（obj，0）
2）  X类中重载二元运算符@
  返回类型   X：：operator@（参数说明）
      { …  }
由于类的成员函数带有一个this参数，此时只能指定一个参数，对obj对象：
       表达式               C++编译器的解释
     obj1@obj2            obj1·operator@（obj2）
在多数情况下，将运算符重载为类的成员函数和类的友元函数都是可以的。但成员函数运算符与友元函数运算符也具有各自的一些特点：
(1) 一般情况下，单目运算符最好重载为类的成员函数；双目运算符则最好重载为类的友元函数。
(2) 以下一些双目运算符不能重载为类的友元函数：=、()、[]、-&gt;。
(3) 类型转换函数只能定义为一个类的成员函数而不能定义为类的友元函数。
(4) 若一个运算符的操作需要修改对象的状态，选择重载为成员函数较好。
(5) 若运算符所需的操作数（尤其是第一个操作数）希望有隐式类型转换，则只能选用友元函数。
(6) 当运算符函数是一个成员函数时，最左边的操作数（或者只有最左边的操作数）必须是运算符类的一　个类对象（或者是对该类对象的引用）。如果左边的操作数必须是一个不同类的对象，或者是一个内部　类型的对象，该运算符函数必须作为一个友元函数来实现。
(7) 当需要重载运算符具有可交换性时，选择重载为友元函数。
例：给复数运算符重载的四则运算符。复数由实部和虚部构造，定义一个复数类，再在类中重载复数四则运算的运算符。
#include&lt;iostream.h&gt;
class complex
{ 
private: 
     float real,imag;
public:
     complex(float r=0,float i=0);
     complex operator+(complex &amp;c);
     complex operator-(complex &amp;c);
     friend void print(complex &amp;c);
};
complex::complex(float r,float i)
{  
    real=r;  
    imag=i; 
}

complex complex::operator+(complex &amp;c)
{  
    float r=real+c.real;       
    float i=imag+c.imag;      
    return complex(r,i) ;
}

complex complex::operator-(complex &amp;c)
{  
    float r=real-c.real;       
    float i=imag-c.imag;      
    return complex(r, i) ; 
}

void print(complex &amp;c )
{ 
    cout&lt;&lt;'('&lt;&lt;c.real&lt;&lt;','&lt;&lt;c.imag&lt;&lt;')'&lt;&lt;endl;   
}

void main( )
{ 
     complex c1(2.5,3.7), c2(4.2,6.5) ;
     complex c;
     c=c1-c2;             //c=c1·operator-(c2)
     print(c);
     c=c1+c2;             //c=c1·operator+(c2)
     print(c);
 }
 
该程序中定义一个complex类，定义了2个成员函数作为运算符重载函数。
c1+c2编译程序解释为：c1.operator+(c2)
c1、c2是complex 类的对象。operator+()是运算符+的重载函数。
该运算符重载函数仅有一个参数c2。当重载为成员函数时，双目运算符仅有一个参数。
对单目运算符，重载为成员函数时，不能再显式说明参数。
重载为成员函数时，总是隐含了一个参数，即this指针，它是指向调用该成员函数对象的指针。
在重载运算符函数中，operator +( )中参数用引用传递而不用指针传递。
因为指针传递存在程序的可读性问题。如操作符重载声明为：
complex operator +( complex *c)；
则调用时
    complexc1(2.0,3.0),c2(4.0,-2.0),c3;
      c3=&amp;c1+&amp;c2;   会认为是c1的地址和c2的地址相加
而声明为complex operator +(const complex &amp;c)；
则    c3=c1+c2;
 
2. 重载为友元函数
    运算符重载函数还可以为友元函数。当重载为友元函数时，没有隐含的参数this指针。这样对双目运算符，友元函数有2个参数，对单目运算符，友元函数有1个参数。但有些运算符不能重载为友元函数，它们是：=、（）、[]、-&gt;。
1） X类中重载一元运算符@
  返回类型  operator@（X&amp;obj ）
      { …  }
　只能为友元运算符指定一个参数，对X类的一个对象obj：
       表达式               C++编译器的解释
      @obj                 operator @（obj）
      obj@                 operator @（obj，0）
2） X类中重载二元运算符@
返回类型 operator@(参数说明1,参数说明2)
  {……}]
       两个参数中必须至少有一个是X类类型，设有对象obj1，obj2
       表达式               C++编译器的解释
      obj1@obj2          operator @（obj1，obj2）
例：用友元函数代替成员函数编上述程序：
#include&lt;iostream.h&gt;

class complex

{ private: 

     float real,imag;

public:

     complex(float r=0,float i=0);

     friend complex operator+(complex &amp;c1,complex &amp;c2);

     friend complex operator-(complex &amp;c1,complex &amp;c2);

     friend void print(complex &amp;c);

};

complex::complex(float r,float i)

{  real=r;  imag=i;  }

complex operator+(complex &amp;c1,complex &amp;c2)

{  float r=c1.real+c2.real;     float i=c1.imag+c2.imag;  

   return complex(r,i) ; }

complex operator-(complex &amp;c1,complex &amp;c2)

{  float r=c1.real-c2.real;       float i=c1.imag-c2.imag;  

   return complex(r, i) ; }

void print(complex &amp;c )

{ cout&lt;&lt;'('&lt;&lt;c.real&lt;&lt;','&lt;&lt;c.imag&lt;&lt;')'&lt;&lt;endl;   }

void main( )

{ complex c1(2.5,3.7), c2(4.2,6.5) ;

 complex c;

 c=c1-c2;             //c=c1·operator-(c2)

 print(c);

 c=c1+c2;             //c=c1·operator+(c2)

 print(c);

 }
双目运算符重载为成员函数时，仅有一个参数，另一个被隐含；
重载为友元函数时，有两个参数，没有隐含参数；
c1+c2编译程序解释为：operator+( c1，c2)
调用如下函数，进行求值，
complex operator +(const complex&amp;c1,consrt complex &amp;c2)
结论1：对二元运算符，将它重载为一个友元函数比重载为一个成员函数要便于使用。作为一个友元函数，二元运算符不要求第一个参数一定为某类的对象。
结论2：对一元运算符，将它重载为一个成员函数最恰当。重载为友员函数也可以。
例：
#include &lt;iostream.h&gt;

class A{

public:

       A(){X=Y=0;}

       A(int i,int j){X=i;Y=j;}

    A(A &amp;p){X=p.X;Y=p.Y;}

    A&amp; operator =(A &amp;p);

       int getX(){return X;}

       int getY(){return Y;}

private:

       int X,Y;

};

A&amp; A::operator =(A &amp;p)

{

       X=p.X;

       Y=p.Y;

       cout&lt;&lt;"Assignment operator called.\n";

       return *this;

}

void main()

{

       A a(7,8);

       A b;

       b=a;

       cout&lt;&lt;b.getX()&lt;&lt;","&lt;&lt;b.getY()&lt;&lt;endl;

}
Assignment operator called.
7，8
该程序中，在类A中定义了一个赋值运算符函数，被定义为成员函数。
b=a解释为： b.operator=(a)
调用下列函数：A&amp; A::operator=(A &amp;p)完成赋值操作。
 我补充点内容：
在c++中可以这样调用一个 []操作符：
下面的代码中我重载了[]，
class temp{ 
public: 
 const char&amp; operator [] (intindex) const
    {
            return buffer[index];
  
    }
　　char* buffer  
} C++ 操作符重载
 那么我可以通过temp().operator[](4);来调用[]。
由此观之C++的操作符重载其实就是函数的调用。
还有操作符重载中返回左值和右值是按照等号来区分的，可以这么理解左值就是引用（要被赋值），右值就是临时变量。
 类成员 非类成员友元 左值 右值
 

 Vector用法：
(1)vector&lt; 类型 &gt; 标识符 ;
(2)vector&lt; 类型 &gt; 标识符(最大容量) ；
(3)vector&lt; 类型 &gt; 标识符（最大容量，初始所有值）；
(4) int i[4] = {12,3,4,5};
    vector&lt; 类型 &gt; vi(i , i+2); //得到i索引值为3以后的值；  
(5)vector&lt; vector &gt;   //vi 定义2维的容器；记得一定要有空格，不然会报错
  vector&lt; int &gt; line   
  // 在使用的时候一定要首先将vi个行进行初始化;   
  for(int i = 0 ; i &lt; 10 ; i ++)  
  {  
    vector.push_back(line);  
  }  
  /// 个人认为使用vector定义二维数组很好，因为是长度可以不预先确定。很好。 
(6)C++ Vector排序
  vector&lt; int &gt; vi ;   
  vi.push_back(1);  
  vi.push_back(3);  
  vi.push_back(0);  
  sort(vi.begin() , vi.end()); /// /小到大  
  reverse(vi.begin(),vi.end()) /// 从大道小 
(7)顺序访问
  vector &lt; int &gt; vi ;   
  for( int i = 0 ; i &lt; 10 ; i ++)  
  {  
    vector.push_back(i);  
  } 
  
  for(int i = 0 ; i &lt; 10 ; i ++) /// 第一种调用方法  
  {  
    cout &lt;&lt;&lt;"="" "="";=""   &lt;="" span=""style="word-wrap: break-word;"&gt;
  } 
   for(vector::iterator it = vi.begin() ; 
  it !=vi.end() ; it++) ///第二种调用方法  
  {  
    cout &lt;&lt; *it &lt;&lt; " " ;  
  }
(8)查找
vector &lt; int &gt; vi ;   
for( int i = 0 ; i &lt; 10 ; i ++)  
{  
  vector.push_back(i);  
} 
vector &lt; int &gt;::interator it = find(vi.begin() , vi.end,3) ;  
cout &lt;&lt; *it &lt;&lt; endl ; ///返回容器内找到值的位置。 
(9)使用数组对C++ Vector进行初始化
int i[10] ={1,2,3,4,5,6,7,78,8} ;  
///第一种   
vector vi(i+1,i+3); ///从第2个元素到第三个元素  
for(vector ::interator it = vi.begin() ; it != vi.end() ; it++)  
{  
  cout &lt;&lt; *it &lt;&lt;" " ;   
} 
(10) 结构体类型
struct temp  
{  
public :  
  string str ;   
public :  
  int id ;  
}tmp
int main()  
{  
  vector t ;   
  temp w1 ;   
  w1.str = "Hellowor" ;  
  w1.id = 1 ;   
  t.push_back(t1);  
  cout &lt;&lt; w1.str &lt;&lt; "," &lt;&lt;&lt;=""span="" style="word-wrap: break-word;"&gt;
  return 0 ;   

} 

iterator迭代器用法：
1. 迭代器(iterator)是一中检查容器内元素并遍历元素的数据类型。
(1) 每种容器类型都定义了自己的迭代器类型，如vector:
vector&lt;int&gt;::iterator iter;这条语句定义了一个名为iter的变量，它的数据类型是由vector&lt;int&gt;定义的iterator类型。
(2) 使用迭代器读取vector中的每一个元素：
vector&lt;int&gt; ivec(10,1);
for(vector&lt;int&gt;::iterator iter=ivec.begin();iter!=ivec.end();++iter)
{
*iter=2; //使用 * 访问迭代器所指向的元素
}
const_iterator:
只能读取容器中的元素，而不能修改。
for(vector&lt;int&gt;::const_iteratorciter=ivec.begin();citer!=ivec.end();citer++)
{
cout&lt;&lt;*citer;
//*citer=3; error
}
vector&lt;int&gt;::const_iterator 和 constvector&lt;int&gt;::iterator的区别
const vector&lt;int&gt;::iterator newiter=ivec.begin();
*newiter=11; //可以修改指向容器的元素
//newiter++; //迭代器本身不能被修改 
(3) iterator的算术操作：
iterator除了进行++,- -操作，可以将iter+n,iter-n赋给一个新的iteraor对象。还可以使用一个iterator减去另外一个iterator.
const vector&lt;int&gt;::iterator newiter=ivec.begin();
vector&lt;int&gt;::iterator newiter2=ivec.end();
cout&lt;&lt;"\n"&lt;&lt;newiter2-newiter; 
一個很典型使用vector的STL程式:
#include &lt;vector&gt;
#include &lt;iostream&gt;
 using namespace std;
 
 int main() {
 vector&lt;int&gt; ivec;
 ivec.push_back(1);
 ivec.push_back(2);
 ivec.push_back(3);
 ivec.push_back(4);
 
 for(vector&lt;int&gt;::iterator iter =ivec.begin();1. iter != ivec.end(); ++iter)
 cout &lt;&lt; *iter &lt;&lt; endl;
 }
2. Iterator（迭代器）模式
一、概述
    Iterator（迭代器）模式又称Cursor（游标）模式，用于提供一种方法顺序访问一个聚合对象中各个元素, 而又不需暴露该对象的内部表示。或者这样说可能更容易理解：Iterator模式是运用于聚合对象的一种模式，通过运用该模式，使得我们可以在不知道对象内部表示的情况下，按照一定顺序（由iterator提供的方法）访问聚合对象中的各个元素。
    由于Iterator模式的以上特性：与聚合对象耦合，在一定程度上限制了它的广泛运用，一般仅用于底层聚合支持类，如STL的list、vector、stack等容器类及ostream_iterator等扩展iterator。
    根据STL中的分类，iterator包括：
Input Iterator：只能单步向前迭代元素，不允许修改由该类迭代器引用的元素。
Output Iterator：该类迭代器和Input Iterator极其相似，也只能单步向前迭代元素，不同的是该类迭代器对元素只有写的权力。
Forward Iterator：该类迭代器可以在一个正确的区间中进行读写操作，它拥有Input Iterator的所有特性，和Output Iterator的部分特性，以及单步向前迭代元素的能力。
Bidirectional Iterator：该类迭代器是在Forward Iterator的基础上提供了单步向后迭代元素的能力。
Random Access Iterator：该类迭代器能完成上面所有迭代器的工作，它自己独有的特性就是可以像指针那样进行算术计算，而不是仅仅只有单步向前或向后迭代。
    这五类迭代器的从属关系，如下图所示，其中箭头A→B表示，A是B的强化类型，这也说明了如果一个算法要求B，那么A也可以应用于其中。
input output
     \ /
forward
       |
bidirectional
       |
random access
 图1、五种迭代器之间的关系
    vector 和deque提供的是RandomAccessIterator，list提供的是BidirectionalIterator，set和map提供的 iterators是 ForwardIterator，关于STL中iterator迭代器的操作如下：
说明：每种迭代器均可进行包括表中前一种迭代器可进行的操作。
迭代器操作                     说明
(1)所有迭代器
p++：后置自增迭代器                           
++p：前置自增迭代器
(2)输入迭代器
*p：复引用迭代器，作为右值
p=p1： 将一个迭代器赋给另一个迭代器
p==p1：比较迭代器的相等性
p!=p1：比较迭代器的不等性
(3)输出迭代器
*p：复引用迭代器，作为左值
p=p1：将一个迭代器赋给另一个迭代器
(4)正向迭代器
提供输入输出迭代器的所有功能
(5)双向迭代器
--p： 前置自减迭代器                             
p--：后置自减迭代器                               
(6)随机迭代器
p+=i：将迭代器递增i位                             
p-=i：将迭代器递减i位                             
p+i：在p位加i位后的迭代器                               
p-i：在p位减i位后的迭代器                               
p[i]： 返回p位元素偏离i位的元素引用                             
p&lt;p1：如果迭代器p的位置在p1前，返回true，否则返回false                            
p&lt;=p1：p的位置在p1的前面或同一位置时返回true，否则返回false                          
p&gt;p1：如果迭代器p的位置在p1后，返回true，否则返回false                            
p&gt;=p1：p的位置在p1的后面或同一位置时返回true，否则返回false                          
    只有顺序容器和关联容器支持迭代器遍历，各容器支持的迭代器的类别如下：
容器                 支持的迭代器类别            容器               支持的迭代器类别            容器                 支持的迭代器类别
vector              随机访问                      deque              随机访问                       list                   双向
set                  双向                            multiset            双向                           map                 双向
multimap          双向                             stack                不支持                        queue              不支持
priority_queue   不支持


二、结构
Iterator模式的结构如下图所示：
                                   
                                                             图2、Iterator模式类图示意
三、应用
    Iterator模式有三个重要的作用：
1）它支持以不同的方式遍历一个聚合.复杂的聚合可用多种方式进行遍历，如二叉树的遍历，可以采用前序、中序或后序遍历。迭代器模式使得改变遍历算法变得很容易: 仅需用一个不同的迭代器的实例代替原先的实例即可，你也可以自己定义迭代器的子类以支持新的遍历，或者可以在遍历中增加一些逻辑，如有条件的遍历等。
2）迭代器简化了聚合的接口. 有了迭代器的遍历接口，聚合本身就不再需要类似的遍历接口了，这样就简化了聚合的接口。
3）在同一个聚合上可以有多个遍历每个迭代器保持它自己的遍历状态，因此你可以同时进行多个遍历。
4）此外，Iterator模式可以为遍历不同的聚合结构（需拥有相同的基类）提供一个统一的接口，即支持多态迭代。
    简单说来，迭代器模式也是Delegate原则的一个应用，它将对集合进行遍历的功能封装成独立的Iterator，不但简化了集合的接口，也使得修改、增加遍历方式变得简单。从这一点讲，该模式与Bridge模式、Strategy模式有一定的相似性，但Iterator模式所讨论的问题与集合密切相关，造成在Iterator在实现上具有一定的特殊性，具体将在示例部分进行讨论。
四、优缺点
     正如前面所说，与集合密切相关，限制了 Iterator模式的广泛使用，就个人而言，我不大认同将Iterator作为模式提出的观点，但它又确实符合模式“经常出现的特定问题的解决方案”的特质，以至于我又不得不承认它是个模式。在一般的底层集合支持类中，我们往往不愿“避轻就重”将集合设计成集合 + Iterator 的形式，而是将遍历的功能直接交由集合完成，以免犯了“过度设计”的诟病，但是，如果我们的集合类确实需要支持多种遍历方式（仅此一点仍不一定需要考虑 Iterator模式，直接交由集合完成往往更方便），或者，为了与系统提供或使用的其它机制，如STL算法，保持一致时，Iterator模式才值得考虑。
五、举例
    可以考虑使用两种方式来实现Iterator模式：内嵌类或者友元类。通常迭代类需访问集合类中的内部数据结构，为此，可在集合类中设置迭代类为friend class，但这不利于添加新的迭代类，因为需要修改集合类，添加friend class语句。也可以在抽象迭代类中定义protected型的存取集合类内部数据的函数，这样迭代子类就可以访问集合类数据了，这种方式比较容易添加新的迭代方式，但这种方式也存在明显的缺点：这些函数只能用于特定聚合类，并且，不可避免造成代码更加复杂。
    STL的list::iterator、deque::iterator、rbtree::iterator等采用的都是外部Iterator类的形式，虽然STL的集合类的iterator分散在各个集合类中，但由于各Iterator类具有相同的基类，保持了相同的对外的接口（包括一些traits及tags等，感兴趣者请认真阅读参考1、2），从而使得它们看起来仍然像一个整体，同时也使得应用algorithm成为可能。我们如果要扩展STL的iterator，也需要注意这一点，否则，我们扩展的iterator将可能无法应用于各algorithm。
Array用法：

    在c++中创建数组时,大量使用new和delete,很繁琐，但使用&lt;array&gt;很方便,用起来和STL&lt;vector&gt;一样的，而且执行效率比&lt;vector&gt;高,差不多和int myarray[5]效率一样：
但是要注意的是：
（1）     array 定义的时候必须定义数组的元素个数;而vector不需要；且只能包含整型字面值常量，枚举常量或者用常量表达式初始化的整型const对象，
非const变量以及需要到运行阶段才知道其值的const变量都不能用来定义数组的维度.
（2）     array 定义后的空间是固定的了，不能改变；而vector 要灵活得多，可再加或减.
（3）     vector有一系列的函数操作，非常方便使用.和vector不同，数组不提供push——back或者其他的操作在数组中添加新元素，数组一经定义就不允许添加新元素；
若需要则要充许分配新的内存空间，再将员数组的元素赋值到新的内存空间。
（4）     数组和vector不同，一个数组不能用另一个数组初始化，也不能将一个数组赋值给另一个数组；
示例：

#include &lt;iostream&gt;  
#include &lt;array&gt;     
using namespace std;  
  int main ()  
{  
 //--这是1维数组  
    array&lt;int,5&gt; myarray={1,2,3,4,5};  
  cout &lt;&lt;"myarray="&lt;&lt;endl;  
  for (size_t n=0; n&lt;myarray.size(); n++){  
      cout &lt;&lt; myarray[n] &lt;&lt;'\t';  
  }  
  cout &lt;&lt; endl;  
   //当然也可以使用  
  cout &lt;&lt;"myarray="&lt;&lt;endl;  
  for (size_t n=0; n&lt;myarray.size(); n++){  
      cout &lt;&lt; myarray.at(n) &lt;&lt; '\t';  
  }  
   cout &lt;&lt; endl;  
//--这是2维数组,共3行2列  
  array&lt;array&lt;int,2&gt;,3 &gt; myarray2D={1,2,3,4,5,6};  
   cout &lt;&lt;"myarray2D="&lt;&lt;endl;  
  for (size_t m=0; m&lt;myarray2D.size(); m++){  
    for (size_t n=0; n&lt;myarray2D[m].size(); n++){      
        cout &lt;&lt; myarray2D[m][n] &lt;&lt;'\t';  
    }   
   cout &lt;&lt; endl;   
  }  
  cout &lt;&lt; endl;  
  return 0;  
}  

C++智能指针处理Array对象示例：
//C++11的&lt;memory&gt;中有一整套智能指针，
//完全可以避免写手动的delete代码，
//但是它默认使用delete删除对象，
//如果是数组对象，需要指定自定义的删除方法，支持delete[]
std::shared_ptr&lt;int&gt; p(new int[10],
    [](int* p){
        delete[] p;
    });
//或者使用helper
std::shared_ptr&lt;int&gt; p(new int[10],std::default_delete&lt;int[]&gt;()); 

unique_ptr跟shared_ptr不一样，它直接支持持有数组对象

std::unique_ptr&lt;int[]&gt; p(new int[10]);//ok
std::shared_ptr&lt;int[]&gt; p(new int[10]);//error, does not compile

std::unique_ptr&lt;int, void(*)(int*)&gt; p(new int[10],
    [](int* p){
        delete[] p;
    });




（1）指针，是object的地址；
（2）引用，是object的别名。
      备注：（不存在引用的指针，因为引用不是实际的object；存在指针的引用，因为指针是实际的object。）
      那指针的引用怎么定义？
int i = 42;
int *p;        // p是整型的指针
int *&amp;r = p;    // r是引用，它引用的类型是int*
r = &amp;i;        // r就是p，因此p被赋值为i的地址
*r = 0;        // r就是p，p指向i，i被赋值为0
     理解r的定义时，从右往左依次理解&amp;和*。

    指针-对于一个类型T，T*就是指向T的指针类型，也即一个T*类型的变量能够保存一个T对象的地址，而类型T是可以加一些限定词的，如const、volatile等等。见下图，所示指针的含义：

                                               


     引用-引用是一个对象的别名，主要用于函数参数和返回值类型，符号X&amp;表示X类型的引用。见下图，所示引用的含义：

                                              

     引用不可以为空，但指针可以为空。前面也说过了引用是对象的别名，引用为空——对象都不存在，怎么可能有别名！故定义一个引用的时候，必须初始化。因此如果你有一个变量是用于指向另一个对象，但是它可能为空，这时你应该使用指针；如果变量总是指向一个对象，你的设计不允许变量为空，这时你应该使用引用。不初始化，连编译都通不过！如图1.1所示：

                              

                                                                                                  如图1.1

声明指针是可以不指向任何对象，也正是因为这个原因，使用指针之前必须做判空操作，而引用就不必。

引用不可以改变指向，对一个对象"至死不渝"；但是指针可以改变指向，而指向其它对象。说明：虽然引用不可以改变指向，但是可以改变初始化对象的内容。例如就++操作而言，对引用的操作直接反应到所指向的对象，而不是改变指向；而对指针的操作，会使指针指向下一个对象，而不是改变所指对象的内容。如图1-2所示：



                                 

                                

                                                                                                           如图1-2

     可以通过断点调试，自行验证。

     引用的大小是所指向的变量的大小，因为引用只是一个别名而已；指针是指针本身的大小，4个字节。见下图1-3所示：

                                         



                                                                                                             图1-3

     由上可见，引用比指针使用起来形式上更漂亮，使用引用指向的内容时可以之间用引用变量名，而不像指针一样要使用*；定义引用的时候也不用像指针一样使用&amp;取址。

     引用比指针更安全，在C++标准开发建议：尽可能多的使用引用，少用指针。指针指向一块内存，它的内容是所指内存的地址；而引用则是某块内存的别名，引用不改变指向；由于不存在空引用，并且引用一旦被初始化为指向一个对象，它就不能被改变为另一个对象的引用，因此引用很安全。对于指针来说，它可以随时指向别的对象，并且可以不被初始化，或为NULL，所以不安全。const 指针虽然不能改变指向，但仍然存在空指针，并且有可能产生野指针（即多个指针指向一块内存，free掉一个指针之后，别的指针就成了野指针）。

  （1）常量指针与常量引用，Const关键字对引用和指针的影响：

     常量指针：指向常量的指针，在指针定义语句的类型前加const，表示指向的对象是常量。定义指向常量的指针只限制指针的间接访问操作，而不能规定指针指向的值本身的操作规定性。

                                         



     常量指针定义"constint* pointer=&amp;a"告诉编译器，*pointer是常量，不能将*pointer作为左值进行操作。

                                     



     常量引用：指向常量的引用，在引用定义语句的类型前加const，表示指向的对象是常量。也跟指针一样不能利用引用对指向的变量进行重新赋值操作。

   （2）指针常量VS引用常量：

    在指针定义语句的指针名前加const，表示指针本身是常量。在定义指针常量时必须初始化！而这是引用天生具来的属性，不用再引用指针定义语句的引用名前加const。

    指针常量定义"int* const pointer=&amp;b"告诉编译器，pointer是常量，不能作为左值进行操作，但是允许修改间接访问值，即*pointer可以修改。

                                    



   （3）常量指针常量VS常量引用常量

     常量指针常量：指向常量的指针常量，可以定义一个指向常量的指针常量，它必须在定义时初始化。常量指针常量定义"const int* const pointer=&amp;c"告诉编译器，pointer和*pointer都是常量，他们都不能作为左值进行操作。

     而就不存在所谓的"常量引用常量"，因为跟上面讲的一样引用变量就是引用常量。C++不区分变量的const引用和const变量的引用。程序决不能给引用本身重新赋值，使他指向另一个变量，因此引用总是const的。如果对引用应用关键字const，起作用就是使其目标称为const变量。即没有：Const double const&amp; a=1；只有const double&amp; a=1；

    总结：有一个规则可以很好的区分const是修饰指针，还是修饰指针指向的数据——画一条垂直穿过指针声明的星号（*），如果const出现在线的左边，指针指向的数据为常量；如果const出现在右边，指针本身为常量。而引用本身与天俱来就是常量，即不可以改变指向。

   （4）   指针传递和引用传递：

     指针传递参数本质上是值传递的方式，它所传递的是一个地址值。值传递过程中，被调函数的形式参数作为被调函数的局部变量处理，即在栈中开辟了内存空间以存放由主调函数放进来的实参的值，从而成为了实参的一个副本。值传递的特点是被调函数对形式参数的任何操作都是作为局部变量进行，不会影响主调函数的实参变量的值。引用传递过程中，被调函数的形式参数也作为局部变量在栈中开辟了内存空间，但是这时存放的是由主调函数放进来的实参变量的地址。被调函数对形参的任何操作都被处理成间接寻址，即通过栈中存放的地址访问主调函数中的实参变量。正因为如此，被调函数对形参做的任何操作都影响了主调函数中的实参变量。引用传递和指针传递是不同的，虽然它们都是在被调函数栈空间上的一个局部变量，但是任何对于引用参数的处理都会通过一个间接寻址的方式操作到主调函数中的相关变量。而对于指针传递的参数，如果改变被调函数中的指针地址，它将影响不到主调函数的相关变量。

    如果想通过指针参数传递来改变主调函数中的相关变量，那就得使用指向指针的指针，或者指针引用。如图1-4所示：

                                                       




   （5）    我们利用下面一段简单的代码来深入分析指针和引用:

                                                   

     敲完以后，G++ test1.cpp,对生成的a.out,反汇编：objdump -d a.out，得到main函数的一段汇编代码如下：

                                                      

                                                     

                                                    





          从汇编代码可以看出实际上指针和引用在编译器中的实现是一样的：有上面分析可知：虽然指针和引用最终在编译中的实现是一样的，但是引用的形式大大方便了使用也更安全。而且可以知道，引用也是占用内存的。

一、结构体是类的一种、一般来说，结构体会定义在头文件中。
由于暂时不涉及到类的方法，只涉及到类的数据，因此可以先从结构体开始。
比如下面这个结构体：
   struct Sales_data {
       string bookNo;
       unsigned units_sold;
       double revenue;
    };
新的C++语言标准规定，可以在结构体的声明里面进行初始化，这可能需要对编译器进行设置。
二、一般来说，头文件的名称来源于结构体的名称。
使用预定义来使得头文件仅包含一次：
#ifndef SALES_ITEM_H_
#define SALES_ITEM_H_
/* ...... */
#endif /* SALES_ITEM_H_ */
1.       #include“headfile.h”
搜索顺序为：
①先搜索当前目录
②然后搜索-I指定的目录
③再搜索gcc的环境变量CPLUS_INCLUDE_PATH（C程序使用的是C_INCLUDE_PATH）
④最后搜索gcc的内定目录
/usr/include
/usr/local/include
/usr/lib/gcc/x86_64-redhat-linux/4.1.1/include
各目录存在相同文件时，先找到哪个使用哪个。
2.       #include&lt;headfile.h&gt;
①先搜索-I指定的目录
②然后搜索gcc的环境变量CPLUS_INCLUDE_PATH
③最后搜索gcc的内定目录
/usr/include
/usr/local/include
/usr/lib/gcc/x86_64-redhat-linux/4.1.1/include
 与上面的相同，各目录存在相同文件时，先找到哪个使用哪个。这里要注意，#include&lt;&gt;方式不会搜索当前目录！
这里要说下include的内定目录，它不是由$PATH环境变量指定的，而是由g++的配置prefix指定的(知道它在安装g++时可以指定，不知安装后如何修改的，可能是修改配置文件，需要时再研究下)：
-bash-3.2$ g++ -v
Using built-in specs.
Target: x86_64-redhat-linux
Configured with: ../configure --prefix=/usr --mandir=/usr/share/man--infodir=/usr/share/info --enable-shared --enable-threads=posix--enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions--enable-libgcj-multifile--enable-languages=c,c++,objc,obj-c++,java,fortran,ada
 --enable-java-awt=gtk--disable-dssi --enable-plugin--with-java-home=/usr/lib/jvm/java-1.4.2-gcj-1.4.2.0/jre --with-cpu=generic--host=x86_64-redhat-linux
Thread model: posix
gcc version 4.1.2 20080704 (Red Hat4.1.2-46)
 
在安装g++时，指定了prefix，那么内定搜索目录就是：
Prefix/include
Prefix/local/include
Prefix/lib/gcc/--host/--version/include
编译时可以通过-nostdinc++选项屏蔽对内定目录搜索头文件。
 库文件：
编译的时候：
①gcc会去找-L
②再找gcc的环境变量LIBRARY_PATH
③再找内定目录 /lib /usr/lib /usr/local/lib 这是当初compilegcc时写在程序内的（不可配置的？）
运行时动态库的搜索路径：
动态库的搜索路径搜索的先后顺序是：
①编译目标代码时指定的动态库搜索路径（这是通过gcc 的参数"-Wl,-rpath,"指定。当指定多个动态库搜索路径时，路径之间用冒号"："分隔）
②环境变量LD_LIBRARY_PATH指定的动态库搜索路径（当通过该环境变量指定多个动态库搜索路径时，路径之间用冒号"："分隔）
③配置文件/etc/ld.so.conf中指定的动态库搜索路径；
④默认的动态库搜索路径/lib；
⑤默认的动态库搜索路径/usr/lib。
（应注意动态库搜寻路径并不包括当前文件夹，所以当即使可执行文件和其所需的so文件在同一文件夹，也会出现找不到so的问题，类同#include &lt;header_file&gt;不搜索当前目录）


 参考资料: http://my.oschina.net/alphajay/blog/4953?from=rss
 

一、Const常量
1、Const定义：
常类型是指使用类型修饰符const修饰的类型，常类型的变量或对象的值是不能被更新的。const 推出的初始目的，正是为了取代预编译指令，消除它的缺点，同时继承它的优点。
2、Const作用：
（1）可以定义const常量，具有不可变性。例如：
               const int Max=100; int Array[Max]; 
&lt;span style="color:#333333;"&gt;（&lt;/span&gt;&lt;span style="color:#333333;"&gt;2&lt;/span&gt;&lt;span style="color:#333333;"&gt;）便于进行类型检查，使编译器对处理内容有更多了解，消除了一些隐患。&lt;/span&gt;&lt;span style="color:#333333;"&gt;例如：&lt;/span&gt;&lt;span style="color:#333333;"&gt; void f(const int i) { .........} &lt;/span&gt;&lt;span style="color:#333333;"&gt;编译器就会知道&lt;/span&gt;&lt;span style="color:#333333;"&gt;i&lt;/span&gt;&lt;span style="color:#333333;"&gt;是一个常量，不允许修改。&lt;/span&gt;&lt;span style="color:#333333;"&gt;（&lt;/span&gt;&lt;span style="color:#333333;"&gt;3&lt;/span&gt;&lt;span style="color:#333333;"&gt;）可以避免意义模糊的数字出现，同样可以很方便地进行参数的调整和修改。&lt;/span&gt;&lt;span style="color:#333333;"&gt;同宏定义一样，可以做到不变则已，一变都变！如（&lt;/span&gt;&lt;span style="color:#333333;"&gt;1&lt;/span&gt;&lt;span style="color:#333333;"&gt;）中，如果想修改&lt;/span&gt;&lt;span style="color:#333333;"&gt;Max&lt;/span&gt;&lt;span style="color:#333333;"&gt;的内容，只需要：&lt;/span&gt;&lt;span style="color:#333333;"&gt;const int Max=you want;&lt;/span&gt;&lt;span style="color:#333333;"&gt;即可！&lt;/span&gt;&lt;span style="color:#333333;"&gt; &lt;/span&gt;&lt;span style="color:#333333;"&gt;（&lt;/span&gt;&lt;span style="color:#333333;"&gt;4&lt;/span&gt;&lt;span style="color:#333333;"&gt;）可以保护被修饰的东西，防止意外的修改，增强程序的健壮性。&lt;/span&gt;&lt;span style="color:#333333;"&gt;还是上面的例子，如果在函数体内修改了&lt;/span&gt;&lt;span style="color:#333333;"&gt;i&lt;/span&gt;&lt;span style="color:#333333;"&gt;，编译器就会报错；&lt;/span&gt;&lt;span style="color:#333333;"&gt;例如：&lt;/span&gt;
void f(const int i) { i=10;//error! } 
&lt;span style="color:#333333;"&gt;（&lt;/span&gt;&lt;span style="color:#333333;"&gt;5&lt;/span&gt;&lt;span style="color:#333333;"&gt;）&lt;/span&gt;&lt;span style="color:#333333;"&gt;为函数重载提供了一个参考。例如如下代码：&lt;/span&gt;
class A { ......
void f(int i) {......} //一个函数
void f(int i) const {......} //上一个函数的重载 ......
};
（6）可以节省空间，避免不必要的内存分配。例如如下代码：
#define PI 3.14159 //常量宏   
const doulbe Pi=3.14159; //此时并未将Pi放入ROM中 ......   
double i=Pi; //此时为Pi分配内存，以后不再分配！   
double I=PI; //编译期间进行宏替换，分配内存   
double j=Pi; //没有内存分配   
double J=PI; //再进行宏替换，又一次分配内存！   
const定义常量从汇编的角度来看，只是给出了对应的内存地址，而不是象#define一样给出的是立即数，所以，const定义的常量在程序运行过程中只有一份拷贝，而#define定义的常量在内存中有若干个拷贝。 
&lt;span style="color:#333333;"&gt;（&lt;/span&gt;&lt;span style="color:#333333;"&gt;7&lt;/span&gt;&lt;span style="color:#333333;"&gt;）&lt;/span&gt;&lt;span style="color:#333333;"&gt;提高了效率。&lt;/span&gt;&lt;span style="color:#333333;"&gt;编译器通常不为普通&lt;/span&gt;&lt;span style="color:#333333;"&gt;const&lt;/span&gt;&lt;span style="color:#333333;"&gt;常量分配存储空间，而是将它们保存在符号表中，这使得它成为一个编译期间的常量，没有了存储与读内存的操作，使得它的效率也很高。&lt;/span&gt;
3、Const常用的几种情形：
（1）修饰一般常量一般常量是指简单类型的常量。这种常量在定义时，修饰符const可以用在类型说明符前，也可以用在类型说明符后。
&lt;span style="color:#333333;"&gt;               &lt;/span&gt;&lt;span style="color:#333333;"&gt;例如：&lt;/span&gt;&lt;span style="color:#333333;"&gt; int const x=2; &lt;/span&gt;&lt;span style="color:#333333;"&gt;或&lt;/span&gt;&lt;span style="color:#333333;"&gt; const int x=2; &lt;/span&gt;&lt;span style="color:#333333;"&gt;（&lt;/span&gt;&lt;span style="color:#333333;"&gt;2&lt;/span&gt;&lt;span style="color:#333333;"&gt;）修饰常数组&lt;/span&gt;&lt;span style="color:#333333;"&gt;定义或说明一个常数组可采用如下格式：&lt;/span&gt;&lt;span style="color:#333333;"&gt;               int const a[5]={1, 2, 3, 4, 5}; &lt;/span&gt;&lt;span style="color:#333333;"&gt;               const int a[5]={1, 2, 3, 4, 5}; &lt;/span&gt;&lt;span style="color:#333333;"&gt;（&lt;/span&gt;&lt;span style="color:#333333;"&gt;3&lt;/span&gt;&lt;span style="color:#333333;"&gt;）修饰常对象&lt;/span&gt;&lt;span style="color:#333333;"&gt;常对象是指对象常量，定义格式如下：&lt;/span&gt;&lt;span style="color:#333333;"&gt;   &lt;/span&gt;&lt;span style="color:#333333;"&gt;               class A; &lt;/span&gt;&lt;span style="color:#333333;"&gt;               const A a; &lt;/span&gt;
               A const a; 
&lt;span style="color:#333333;"&gt;定义常对象时，同样要进行初始化，并且该对象不能再被更新，修饰符&lt;/span&gt;&lt;span style="color:#333333;"&gt;const&lt;/span&gt;&lt;span style="color:#333333;"&gt;可以放在类名后面，也可以放在类名前面。&lt;/span&gt;&lt;span style="color:#333333;"&gt; &lt;/span&gt;&lt;span style="color:#333333;"&gt;（&lt;/span&gt;&lt;span style="color:#333333;"&gt;4&lt;/span&gt;&lt;span style="color:#333333;"&gt;）修饰常指针&lt;/span&gt;
               const int *A; //const修饰指向的对象，A可变，A指向的对象不可变
               int const *A; //const修饰指向的对象，A可变，A指向的对象不可变
               int *const A; //const修饰指针A， A不可变，A指向的对象可变
               const int *const A;//指针A和A指向的对象都不可变 
&lt;span style="color:#333333;"&gt;（&lt;/span&gt;&lt;span style="color:#333333;"&gt;5&lt;/span&gt;&lt;span style="color:#333333;"&gt;）修饰常引用&lt;/span&gt;&lt;span style="color:#333333;"&gt;使用&lt;/span&gt;&lt;span style="color:#333333;"&gt;const&lt;/span&gt;&lt;span style="color:#333333;"&gt;修饰符也可以说明引用，被说明的引用为常引用，该引用所引用的对象不能被更新。&lt;/span&gt;&lt;span style="color:#333333;"&gt;               &lt;/span&gt;&lt;span style="color:#333333;"&gt;其定义格式如下：&lt;/span&gt;
               const double &amp; v; 
&lt;span style="color:#333333;"&gt;（&lt;/span&gt;&lt;span style="color:#333333;"&gt;6&lt;/span&gt;&lt;span style="color:#333333;"&gt;）修饰函数的常参数&lt;/span&gt;&lt;span style="color:#333333;"&gt; const&lt;/span&gt;&lt;span style="color:#333333;"&gt;修饰符也可以修饰函数的传递参数，格式如下：&lt;/span&gt;
               void Fun(const int Var); 告诉编译器Var在函数体中的无法改变，从而防止了使用者的一些无意的或错误的修改。
&lt;span style="color:#333333;"&gt;（&lt;/span&gt;&lt;span style="color:#333333;"&gt;7&lt;/span&gt;&lt;span style="color:#333333;"&gt;）修饰函数的返回值：&lt;/span&gt;&lt;span style="color:#333333;"&gt; const&lt;/span&gt;&lt;span style="color:#333333;"&gt;修饰符也可以修饰函数的返回值，是返回值不可被改变，格式如下：&lt;/span&gt;
               const int Fun1(); const MyClass Fun2(); 
&lt;span style="color:#333333;"&gt;（&lt;/span&gt;&lt;span style="color:#333333;"&gt;8&lt;/span&gt;&lt;span style="color:#333333;"&gt;）修饰类的成员函数：&lt;/span&gt;&lt;span style="color:#333333;"&gt; const&lt;/span&gt;&lt;span style="color:#333333;"&gt;修饰符也可以修饰类的成员函数，格式如下：&lt;/span&gt;
class ClassName {   
public:   
int Fun() const; .....   
};   
这样，在调用函数Fun时就不能修改类里面的数据 
&lt;span style="color:#333333;"&gt;（&lt;/span&gt;&lt;span style="color:#333333;"&gt;9&lt;/span&gt;&lt;span style="color:#333333;"&gt;）在另一连接文件中引用&lt;/span&gt;&lt;span style="color:#333333;"&gt;const&lt;/span&gt;&lt;span style="color:#333333;"&gt;常量&lt;/span&gt;
               extern const int i;//正确的引用
               extern const int j=10;//错误！常量不可以被再次赋值，另外，还要注意，常量必须初始化！例如： const int i=5; 
（10）const修饰this指针
this指针是个什么类型的？这要看具体情况：如果在非const成员函数中，this指针只是一个类类型的；如果在const成员函数中，this指针是一个const类类型的；如果在volatile成员函数中,this指针就是一个volatile类类型的。 
二、auto1、使用迭代器：vector&lt;vector&lt;int&gt; &gt; v;vector&lt;vector&lt;int&gt; &gt;::iterator it = v.begin(); 2、函数指针也同样, 类型声明很特别：int add(int x,int y){    return x+y;}int main(){    int (*func)(int,int) =add;    cout&lt;&lt;func(1,2)&lt;&lt;endl;}3、既然把v.begin()赋给it, 类型已经在编译期确定了，编译器知道正确的类型是什么，再加一个类型声明实在很繁琐。C++11有了auto。我们可以这样写：vector&lt;vector&lt;int&gt;&gt; v; // C++11 可以不用在'&gt;&gt;'之间加空格了！
auto it = v.begin();auto func = add;编译器会根据值的类型，推导出autob变量。类型的推导是在编译期就完成的，仍是静态类型，和脚本语言不同。实际上是一个语法糖。但由于C++对模板的大量使用，一个变量的类型有时过于复杂难以写出，这样的语法糖是必要的。三、decltype1、有时候，我们需要让编译器根据表达式来确定数据类型，但又不初始化这个变量。
这个时候就可以使用decltype关键字：
decltype(f())sum = x;
2、函数f()并不会被调用，但是它的返回值类型会在这里被使用。
decltype不同于auto，它会完整的保留const。
int i = 42, *p = &amp;i, &amp;r = i;
decltype(r+ 0) b;
decltype(*p)c;
decltype(r)是一个引用类型，而decltype(r + 0)是一个int类型。
decltype(*p)表示返回的将是一个引用，即int&amp;。
3、还需要注意括号的使用：
decltype((i))d; // 错误的表达，d是int&amp;，必须被初始化。
decltype(i)e;   // 正确的表达，e是int。
注意：不加括号，返回的是变量类型；加了括号，返回的是表达式，赋值表达式会产生一个“=”左边的类型的引用。四、变量别名
1、变量别名可以使用typedef：
typedefdouble wages;也可以使用using:
using SI= Sales_item;
2、变量别名和const结合，会产生有趣的结果：
typedefchar *pstring;
constpstring cstr = 0;

cstr是指向char类型的常量指针，因为const要和base type结合。
3、这里，对象可以被更改，但cstr不可被更改。
typedef不是define，不能直接替代变量的内容。

 

在C++中，有char*的字符串，有string的字符串，我们平时大多选用C++标准程序库中的string类，是因为他和前者比较起来，不必担心内存是否足够、字符串长度等等，而且作为一个类出现，他集成的操作函数足以完成我们绝大多数情况下的需要。
在使用string字符串时，首先先包含头文件:#include &lt;string&gt; //注意这里不是string.h string.h是C字符串头文件
其次，声明: string Str;
这样我们就声明了一个字符串变量，但既然是一个类，就有构造函数和析构函数。上面的声明没有传入参数，所以就直接使用了string的默认的构造函数，这个函数所作的就是把Str初始化为一个空字符串。String类的构造函数和析构函数如下：
1. string有多种构造方式,按使用频率排列为:
    string()  or  string s;                            //构造空字符串
    string( conststring&amp; s );   or  string s(str)         //拷贝构造
    string(input_iterator start, input_iterator end );    //迭代器构造
    string( conststring&amp; str, size_type index, size_type length );//拷贝一个字符串的子串
    string( size_typelength, const char&amp; ch );   //用单个字符构造
    string( const char*str);                                   //用C风格字符串构造
    string( constchar* str, size_type length );//截取C风格字符窜的前length个字符构造
   strings(str,stridx) //将字符串str内“始于位置stridx”的部分当作字符串的初值
   string s(cstr) //将C字符串作为s的初值
   string s(num,c) //生成一个字符串，包含num个c字符
   strings(str,stridx,strlen) //将字符串str内“始于stridx且长度顶多strlen”的部分作为字符串的初值
   strings(beg,end) //以区间beg;end(不包含end)内的字符作为字符串s的初值
   strings(chars,chars_len) //将C字符串前chars_len个字符作为字符串s的初值。
  s.~string()//销毁所有字符，释放内存
2. 为了方便使用string重载了以下操作符:
     //比较 全部是以字典序为基础的
    booloperator==(const string&amp; c1, const string&amp; c2);
    booloperator!=(const string&amp; c1, const string&amp; c2);
    booloperator&lt;(const string&amp; c1, const string&amp; c2);
    booloperator&gt;(const string&amp; c1, const string&amp; c2);
    booloperator&lt;=(const string&amp; c1, const string&amp; c2);
    booloperator&gt;=(const string&amp; c1, const string&amp; c2);
 
    //+   可与 string,C风格字符串,字符 相加
    stringoperator+(const string&amp; s1, const string&amp; s2 );
    stringoperator+(const char* s, const string&amp; s2 );
    string operator+(char c, const string&amp; s2 );
    string operator+(const string&amp; s1, const char* s );
    string operator+(const string&amp; s1, char c );
    basic_string&amp;operator+=(const basic_string&amp; append);
    basic_string&amp;operator+=(const char* append);
    basic_string&amp;operator+=(const char  append);
 
    //输入输出
    ostream&amp;operator&lt;&lt;( ostream&amp; os, const string&amp; s );
    istream&amp; operator&gt;&gt;(istream&amp; is, string&amp; s );
    string&amp;operator=( const string&amp; s );
 
    //赋值
    string&amp;operator=( const char* s );
    string&amp;operator=( char ch );       //不常用的赋值
    char&amp;operator[]( size_type index );
 
     其中最后一个可能是使用的最频繁的,它让程序员可以像访问字符数组一样访问字符串的元素.
 
函数名: stpcpy
功 能: 拷贝一个字符串到另一个
用 法: char*stpcpy(char *destin, char *source);
程序例:
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
int main(void)
{
   char string[10];
   char *str1 = "abcdefghi";
   stpcpy(string, str1);
   printf("%s\n", string);
   return 0;
}
 
函数名: strcat
功 能: 字符串拼接函数
用 法: char*strcat(char *destin, char *source);
程序例:
#include &lt;string.h&gt;
#include &lt;stdio.h&gt;
int main(void)
{
   char destination[25];
   char *blank = " ", *c = "C++", *Borland ="Borland";
   strcpy(destination, Borland);
   strcat(destination, blank);
   strcat(destination, c);
   printf("%s\n", destination);
   return 0;
}
 
函数名: strchr
功 能: 在一个串中查找给定字符的第一个匹配之处\
用 法: char*strchr(char *str, char c);
程序例:
#include &lt;string.h&gt;
#include &lt;stdio.h&gt;
int main(void)
{
    char string[15];
    char *ptr, c = 'r';
    strcpy(string, "This is a string");
    ptr = strchr(string, c);
    if (ptr)
       printf("The character %c is atposition: %d\n", c, ptr-string);
    else
       printf("The character was notfound\n");
    return 0;
}
 
函数名: strcmp
功 能: 串比较
用 法: intstrcmp(char *str1, char *str2);
看Asic码，str1&gt;str2，返回值 &gt; 0；两串相等，返回0
程序例:
#include &lt;string.h&gt;
#include &lt;stdio.h&gt;
int main(void)
{
    char *buf1 = "aaa", *buf2 = "bbb",*buf3 = "ccc";
    int ptr;
    ptr = strcmp(buf2, buf1);
    if (ptr &gt; 0)
       printf("buffer 2 is greater thanbuffer 1\n");
    else
       printf("buffer 2 is less thanbuffer 1\n");
    ptr = strcmp(buf2, buf3);
    if (ptr &gt; 0)
       printf("buffer 2 is greater thanbuffer 3\n");
    else
       printf("buffer 2 is less thanbuffer 3\n");
    return 0;
}
 
函数名: strncmpi
功 能: 将一个串中的一部分与另一个串比较, 不管大小写
用 法: intstrncmpi(char *str1, char *str2, unsigned maxlen);
程序例:
#include &lt;string.h&gt;
#include &lt;stdio.h&gt;
int main(void)
{
   char *buf1 = "BBB", *buf2 = "bbb";
   int ptr;
   ptr = strcmpi(buf2, buf1);
   if (ptr &gt; 0)
      printf("buffer 2 is greater thanbuffer 1\n");
   if (ptr &lt; 0)
      printf("buffer 2 is less than buffer1\n");
   if (ptr == 0)
      printf("buffer 2 equals buffer1\n");
   return 0;
}
 
函数名: strcpy
功 能: 串拷贝
用 法: char*strcpy(char *str1, char *str2);
程序例:
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
int main(void)
{
    char string[10];
    char *str1 = "abcdefghi";
    strcpy(string, str1);
    printf("%s\n", string);
    return 0;
}
 
函数名: strcspn
功 能: 在串中查找第一个给定字符集内容的段
用 法: intstrcspn(char *str1, char *str2);
程序例:
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;alloc.h&gt;
int main(void)
{
    char *string1 = "1234567890";
    char *string2 = "747DC8";
    int length;
    length = strcspn(string1, string2);
    printf("Character where strings intersect is atposition %d\n", length);
    return 0;
}
 
函数名: strdup
功 能: 将串拷贝到新建的位置处
用 法: char*strdup(char *str);
程序例:
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;alloc.h&gt;
int main(void)
{
    char *dup_str, *string = "abcde";
    dup_str = strdup(string);
    printf("%s\n", dup_str);
    free(dup_str);
    return 0;
}
 
函数名: stricmp
功 能: 以大小写不敏感方式比较两个串
用 法: intstricmp(char *str1, char *str2);
程序例:
#include &lt;string.h&gt;
#include &lt;stdio.h&gt;
int main(void)
{
   char *buf1 = "BBB", *buf2 = "bbb";
   int ptr;
   ptr = stricmp(buf2, buf1);
   if (ptr &gt; 0)
      printf("buffer 2 is greater thanbuffer 1\n");
   if (ptr &lt; 0)
      printf("buffer 2 is less than buffer1\n");
   if (ptr == 0)
      printf("buffer 2 equals buffer 1\n");
   return 0;
}
 
函数名: strerror
功 能: 返回指向错误信息字符串的指针
用 法: char*strerror(int errnum);
程序例:
#include &lt;stdio.h&gt;
#include &lt;errno.h&gt;
int main(void)
{
   char *buffer;
   buffer = strerror(errno);
   printf("Error: %s\n", buffer);
   return 0;
}
 
函数名: strcmpi
功 能: 将一个串与另一个比较, 不管大小写
用 法: intstrcmpi(char *str1, char *str2);
程序例:
#include &lt;string.h&gt;
#include &lt;stdio.h&gt;
int main(void)
{
   char *buf1 = "BBB", *buf2 = "bbb";
   int ptr;
   ptr = strcmpi(buf2, buf1);
   if (ptr &gt; 0)
      printf("buffer 2 is greater thanbuffer 1\n");
   if (ptr &lt; 0)
      printf("buffer 2 is less than buffer1\n");
   if (ptr == 0)
      printf("buffer 2 equals buffer1\n");
   return 0;
}
 
函数名: strncmp
功 能: 串比较
用 法: intstrncmp(char *str1, char *str2, int maxlen);
程序例:
#include &lt;string.h&gt;
#include &lt;stdio.h&gt;
int main(void)
{
   char *buf1 = "aaabbb", *buf2 = "bbbccc",*buf3 = "ccc";
   int ptr;
   ptr = strncmp(buf2,buf1,3);
   if (ptr &gt; 0)
      printf("buffer 2 is greater thanbuffer 1\n");
   else
      printf("buffer 2 is less than buffer1\n");
   ptr = strncmp(buf2,buf3,3);
   if (ptr &gt; 0)
      printf("buffer 2 is greater thanbuffer 3\n");
   else
      printf("buffer 2 is less than buffer3\n");
   return(0);
}
 
函数名: strncmpi
功 能: 把串中的一部分与另一串中的一部分比较, 不管大小写
用 法: intstrncmpi(char *str1, char *str2);
程序例:
#include &lt;string.h&gt;
#include &lt;stdio.h&gt;
int main(void)
{
   char *buf1 = "BBBccc", *buf2 = "bbbccc";
   int ptr;
   ptr = strncmpi(buf2,buf1,3);
   if (ptr &gt; 0)
      printf("buffer 2 is greater thanbuffer 1\n");
   if (ptr &lt; 0)
      printf("buffer 2 is less than buffer1\n");
   if (ptr == 0)
      printf("buffer 2 equals buffer1\n");
   return 0;
}
 
函数名: strncpy
功 能: 串拷贝
用 法: char*strncpy(char *destin, char *source, int maxlen);
程序例:
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
int main(void)
{
   char string[10];
   char *str1 = "abcdefghi";
   strncpy(string, str1, 3);
   string[3] = '\0';
   printf("%s\n", string);
   return 0;
}
 
函数名: strnicmp
功 能: 不注重大小写地比较两个串
用 法: intstrnicmp(char *str1, char *str2, unsigned maxlen);
程序例:
#include &lt;string.h&gt;
#include &lt;stdio.h&gt;
int main(void)
{
   char *buf1 = "BBBccc", *buf2 = "bbbccc";
   int ptr;
   ptr = strnicmp(buf2, buf1, 3);
   if (ptr &gt; 0)
      printf("buffer 2 is greater thanbuffer 1\n");
   if (ptr &lt; 0)
      printf("buffer 2 is less than buffer1\n");
   if (ptr == 0)
      printf("buffer 2 equals buffer1\n");
   return 0;
}
 
函数名: strnset
功 能: 将一个串中的所有字符都设为指定字符
用 法: char*strnset(char *str, char ch, unsigned n);
程序例:
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
int main(void)
{
   char *string = "abcdefghijklmnopqrstuvwxyz";
   char letter = 'x';
   printf("string before strnset: %s\n", string);
   strnset(string, letter, 13);
   printf("string after strnset: %s\n", string);
   return 0;
}
 
函数名: strpbrk
功 能: 在串中查找给定字符集中的字符
用 法: char*strpbrk(char *str1, char *str2);
程序例:
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
int main(void)
{
   char *string1 = "abcdefghijklmnopqrstuvwxyz";
   char *string2 = "onm";
   char *ptr;
   ptr = strpbrk(string1, string2);
   if (ptr)
      printf("strpbrk found first character:%c\n", *ptr);
   else
      printf("strpbrk didn't find characterin set\n");
   return 0;
}
 
函数名: strrchr
功 能: 在串中查找指定字符的最后一个出现
用 法: char*strrchr(char *str, char c);
程序例:
#include &lt;string.h&gt;
#include &lt;stdio.h&gt;
int main(void)
{
   char string[15];
   char *ptr, c = 'r';
   strcpy(string, "This is a string");
   ptr = strrchr(string, c);
   if (ptr)
      printf("The character %c is atposition: %d\n", c, ptr-string);
   else
      printf("The character was not found\n");
   return 0;
}
 
函数名: strrev
功 能: 串倒转
用 法: char*strrev(char *str);
程序例:
#include &lt;string.h&gt;
#include &lt;stdio.h&gt;
int main(void)
{
   char *forward = "string";
   printf("Before strrev(): %s\n", forward);
   strrev(forward);
   printf("After strrev(): %s\n", forward);
   return 0;
}
 
函数名: strset
功 能: 将一个串中的所有字符都设为指定字符
用 法: char*strset(char *str, char c);
程序例:
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
int main(void)
{
   char string[10] = "123456789";
   char symbol = 'c';
   printf("Before strset(): %s\n", string);
   strset(string, symbol);
   printf("After strset(): %s\n", string);
   return 0;
}
 
函数名: strspn
功 能: 在串中查找指定字符集的子集的第一次出现
用 法: intstrspn(char *str1, char *str2);
程序例:
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;alloc.h&gt;
int main(void)
{
   char *string1 = "1234567890";
   char *string2 = "123DC8";
   int length;
   length = strspn(string1, string2);
   printf("Character where strings differ is at position%d\n", length);
   return 0;
}
 
函数名: strstr
功 能: 在串中查找指定字符串的第一次出现
用 法: char*strstr(char *str1, char *str2);
程序例:
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
int main(void)
{
   char *str1 = "Borland International", *str2 ="nation", *ptr;
   ptr = strstr(str1, str2);
   printf("The substring is: %s\n", ptr);
   return 0;
}
 
函数名: strtod
功 能: 将字符串转换为double型值
用 法: doublestrtod(char *str, char **endptr);
程序例:
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
int main(void)
{
   char input[80], *endptr;
   double value;
   printf("Enter a floating point number:");
   gets(input);
   value = strtod(input, &amp;endptr);
   printf("The string is %s the number is %lf\n",input, value);
   return 0;
}
 
函数名: strtok
功 能: 查找由在第二个串中指定的分界符分隔开的单词
用 法: char*strtok(char *str1, char *str2);
程序例:
#include &lt;string.h&gt;
#include &lt;stdio.h&gt;
int main(void)
{
   char input[16] = "abc,d";
   char *p;
  
   p = strtok(input, ",");
   if (p)   printf("%s\n", p);
  
   p = strtok(NULL, ",");
   if (p)   printf("%s\n", p);
   return 0;
}
 
函数名: strtol
功 能: 将串转换为长整数
用 法: longstrtol(char *str, char **endptr, int base);
程序例:
#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
int main(void)
{
   char *string = "87654321", *endptr;
   long lnumber;
  
   lnumber = strtol(string, &amp;endptr, 10);
   printf("string = %s long = %ld\n", string,lnumber);
   return 0;
}
 
函数名: strupr
功 能: 将串中的小写字母转换为大写字母
用 法: char*strupr(char *str);
程序例:
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
int main(void)
{
   char *string = "abcdefghijklmnopqrstuvwxyz", *ptr;
  
   ptr = strupr(string);
   printf("%s\n", ptr);
   return 0;
}
 
函数名: swab
功 能: 交换字节
用 法: void swab(char *from, char *to, int nbytes);
程序例:
#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
char source[15] = "rFna koBlrna d";
char target[15];
int main(void)
{
   swab(source, target, strlen(source));
   printf("This is target: %s\n", target);
   return 0;
}
 

    Common：在2.2.0以前的大多数版本中，包含HDFS、MapReduce和其他项目公共内容，从2.2.0开始HDFS和MapReduce被分离为独立的子项目，其余内容为Hadoop
 Common。
    Avro：新的数据序列化格式与传输工具，将逐步取代Hadoop原有的IPC机制。
    MapReduce：并行计算框架，0.20前使用org.apache.hadoop.mapred旧接口，2.2.0版本开始引入org.apache.hadoop.mapreduce的新API。
    HDFS：Hadoop分布式文件系统(Hadoop
 Distributed FileSystem)。
    Pig：大数据分析平台，为用户提供多种接口。
    Hive：数据仓库工具，由Facebook贡献。
    Hbase：类似Google BigTable的分布式NoSQL列数据库。(HBase和Avro已经于2010年5月成为顶级Apache项目)。
    ZooKeeper：分布式锁设施，提供类似Google Chubby的功能，由Facebook贡献。
   Sqoop：Sqoop是一个用来将Hadoop和关系型数据库中的数据相互转移的工具，可以将一个关系型数据库(例如：MySQL,
 Oracle, Postgres等)中的数据导入到Hadoop的HDFS中，也可以将HDFS的数据导入到关系型数据库中。
   Oozie：负责MapReduce作业调度。



       C++精进篇（一）之―数据类型：
                         
                                       


（1）类型修饰符signed和unsigned用于修饰字符型和整形。

（2）类型修饰符short和long用于修饰字符型和整形。

（3）当用signed和unsigned、short和long修饰int整形时，int可省略。当值不可能为负时，使用unsigned类型；

（4）其中bool和wchar_t是C++特有的。 当将非bool的值赋给bool，不为0则返回true，为0则返回false；当将boot的值赋给非bool，为true则返回1，为false则返回0；

（5）除上表以外，C/C++都可以自定义枚举enum、联合union和struct结构体类型。

（6）以上sizeof通过Windows XP 32位平台测试，其中某些类型数据的字节数和数值范围由操作系统和编译平台决定。比如16位机上，sizeof(int)
 = 2，而32位机上sizeof(int) = 4；32位机上sizeof(long) = 4，而64位机上sizeof(long)
 = 8。除此之外，注意64位机上的pointer占8byte。

（7）void的字面意思是“无类型”，不能用来定义变量。void真正发挥的作用在于：&lt;1&gt; 对函数返回和函数参数的限定，例如自定义既不带参数也无返回值的函数void MyFunc(void);&lt;2&gt;定义无类型通用指针void *，指向任何类型的数据。

（8）标准C++库及STL还提供了通用数据结构：字符串类string；向量类模板vector；双端队列类模板deque；链表类模板list；容器适配器堆栈类stack（实现先进后出的操作）；容器适配器队列类queue（实现先进先出的操作）；集合类set；多重集合类multiset；映射类map；多重映射类multimap；位集合bitset；迭代器iterator
 (类似指针的功能,对容器的内容进行访问)。

（9）在标准c++中，int的定义长度要依靠你的机器的字长，也就是说，如果你的机器是32位的，int的长度为32位，如果你的机器是64位的，那么int的标准长度就是64位，而vc中__int64是为在32机位机器长实现64位长度的整形数。

（10）关于32位平台下的int和long

long从字面上看，应该是64位才更合理，把long当成32位实在是一个历史的包袱。像C#那样新起炉灶的程序语言，由于没有需要支持老代码的问题，就把long当作64位来处理了。

在32位平台下，long是相对short而言，long（short）类型是long（short） int类型的简称，sizeof(long)
 = sizeof(int) = 4。int和long的范围虽然一样,但输入输出格式不同,printf int的格式为%d，而printf
 long的格式为%ld。

考虑到程序的可移植性，还是要将他们区分开来。但当要求的数值范围为4byte时，建议使用int类型，因为第一版的C语言只有一种类型，那就是int。不要使用char或bool做运算，仅仅使用它们存放字符或真值（char的符号根据架构的不同而不同）；

（11）在Win32 API及MFC中为了使类型名称在语意上更明了，对以上基本类型进行了大量的typedef。例如WINDEF.H中的BYTE,WORD,DWORD。

（12）将float赋给int，值会被截断，仅仅保留小数点之前的整数；如果将值赋给小于它位数的无符号类型，返回取模的结果，例如将258赋给unsighed
 char，则返回2；如果将值赋给小于它位数的有符号类型，返回结果不确定。不要混合signed和unsigned来运算。

（13）使用double做浮点数，一般来说，float的精度不够，且它们的运算量差别不大，甚至在有的架构上，double的运算速度还快于float，long
 double的开销很大，能不用就别用了。

（14）空指针请使用nullptr，不要使用NULL。

标识符的命名惯例：
 （1）名称最好能够指示它的实际意义；
 （2）变量名称一般小写；
 （3）类名称一般首字母大写；
 （4）使用下划线或者大写来隔开字母。



       Java语言是一种强类型的语言，对各种数据类型都有明确的区分，而计算机使用内存来记忆大量运算时需要使用的数据，而当声明一个变量时，即在内存中划分一块空间存储数据，而变量类型决定划分内存空间的大小。
      Java中分为基本数据类型及引用数据类型：
                                         

       Java基本数据类型：
                                         
byte：java中最小的数据类型，在内存中占8位(bit)，即1个字节，取值范围-128~127
short：短整型，在内存中占2个字节，取值范围-32768~32717
int：整型，用于存储整数，在内在中占4个字节，取值范围-2147483648~2147483647
long：长整型，在内存中占8个字节
float：浮点型，在内存中占4个字节，用于存储带小数点的数字（与double的区别在于float类型有效小数点只有6~7位）
double：双精度浮点型，用于存储带有小数点的数字，在内存中占8个字节
char：字符型，用于存储单个字符，占2个字节
boolean：布尔类型，占8个字节，用于判断真或假（仅有两个值，即true、false）
引用数据类型：类，数据接口，数组
String：字符串型，用于存储一串字符
Java变量声明及使用：
数据类型变量名 = 值、表达式；
例：String name = "柯南";
    int a = 50;
注：“=”并不是数学中的“等号”，而是一个赋值运算符
Java变量命名规则：
1：必须以字母、下划线“_”、或“$”符号开头
2：可以包括数字、区分大小写
3：不能使用Java语言的关键字，例如int、class、public等
Java中的六种运算符：
算术运算符
赋值运算符
关系运算符
逻辑运算符
位运算符
三元运算符
算术运算符：
+：加法运算，求操作数的和
-：减法运算，求操作数的差
*：乘法运算，求操作数的乘积
/：除法运算，求操作数的商
%：求余运算，求操作数相除的余数
++：自增，操作数自加1
--：自减，操作数自减1

赋值运算符：
=:将右边的值赋给左边，例：int a =1;
+=:左右两边的和相加赋给左边，例：int a= 1; a+=2;结果a的值为3
-=：左边减去右边的差赋给左边，例：int a=5;a-=2;结果a的值为3
*=：两边数值相乘的值赋给左边，例：int a= 2;a*=2;结果a的值为4
/=：左边除以右边的值赋给左边，例：int a= 6;a/=2;结果a的值为3
%=：左边除以右边的余数赋给左边，例：inta =7;a%=2;结果a的值为1

关系运算符：
&gt;：大于，例：int a = 1;int b= 2;System.out.print(a &gt; b);其结果为false
&lt;：小于,例：int a =1;int b = 2;System.out.print(a &lt; b);其结果为true
&gt;=：大于等于,例：int a =1;int b = 2;System.out.print(a &gt;= b);其结果为false
&lt;=：小于等于,例：int a =1;int b = 2;System.out.print(a &lt;= b);其结果为true
==：等于,例：int a =1;int b = 2;System.out.print(a == b);其结果为false
!=：不等于,例：int a =1;int b = 2;System.out.print(a != b);其结果为true
  其结果都是boolean类型，即要么是true要么是false
逻辑运算符:
&amp;&amp;：与、并且(短路)，   两个条件同时为真时，结果为真
||：或、或者(短路)，   两个条件有一个为真时，结果即为真
!：非，(!+条件)  条件为真时，结果为假
Java中的数据类型转换：
1：自动数据类型转换（放大转换）
满足自动数据类型转换条件：
1）两种类型要兼容：如数值类型（整型和浮点型）
2）目标类型大于源类型：例如int型数据可以自动转换为double类型
2：强制数据类型转换（缩小转换）
在变量前加上括号，在括号中指定要强制转换的类型
例：double a = 20.9;
    int b = (int)a;
注：强制转换会损失数值精度，例如double类型变量a，经强制转换为int类型后值变为20
 




     JDK、JRE、JVM之间的关系：

     首先看看JDK与JRE的区别与联系，如下图所示：

                      

   由图可知： JDK = JRE + Tools&amp;Tool APIs



   JDK的核心是Java SE API。Java SE API是一些预定义的类库，开发人员需要用这些类来访问Java语言的功能。Java SE API包括一些重要的语言结构以及基本图形，网络和文件I/O。我们在自己的程序中，调用前辈们写好的这些Class，来作为我们自己开发的一个基础。当然，现在已经有越来越多的性能更好或者功能更强大的第三方类库供我们使用。一般来说，    Java API的非I/O部分对于运行Java的所有平台是相同的，而I/O部分则仅在通用Java环境中实现。

  JDK的种类：最主流的JDK是Sun公司发布的JDK，除了Sun之外，还有很多公司和组织都开发了自己的JDK，例如IBM公司开发的JDK，BEA公司的Jrocket，还有GNU组织开发的JDK等等。其中IBM的JDK包含的JVM（Java Virtual Machine）运行效率要比Sun JDK包含的JVM高出许多。而专门运行在x86平台的Jrocket在服务端运行效率也要比Sun JDK好很多。

  JRE（Java RuntimeEnvironment，Java运行环境），运行JAVA程序所必须的环境的集合。J2RE是Java2 Runtime Environment，即Java运行环境，有时简称JRE.如果你只需要运行Java程序或Applet，下载并安装它即可。如果你要自行开发 Java软件，请下载JDK. 在JDK中附带有JRE.注意由于Microsoft对Java的支持不完全，请不要使用IE自带的虚拟机来运行 Applet，务必安装一个JRE或JDK.包含：

  1. Java Runtime Environment（JRE）是可以在其上运行、测试和传输应用程序的Java平台。它包括Java JVM标准实现、Java平台核心类库和支持文件。它不包含开发工具——编译器、调试器和其它工具。

  JVM：Java Virtual Mechinal(JAVA虚拟机)。JVM是JRE的一部分，它是一个虚构出来的计算机，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。JVM有自己完善的硬件架构，如处理器、堆栈、寄存器等，还具有相应的指令系统。JVM 的主要工作是解释自己的指令集（即字节码）并映射到本地的 CPU 的指令集或 OS 的系统调用。Java语言是跨平台运行的，其实就是不同的操作系统，使用不同的JVM映射规则，让其与操作系统无关，完成了跨平台性。JVM 对上层的 Java 源文件是不关心的，它关注的只是由源文件生成的类文件（
 class file ）。类文件的组成包括 JVM 指令集，符号表以及一些补助信息。

  2. JRE所需辅助软件——Java Plug-in——以便在浏览器中运行applet.
     一般情况下，如果您安装了JDK，那么你的电脑地下一定会有两套JRE、一套位于&lt;jdk安装目录&gt;\jre底下，另一套位于C:\Program File\JavaSoft底下（JDK1.4则是放在C:\Program File\Java底下）。如果你只下载了JRE而非JDK，那么就只会在C:\Program File\JavaSoft底下安装唯一的一套JRE。

    由图可知JRE的地位就像一台PC一样，WIN32程序需要作业系统来帮我们执行，同样，Java应用程序需要JRE才能帮我们执行。当安装完JDK之后，如果分别在硬盘的不同地方安装了两套JRE，那么你可以想象你的电脑有两台虚拟的Java PC，都具有执行Java程序的功能。所以Java虚拟机只是JRE其中的一个成员而已，以更技术的角度来看，Java虚拟机只是JRE里头的一个动态连结函数库。

    那么为什么会有两套JRE？真正原因是-JDK里面也附上了很多用Java写的开发工具（例如javac.exe、jar.exe等），而且它们都放置在&lt;jdk安装目录\lib\tools.jar&gt;之中。有人会问用Java编写的应用程序不是.class才对吗？其实在命令提示符中输入javac.exe和输入java -classpath

    d:\j2sdk1.4.0\lib\tools.jar com.sun.tools.javac.Main会得到相同的结果，javac.exe只是一个包装器而已。JDK还有很多开发工具采用包装器的概念，在&lt;jdk安装目录\bin&gt;中可以看到。

    JDK里面的工具几乎都是用Java所撰写的，所以JDK本身就是Java应用程序，因此要使用JDK附的工具来开发Java程序，也必需要自行附一套JRE才行，这就是&lt;jdk安装目录&gt;\jre底下需要一套JRE的原因。而位于Program File\底下那套JRE就是哪来执行我们自己撰写的Java应用程序。不过，两套JRE都可以用来执行我们自己写的程序，可以JDK内附的开发工具预设置使用包装器启动的情况下，都会自己去选择用&lt;jdk安装目录&gt;\jre底下的那套JRE。

   这样又有新的问题了，像myeclipse等开发工具也有集成JRE，那么电脑就有很多套JRE，同样执行java.exe有可能会得到不同的结果。

     那么我们执行的是那一个java.exe？

     当我们输入java XXX的时候，java.exe的工作就是找到合适的JRE来执行。Java.exe依照下面的逻辑来寻找JRE：

（1）自己的目录下有没有JRE目录

（2）父目录底下JRE子目录

（3）查询windows registry(HKEY_LOCAL_MACHINE\Software\JavaSoft\Java\Runtime Environment\)

      我们可以用path来指定执行哪个java.exe。当我们指定了path=d:\j2skd1.4.0\bin是，执行java.exe一定是d:\j2skd1.4.0\bin底下的java.exe，所以java.exe选到的是父目录d:\j2skd1.4.0底下的JRE。打开d:\j2skd1.4.0\jre\bin这个目录，您会看到client和server两个目录，里面都会分别看到jvm.dll，这就是Java虚拟机所在。

    我们开发Java应用程序或是执行Java程序的时候，一定要记住两件事：

（1）那一个java.exe被执行。

（2）java.exe找到哪一套JRE。


附：


    JDK目录结构
    安装OS：win7 x86 
    安装路径： C:/Program Files/Java 
    JDK版本：jdk1.7.0_21



//---------------------------JDK开发文件和目录------------------------------------------------
              jdk1.7.0_21
     ___________|_______________________   
   |          |                     |   
    bin        lib                   jre   
   |          |            __________|_______________________   
java.exe    tools.jar       |                                 |   &amp;nbsp;       
javac.exe  dt.jar          bin                               lib                 
javap.exe           _________|____ __________        __________|_______ ________________           
javah.exe          |             |         |       |          |      |       |        |   
javadoc.exe    java.exe       client      server rt.jar      ext  security applet    fonts   
               java.dll          |          |  charsets.jar   |       &amp;nbsp;             
               awt.dll        jvm.dll   jvm.dll          localedata.jar 
C:/Program Files/Java/jdk1.7.0_21 --JDK的根目录，包含一些软件版权，声明，和自述文件，同时包含归档了的Java平台源代码包src.zip      
C:/Program Files/Java/jdk1.7.0_21/src.zip --归档的Java源代码 
C:/Program Files/Java/jdk1.7.0_21/include --C 语言头文件 支持 用Java本地接口和Java虚拟机接口 来本机代码编程
C:/Program Files/Java/jdk1.7.0_21/lib -- Java开发工具要用的一些库文件，有包含了支持JDK工具的非核心类库tool.jar，dt.jar 归档的 BeanInfo 文件,用于告诉IDE这样显示java组件怎样让开发者在自己的应用程序中用户化它们 
================C:/ProgramFiles/Java/jdk1.7.0_21/jre================
C:/Program Files/Java/jdk1.7.0_21/jre -- JDK使用的Java运行环境（JRE）的根目录，这个运行环境实现了Java平台          
C:/Program Files/Java/jdk1.7.0_21/jre/bin --Java平台所要用的工具和库的可执行文件这些可执行文件和 /jdk1.7.0_21/bin相同的。这个路径不需要设置 PATH 环境变量 //Java 启动器工具充当了应用程序启动器(覆盖了1.1版本的JDK推出的旧版本JRE工具)
C:/Program Files/Java/jdk1.7.0_21/jre/bin/client -- 包含Java Hotspot(Java性能引擎) Client Virtual Machine 客户虚拟机要用的DLL文件 
C:/Program Files/Java/jdk1.7.0_21/jre/bin/server --包含Java Hotspot(Java性能引擎) Server Virtual Machine 服务器虚拟机要用的DLL文件 ----JDK 比 JREC:/Program Files/Java/jre7/bin多一个server端的java虚拟机。即这个folder “Server”不存在于JRE下。
C:/Program Files/Java/jdk1.7.0_21/jre/lib --JRE要用的代码库，属性设置，资源文件。 
C:/Program Files/Java/jdk1.7.0_21/jre/lib/rt.jar --Java 引导类库(java 核心APIRunTime类) 
C:/Program Files/Java/jdk1.7.0_21/jre/lib/charsets.jar --字符转换类库 
C:/Program Files/Java/jdk1.7.0_21/jre/lib/ext --默认的Java平台扩展安装环境 
C:/Program Files/Java/jdk1.7.0_21/jre/lib/ext/localedata.jar -- ava.text 和 java.util包要用到的地区数据  
C:/Program Files/Java/jdk1.7.0_21/jre/lib/security --包含安全管理文件，有安全规则(java.policy)和安全属性文件(java.security) 
C:/Program Files/Java/jdk1.7.0_21/jre/lib/applet --Java applets 要的Jar包，可以放到lib/applet/目录,可以节省 applet 类装载器从本地文件系统装载大的applets 所需的applet类时间,减少从网上下载具有相同的保护的时间。 
C:/Program Files/Java/jdk1.7.0_21/jre/lib/fonts --包含平台所需的TrueType字体文件 
================C:/ProgramFiles/Java/jdk1.7.0_21/db================
C:/Program Files/Java/jdk1.7.0_21/db  --db目录,纯Java开发的数据可 Derby，是一个开源的100%Java开发的关系数据库 
        有关 Java DB 的信息，请参见http://developers.sun.com/prodtech/javadb/。
        有关 Derby 的文档，请参见：http://db.apache.org/derby/manuals/index.html 
================C:/ProgramFiles/Java/jdk1.7.0_21/bin================
C:/Program Files/Java/jdk1.7.0_21/bin --JDK包含的一些开发工具执行文件 
C:/Program Files/Java/jdk1.7.0_21/bin/javac.exe --基本工具 - Java语言编译器，将Java源代码转换成字节码
C:/Program Files/Java/jdk1.7.0_21/bin/java.exe -- 基本工具 - Java应用程序启动器，直接从类文件执行Java应用程序字节代码
C:/Program Files/Java/jdk1.7.0_21/bin/javadoc.exe -- 基本工具 - Java API 文档生成器,从源码注释中提取文档
C:/Program Files/Java/jdk1.7.0_21/bin/apt.exe -- 基本工具 - java 注释处理器 
C:/Program Files/Java/jdk1.7.0_21/bin/appletviewer.exe -- 基本工具 - java applet 小程序浏览器，一种执行HTML文件上的Java小程序的Java浏览器。
C:/Program Files/Java/jdk1.7.0_21/bin/jar.exe -- 基本工具 - java文件压缩打包工具 
C:/Program Files/Java/jdk1.7.0_21/bin/jdb.exe -- 基本工具 - Java 调试器，debugger，查错工具
C:/Program Files/Java/jdk1.7.0_21/bin/javah.exe -- 基本工具 - C 头文件和stub生成器，用于写本地化方法，例如生产JNI样式的头文件。产生可以调用Java过程的C过程，或建立能被Java程序调用的C过程的头文件
C:/Program Files/Java/jdk1.7.0_21/bin/javap.exe -- 基本工具 - class文件反编译工具，显示编译类文件中的可访问功能和数据，同时显示字节代码含义。
C:/Program Files/Java/jdk1.7.0_21/bin/extcheck.exe -- 基本工具 - 用于检测jar包中的问题 
 
C:/Program Files/Java/jdk1.7.0_21/bin/keytool.exe-- 安全工具 - 管理密钥库和证书. 
C:/Program Files/Java/jdk1.7.0_21/bin/jarsigner.exe-- 安全工具 - 生产和校验JAR签名 
C:/Program Files/Java/jdk1.7.0_21/bin/policytool.exe-- 安全工具 - 有用户界面的规则管理工具   
C:/Program Files/Java/jdk1.7.0_21/bin/kinit.exe.exe-- 安全工具 - 用于获得和缓存网络认证协议Kerberos 票证的授予票证 
C:/Program Files/Java/jdk1.7.0_21/bin/klist.exe.exe-- 安全工具 - 凭据高速缓存和密钥表中的 Kerberos 显示条目  
C:/Program Files/Java/jdk1.7.0_21/bin/ktab.exe.exe-- 安全工具 - 密钥和证书管理工具  
C:/Program Files/Java/jdk1.7.0_21/bin/native2ascii.exe-- Java国际化工具 - 将文本转化为 Unicode Latin-1。详情参考http://java.sun.com/javase/6/docs/technotes/tools/windows/native2ascii.html 
C:/Program Files/Java/jdk1.7.0_21/bin/rmic.exe-- 远程方法调用工具 - 生成远程对象的stubs and skeletons(存根和框架) 
C:/Program Files/Java/jdk1.7.0_21/bin/rmid.exe-- 远程方法调用工具 - Java 远程方法调用(RMI:Remote Method Invocation) 活化系统守护进程 
C:/Program Files/Java/jdk1.7.0_21/bin/rmiregistry.exe-- 远程方法调用工具 - Java 远程对象注册表 
C:/Program Files/Java/jdk1.7.0_21/bin/serialver.exe-- 远程方法调用工具 - 返回类的 serialVersionUID.
C:/Program Files/Java/jdk1.7.0_21/bin/tnameserv.exe-- Java IDL and RMI-IIOP 工具 - Provides access to the naming service.  
C:/Program Files/Java/jdk1.7.0_21/bin/idlj.exe-- Java IDL and RMI-IIOP 工具 - 生产映射到OMG IDL接口可以使Java应用程序使用CORBA的.java文件 
C:/Program Files/Java/jdk1.7.0_21/bin/orbd.exe-- Java IDL and RMI-IIOP 工具 - 为客户可以在CORBA环境下透明的定位和调用服务器的稳定的对象提供支持 
C:/Program Files/Java/jdk1.7.0_21/bin/servertool.exe-- Java IDL and RMI-IIOP 工具 - 为应用程序提供易于使用的接口用于注册，注销，启动，关闭服务器 
C:/Program Files/Java/jdk1.7.0_21/bin/pack200.exe-- Java 部署工具 - 使用java gzip压缩工具将JAR文件转换为压缩的pack200文件，生产打包文件是高度压缩的JAR包，可以直接部署，减少下载时间 
C:/Program Files/Java/jdk1.7.0_21/bin/unpack200.exe-- Java 部署工具 - 解包pack200文件为JARs  
C:/Program Files/Java/jdk1.7.0_21/bin/htmlconverter.exe-- Java 插件工具 - Java Plug-in HTML转换器 htmlconverter -gui 可以启动图形界面 
C:/Program Files/Java/jdk1.7.0_21/bin/javaws.exe-- Java web 启动工具 - Java web 启动命令行工具 
C:/Program Files/Java/jdk1.7.0_21/bin/jvisualvm.exe-- Java 故障检修，程序概要分析，监视和管理工具 - 一个图形化的Java虚拟机，不说了大家研究一下就发现太酷了.这是想了解JVM的人的神器http://java.sun.com/javase/6/docs/technotes/guides/visualvm/index.html
C:/Program Files/Java/jdk1.7.0_21/bin/jconsole.exe-- Java 故障检修，程序概要分析，监视和管理工具 -java监视台和管理控制台，图形界面的功能太强大了，运行一下就知道，不想多说，看了就知道 
C:/Program Files/Java/jdk1.7.0_21/bin/schemagen.exe -- Java web 服务工具 - Java构架的XML Schema生成器 
C:/Program Files/Java/jdk1.7.0_21/bin/wsgen.exe -- Java web 服务工具 - 生成 JAX-WS 
C:/Program Files/Java/jdk1.7.0_21/bin/wsimport.exe-- Java web 服务工具 - 生成 JAX-WS 
C:/Program Files/Java/jdk1.7.0_21/bin/xjc.exe-- Java web 服务工具 - 绑定编译器  
C:/Program Files/Java/jdk1.7.0_21/bin/jps.exe-- 监视工具 - JVM Process Status 进程状态工具。列出目标系统的HotSpot JJVM , 监视Java虚拟机的性能，不支持Windows 98 和Windows ME 平台 
C:/Program Files/Java/jdk1.7.0_21/bin/jstat.exe-- 监视工具 - 按照命令行的具体要求记录和收集一个JVM的性能数据 
C:/Program Files/Java/jdk1.7.0_21/bin/jstatd.exe-- 监视工具 - JVM jstat 的守护进程 
C:/Program Files/Java/jdk1.7.0_21/bin/jinfo.exe-- 故障检测和修理工具 - 配置或打印某个Java进程VM flag 
C:/Program Files/Java/jdk1.7.0_21/bin/jhat.exe-- 故障检测和修理工具 - 堆储存查看器 
C:/Program Files/Java/jdk1.7.0_21/bin/jmap.exe -- 故障检测和修理工具 - Java内存图 
C:/Program Files/Java/jdk1.7.0_21/bin/jsadebugd.exe-- 故障检测和修理工具 - Java 的 Serviceability Agent Debug的守护进程 
C:/Program Files/Java/jdk1.7.0_21/bin/jstack.exe-- 故障检测和修理工具 - Java堆栈跟踪 
C:/Program Files/Java/jdk1.7.0_21/bin/jrunscript.exe-- Java脚本工具 - 运行脚本 
 



虽然对如何编写程序没有严格的规定， 但大多数程序员都采用类似的流程。

该程序开发流程如下。

1. 确定程序要做什么，即搞清楚需求。

2. 编写源代码，这里是使用Python 集成开发环境IDLE 或其他文本编辑器编写Python 代码。这一步通常最有趣也最具挑战性，要求你创造性地解决问题。Python 源代码文件使用扩展名.py，如web.py、urlexpand.py、clean.py 等。

3. 使用Python 解释器将源代码转换为目标代码。Python 将目标代码存储在.pyc 文件中， 例如，如果源代码存储在文件urlexpand.py 中， 目标代码将存储在文件urlexpand.pyc 中。

4. 运行或执行程序。就Python 而言，通常紧接着第2 步自动完成这一步。实际上， Python 程序员很少直接与目标代码（.pyc 文件）交互。

5. 最后，检查程序的输出。如果发现错误， 回到第2 步并尽力修复错误。修复错误的过程称为调试。开发庞大或复杂的程序时，可能大部分时间都用在调试上，因此经验丰富的程序员设计程序时，会尽力采用可最大限度地减少调试时间的方式。

如图1-1 所示，这是个循环往复的过程： 编写程序，测试，修复错误，再测试……直到程序正确运行。



               



虽然Python 是一种通用语言，可用于编写任何类型的程序，但它最常用于编写下述应用程序。

脚本。这些简短的程序自动执行常见的管理任务，如在系统中新增用户、将文件上传到网站、在不使用浏览器的情况下下载网页等。

网站开发。作为快速创建动态网站的工具，Django（www.djangoproject.com）、Bottle（www.bottlepy.org）和Zope（www. zope.org）等众多Python
 项目深受开发人员的欢迎。例如，深受欢迎的新闻网站www.reddit.com 就是使用Python 开发的。

文本处理。Python 在字符串和文本文件处理方面提供了强大的支持，包括正则表达式和Unicode。

科学计算。网上有很多卓越的Python 科学计算库，提供了用于统计、数学计算和绘图的函数。

教育。鉴于Python 简洁实用，越来越多的学校将其作为第一门编程教学语言。

当然，Python 并非对任何项目来说都是最佳选择，其速度通常比Java、C#、C++ 等语 言慢，因此开发新操作系统时不会使用Python。

然而，需要最大限度地减少程序员花在项目上的时间时，Python 通常是最佳选择。



RPC简介：
        RPC(Remote Procedure Call）远程过程调用，它允许一台计算机程序远程调用另外一台计算机的子程序，而不用去关心底层的网络通信细节，对我们来说是透明的。经常用于分布式网络通信中。
       Hadoop的进程间交互都是通过RPC来进行的，比如Namenode与Datanode之间，Jobtracker与Tasktracker之间等。
       RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中， RPC跨越了传输层和应用层。 RPC使得开发包括网络分布式多程序在内的应用程序更加容易。  
       RPC采用客户机/服务器模式。请求程序就是一个客户机，而服务提供程序就是一个服务器。
       首先，客户机调用进程发送一个有进程参数的调用信息到服务进程，然后等待应答信息，在服务器端，进程保持睡眠状态直到调用信息的到达为止。当一个调用信息到达，服务器获得进程参数，计算结果，发送答复信息给client，然后等待下一个调用信息，最后，客户端调用进程接收答复信息，获得进程结果，然后调用执行继续进行。
RPC特点:
（1）透明性：远程调用其他机器上的程序，对用户来说就像是调用本地方法一样。
（2）高性能：RPC server能够并发处理多个来自Client的请求（请求队列）。3、可控性：jdk中已经提供了一个RPC框架–RMI，但是该RPC框架过于重量级并且可控之处比较少，所以               HadoopRPC实现了自定义的RPC框架。
Hadoop RPC通信:
（1）序列化层：Client与Server端通信传递的信息采用了Hadoop里提供的序列化类或自定义Writable类型。
（2）函数调用层：Hadoop RPC通过动态代理以及Java反射机制实现函数调用。
（3）网络传输层：Hadoop RPC采用了基于TCP/IP的socket机制。
（4）服务器端框架层：RPC Server利用Java NIO以及采用了事件驱动的I/O模型，提高RPC Server的并发处理能力。
Hadoop的整个体系结构就是构建在RPC之上(org.apache.hadoop.ipc)。
Hadoop RPC设计技术
（1）动态代理
（2）反射3、序列化4、非阻塞的异步IO（NIO）
动态代理:
（1）动态代理可以提供对另一个对象的访问，同时隐藏实际对象的具体事实，代理对象对客户隐藏了实际对象。
（2）动态代理可以对请求进行其他的一些处理，在不允许直接访问某些类，或需要对访问做一些特殊处理等，这时候可以考虑使用代理。3）目前Java开发包中提供了对动态代理的支持，但现在只支持对接口的实现。相关的类与接口：java.lang.reflect.Proxy--类java.lang.reflect.InvocationHandler--接口
动态代理创建对象过程：
InvocationHandler handler= new InvocationHandlerImpl(...) Proxy.newInstance(...)
具体实现可参考如下：
                 
根据上图查看hadoop2.6.0源码
Client:

Server:

RPC:

几个重要的协议:
（1）ClientProtocol是客户端(FileSystem)与NameNode通信的接口。
（2）DatanodeProtocol是DataNode与NameNode通信的接口NamenodeProtocol是SecondaryNameNode与NameNode通信的接口。
（3）DFSClient是直接调用NameNode接口的对象。用户代码是通过DistributedFileSystem调用DFSClient对象，才能与NameNode打交道。




模拟Hadoop RPC通信
package MyRPC; 
import org.apache.hadoop.io.Text; 
import org.apache.hadoop.ipc.VersionedProtocol; 
 
public interface MyRPCProtocal extends VersionedProtocol{ 
    public static long versionID = 23234l;//很重要很重要，搞了一下午才解决掉。 
    public Text test(Text t); 
} 
package MyRPC; 
 
import java.io.IOException; 
import org.apache.hadoop.conf.Configuration; 
import org.apache.hadoop.io.Text; 
import org.apache.hadoop.ipc.ProtocolSignature; 
import org.apache.hadoop.ipc.RPC; 
import org.apache.hadoop.ipc.RPC.Server; 
 
public class RPCServer implements MyRPCProtocal{     
    Server server = null; 
    public RPCServer() throws IOException, InterruptedException{ 
        //server = RPC.getServer(this,"localhost",8888,new Configuration()); 
        //相对于以前的版本有略微的改动 
        RPC.Builder ins = new RPC.Builder(new Configuration()); 
        ins.setInstance(this); 
        ins.setBindAddress("localhost"); 
        ins.setPort(9999); 
        ins.setProtocol(MyRPCProtocal.class); 
        //RPC.setProtocolEngine(new Configuration(), MyRPCProtocal.class, RpcEngine.class); 
        server = ins.build();//获得一个server实例 
        server.start(); 
        server.join();   
    } 
 
    public static void main(String[] args) throws IOException, InterruptedException { 
        new RPCServer(); 
    } 
 
    @Override 
    public long getProtocolVersion(String protocol, long clientVersion) 
            throws IOException { 
        return MyRPCProtocal.versionID; 
    } 
 
    @Override 
    public ProtocolSignature getProtocolSignature(String protocol, 
            long clientVersion, int clientMethodsHash) throws IOException {      
        return new ProtocolSignature(); 
    } 
 
    @Override 
    public Text test(Text t) { 
        if(t.toString().equals("RPC")){ 
            return new Text("ok"); 
        } 
        return new Text("false"); 
    } 
} 
package MyRPC; 
 
import java.net.InetSocketAddress; 
 
import org.apache.hadoop.conf.Configuration; 
import org.apache.hadoop.io.Text; 
import org.apache.hadoop.ipc.RPC; 
 
public class RPCClient { 
 
    private MyRPCProtocal protocal; 
 
    public RPCClient() throws Exception{ 
        InetSocketAddress address = new InetSocketAddress("localhost",9999); 
 
        protocal = (MyRPCProtocal)RPC.waitForProxy 
                (MyRPCProtocal.class,MyRPCProtocal.versionID, address, new Configuration()); 
        //RPC.setProtocolEngine(new Configuration(), MyRPCProtocal.class, RpcEngine.class); 
    } 
 
    public void call(String s){ 
        final Text string = protocal.test(new Text(s)); 
        System.out.println(string.toString()); 
    } 
 
    public static void main(String[] args) throws Exception { 
        RPCClient client = new RPCClient(); 
        client.call("RPC"); 
    } 
} 
 





Java基础, Java平台版本, JDK环境搭建, 编译运行Java HelloWorld程序


JDK是Java Development Kit的缩写，即Java开发工具集。JDK是整个Java的核心，包括了Java运行环境（JRE）、Java开发工具和Java基础类库。
JRE是Java Runtime Environment的缩写，即Java运行时环境。Java程序运行时必需要JRE的支持。

Java的平台版本：
Java分为J2SE、J2EE和J2ME三个不同的平台版本，即标准版（Java 2 Platform, Standard Edition）、企业版（Java 2 Platform, Enterprise Edition）和微型版（Java 2 Platform, Micro Edition）。从Java 1.5（或者Java 5.0）开始，它们被改称为Java SE、Java EE和Java ME。

各平台版本之间的差别在于适用范围不同：


标准版平台（Java SE）允许开发和部署在桌面、服务器、嵌入式和实时环境中使用的Java应用程序。另外，Java SE包含了支持实现Java Web服务的类库，因此标准版是企业版的基础。

企业版平台（Java EE）帮助开发者开发和部署Java Web应用。企业版提供Web服务、组件模型、管理和通信API，可以用来实现企业级的面向服务的体系结构(Service-Oriented Architecture, SOA)和Web 2.0 应用程序。

而微型版平台（Java ME）主要用于移动设备和嵌入式设备，如手机、PDA、电视机顶盒等等。微型版包括灵活的用户界面、健壮的安全模型、许多内置的网络协议、以及对可动态下载的在线和离线应用的支持。基于Java ME规范的应用程序与普通Java程序一样，只需编译一次，即可在许多设备上运行。

Java的版本：


1995年5月23日，Java语言诞生1996年1月，JDK 1.0发布1997年2月18日，JDK 1.1发布1998年12月4日，J2SE 1.2发布，1.2版及其之后的版本也被称为Java 21998年12月8日，Java 2企业版发布2000年5月8日，J2SE 1.3发布2002年2月13日，J2SE 1.4发布2004年9月30日，J2SE 1.5发布，J2SE 1.5版也被称为Java 5.02006年12月，Java 6.0发布2010年9月，Java 7.0发布2011年7月28日，甲骨文发布Java 7.0的正式版；2014年3月19日，甲骨文公司发布Java 8.0的正式版。2014年11月甲骨文公司发布了Java 9.0的新特性，比较重要的的有：

统一的JVM日志支持HTTP 2.0Unicode 7.0支持安全数据包传输（DTLS）支持Linux/AArch64支持

Java开发与运行环境的搭建（Java SE）：

1. 下载JDK/JRE：

首先，访问Oracle公司的Java SE的下载主页 (http://www.oracle.com/technetwork/java/javase/downloads/index.html)，选择一个版本（目前最新版为Java
 SE 7），如下图：







此页面包含多个版本的JDK、JRE、帮助文档、源代码等下载内容的链接。如果不是Java程序的开发者，仅仅想在自己的系统中运行Java程序，那么只需要一个JRE就可以了；如果想使用Java开发自己的应用程序，则需要下载JDK，其中已包含JRE，因此下载了JDK后无需再单独下载JRE。

这里以下载Java SE 7的JDK为例，点击相应的Download按钮，转到下载页面：






在此页面中，包含了对应各种操作系统的JDK下载链接，选择自己系统对应的JDK，将其下载到本地硬盘上。注意，在下载之前需要先阅读“Oracle Binary Code License Agreement for Java SE”，必须接受其中的条款才能下载JDK（选中“Accept License Agreement”）。

2. 安装JDK/JRE:

无论是在Windows还是在Linux下安装JDK都很简单，与安装其他程序没什么不同。（因为我没有其他操作系统的环境，也没用过其他系统，因此不清楚在其他操作系统下的安装方法，但想来应该也不是难事——至少不会比安装其他程序难）。

在Windows中，双击刚才下载的“jdk-7-windows-i586.exe”文件，就会打开安装界面。点击“下一步”按钮，可以在此选择需要安装的组件和安装目录，窗口右侧是对所选组件的说明，包括组件功能和所需的磁盘空间；可以点击“更改”按钮来改变安装目录。点击“下一步”即开始正式安装。安装完毕后，将会显示安装已完成的信息，点击“完成”按钮即可完成安装。

来到安装文件夹下，即可以看到已安装的JDK的目录结构。（注意其中包含名为“jre”的文件夹，这就是前面说的JDK包含JRE的原因所在）

整个安装过程如下面几幅图所示：



  




  


注意：操作系统分为32位操作系统和64位操作系统，对应地，JDK也分为32位版和64位版（名称中带有“i586”或“x86”的为32位版，带有“x64”则表示该JDK为64位版）。64位版JDK只能安装在64位操作系统上，32位版JDK则既可以安装在32位操作系统上，也可以安装在64位操作系统上。原因是64位的操作系统能够兼容32位的应用程序。
换句话说，即使CPU是64位的，但如果安装的操作系统是32位的，那么也无法安装64位版的JDK。

在Linux中安装JDK与安装其他程序相同。下载时可以选择.rpm或.tar.gz格式的安装文件，这里以后者为例进行说明。

首先解压缩下载的文件，输入命令“tar -xf jdk-7-linux-i586.tar.gz -C /usr”，将文件解压到/usr目录下，这样就完成了安装（如图）：







3. 设置环境变量：

环境变量是指在操作系统中用来指定操作系统运行环境的一些参数，比如临时文件夹位置和系统文件夹位置等。环境变量相当于给系统或应用程序设置的一些参数。

编译或运行Java程序时，都是基于命令行的，因此在此之前必须设置一些环境变量的值。有些Java IDE（集成开发环境）内置了JDK，因此使用这些IDE时可以不指定环境变量。还有些程序需要个性化的环境变量（如Apache Tomcat需要JAVA_HOME环境变量）。

与JDK或JRE的使用有关的是PATH、CLASSPATH等几个环境变量。这里先解释一下这些变量的含义：

PATH变量用来告诉操作系统到哪里去查找一个命令。如果清空PATH变量的值，在Windows中运行一个外部命令时，将提示未知命令错误（当然，在Linux中也是一样）：






注意：在Windows中，如“dir”、“cd”等命令是内部命令，类似于DOS中的常驻命令。这些命令在命令行窗口启动时会自动加载到内存中，不需要到磁盘上去寻找对应的可执行文件，因此即使清空了PATH变量的值也不会影响这些命令的使用。然而，像“java”这样的外部命令，在执行时必须先由操作系统到指定的目录找到对应的可执行程序，然后才能加载并运行。到哪里去寻找这些程序就是依靠PATH变量来指定的。
Linux也是类似，甚至可以说在Linux中，PATH环境变量更为重要，因为Linux的很多基本命令都属于外部命令，如“ls”、“mkdir”等。当将PATH变量清空后，这些命令都将无法使用（当然，还是有一些内部命令我们仍然可以使用）。

CLASSPATH是编译或运行Java程序时用来告诉Java编译器或虚拟机到哪里查找Java类文件的，后面会对其做详细介绍。

在Windows XP或之前的版本中，依次点击“右键我的电脑” -&gt; “属性” -&gt; “高级” -&gt; “环境变量”；在Windows Vista和Windows 7中则依次点击“右键我的电脑” -&gt; “属性” -&gt; “高级系统设置” -&gt; “高级” -&gt; “环境变量”，打开环境变量设置窗口：






新建一个用户变量，名称为PATH，值为“C:\Program Files (x86)\Java\jdk1.7.0\bin”（还记得前面JDK安装到哪个目录吗？），点击“确定”按钮。然后用同样的方法新建一个CLASSPATH变量，暂时将值设置为“.”（英文句号）。为什么说CLASSPATH的值是暂时的，后面会解释。



  

  



设置完成后，环境变量设置窗口如下图所示。点击确定按钮，环境变量设置完成。






注意：在Windows中，环境变量分为“用户变量”和“系统变量”，它们的区别是，“用户变量”只对当前的用户起作用，而“系统变量”则对系统中的所有用户起作用。如果希望在多个用户之间共享环境变量的设置，可以将这些环境变量设置为系统变量，否则，应该使用用户变量，避免影响其他用户。在Linux中也有类似的概念，接下来会讲到。

在Linux中，可以通过编辑“~/.bashrc”文件来修改环境变量。在最后添加下面几行脚本，然后保存并退出：
JAVA_HOME=/usr/jdk1.7.0
JAVA_BIN=/usr/jdk1.7.0/bin
PATH=$PATH:$JAVA_HOME/bin
CLASSPATH=.
export JAVA_HOME JAVA_BIN PATH CLASSPATH






注意：Linux中，每个用户的home目录下都有.bashrc文件，这个文件用来保存用户的个性化设置，如命令别名、路径等，当然也可以用来定义环境变量。此文件是与每个用户相关的，一个用户的设置不会影响到其他用户，在这里设置环境变量相当于前面讲的Windows的用户环境变量。Linux中全局设置通常保存在“/etc/profile”文件中。
另外，Linux中PATH和CLASSPATH的分割符都是“:”（冒号），而Windows中是“;”（分号）。

当环境变量设置完成后，在Windows中打开新的命令行窗口，在Linux中使用“source ~/.bashrc”命令重新加载.bashrc文件，即可使新的环境变量生效。输入“java -version”命令，应该会打印出类似下面两幅图所示的内容：

Windows命令行的输出：








Linux的输出：





对以上步骤补充说明几点：



可以在Windows命令行或Linux Shell中使用命令设置环境变量。例如，在Windows中可以使用“set var_name=some value”，在Linux中使用“var_name=some value”，这种方式与上面介绍的方式的区别在于：这种方式的设置是临时性的，当重新启动一个新的命令行窗口（Windows）或重新登录（Linux）后，这些临时变量就会丢失。

JDK版本混乱：有时候，使用“java -version”命令可以打印出JDK的版本信息，但却与我们刚刚安装的JDK版本不一致。比如我们明明安装的是JDK 7，但却打印出JDK 6的版本信息，如下图所示：

                       

检查PATH变量，发现其中有一个路径为“C:\Program Files (x86)\Java\jdk1.6.0_25\bin”，原来我的系统中安装了两个版本的JDK，JDK 6和JDK 7。由于此JDK 6在系统环境变量PATH中，而Windows查找命令对应的程序时，首先查找的是系统变量，当找到了一个可用的java程序时，Windows将运行这个程序，而不再进一步查找。也就是说，系统PATH环境变量屏蔽了用户PATH环境变量。

不光如此，靠近PATH变量前部的路径中的程序将屏蔽其之后的路径中的同名程序。如同样是在系统PATH变量中，“C:\Program Files (x86)\Java\jdk1.6.0_25\bin;C:\Program Files (x86)\Java\jdk1.7.0\bin”，那么JDK 6仍然将屏蔽JDK 7，如果将它们的顺序交换：“C:\Program Files (x86)\Java\jdk1.7.0\bin;C:\Program Files (x86)\Java\jdk1.6.0_25\bin”，结果则相反。

不只是用户安装了多个版本的JDK时可能导致JDK版本的混乱，而且很多软件产品自身会包含JDK，即使用户只安装了一个JDK，但仍有可能与这些软件中的JDK互相屏蔽（如果这些软件同时也设置了环境变量的话）。例如Oracle数据库、MyEclipse等都包含自己的JDK。

在Windows下我还遇到过一个问题，那就是居然在Windows\system32目录下发现了java.exe、javaw.exe、javaws.exe三个文件，因为系统PATH变量中此目录处于较靠前的位置，因此很容易将用户自己安装的JDK屏蔽掉。

有三种方法来解决这个问题：

第一种方法是使用绝对路径，例如我们运行命令时使用“"C:\Program Files (x86)\Java\jdk1.7.0\bin\java.exe"”（当然，.exe可以省略）而不是“java”。使用绝对路径时，操作系统会直接根据路径定位到命令所在的目录，不再通过PATH变量来查找。这种方法的优点是绝对不会导致命令的覆盖，但缺点也很明显：必须输入完整的路径来运行命令（通常也很长）。

需要注意的是，当绝对路径中存在空格时（如上面的例子那样），需要将命令用英文双引号引起来。在设置PATH变量时不需要这样做，操作系统会自动完成这件事。

第二种方法是将自己安装的JDK路径设置到系统PATH变量的开头，这样，操作系统查找命令时就会最先查找我们设置的路径。但这种方法的缺点就是可能会影响其他用户（设置在了系统PATH变量中），并且可能会影响其他程序（其他的JDK被我们的屏蔽了）。

第三种方法就是设置一个新的环境变量，例如“JAVA_HOME”，将其值设置为我们安装的JDK的路径，如“C:\Program Files (x86)\Java\jdk1.7.0”，我们运行时，只需输入“"%JAVA_HOME%\bin\java"”即可（注意当路径中含有空格时要用双引号将命令引起来）。Apache Tomcat就使用这种方法。


4. 编译并运行例子程序：

经过了以上的步骤，JDK的环境就搭建好了，此时，可能需要再编译并运行一个Java例子程序来对刚搭建的环境做最终的检验。在这一节中，也会顺便讲到如何编译和运行一个Java程序，以及CLASSPATH的作用。更详细的，可以参考另一篇文章《JDK下提供的工具详解》。

此程序包含两个.java文件：ExceptionDemo.java和HelloWorldException.java，前者属于main包，而后者位于exceptions包（虽然它位于test\exceptions文件夹，这样安排的目的是为了更好地描述CLASSPATH的作用）：








下面是它们的源代码：



(1) ExceptionDemo.java:



[java] view
 plain copy






package main;  
import exceptions.HelloWorldException;  
public class ExceptionDemo {  
      /** 
       * 
       * @param args 
       * @throws HelloWorldException 
       */  
      public static void main(String[] args)  
                  throws HelloWorldException {  
            throw new HelloWorldException();  
      }  
}  


(2) HelloWorldException.java:




[java] view
 plain copy






package exceptions;  
public class HelloWorldException extends Exception {  
      private static final long serialVersionUID = 8679349130620681877L;  
      public HelloWorldException() {  
            super("Hello World!");  
      }  
}  


此程序仍然是一个经典的HelloWorld程序（虽然这次它是以很不友好的方式向世界问好——通过抛出异常）。

要编译这个程序，首先尝试第一种方法（下面的操作是在Windows命令行下进行的，Linux与此类似）：进入src文件夹，输入“javac main\ExceptionDemo.java”，但编译报错：






为什么会提示找不到HelloWorldException呢？那是因为该Java文件位于“test\exceptions\”目录下，但它的包名却是“exceptions”，从当前的src目录，javac无法找到exceptions目录，因为“src\exceptions”目录是不存在的。

接下来，我们尝试第二种方法：由src目录进入test目录，运行“javac ..\main\Exceptiondemo.java”：






编译居然通过了！可以看到没有报错，并且main目录下生成了ExceptionDemo.class文件（Java字节码文件），说明编译确实成功了。但是为什么？我们使用了“..\main\Exceptiondemo.java”，这明显不是ExceptionDemo的包路径，为什么编译器却不报错呢？另外我们还注意到，我错误地将“ExceptionDemo.java”写成了“Exceptiondemo.java”，即将字母“D”的大小写弄错了，编译器同样没有报错。

原来，javac只是将“..\main\Exceptiondemo.java”当做普通路径来寻找Java源程序文件，找到后即开始编译此文件，而当其在编译过程中发现程序还引用了其他类时（如ExceptionDemo.java中引用了HelloWorldException类），就会暂停对当前文件的编译，开始寻找这个引用的类文件，如果未找到，那么将会报告错误，编译失败。前一种方法就是因为没有找到HelloWorldException类而出错的。

那么javac程序是如何查找程序引用的其他类的呢？答案是按照CLASSPATH指定的路径加上程序所引用类的包名来查找的。CLASSPATH默认为“.”，即当前路径（我们之前也设置了CLASSPATH的值为“.”，但即使不设置，javac也会默认以当前路径为起点来查找所引用的类文件）。

因此在这里javac会检查“src\test\exceptions\”中是否有HelloWorldException.class文件，如果有，则继续检查其中是否有HelloWorldException.java文件，如果两者都存在，则检查HelloWorldException.class是否比HelloWorldException.java更新，如果答案是肯定的，则加载HelloWorldException.class并继续编译ExceptionDemo.java。而如果比较结果是HelloWorldException.java更新，或者不存在HelloWorldException.class，则说明需要重新编译HelloWorldException.java文件。如果只有.class文件而不存在.java文件，则加载之并继续编译ExceptionDemo.java。

如果没有找到目标文件（HelloWorldException.class或HelloWorldException.java），那么javac将报告错误（如之前那样）。

也就是说，编译是递归进行的：当程序中引用了其他类时，javac会判断是否需要编译这些类，如果需要，则javac会首先编译它们，如果这些类再次用到了其他的类，javac将再次重复此过程，直到完成全部编译。只要在此过程中有任何类没有找到，或者在其中发现了任何错误，那么javac将报告错误并中止编译（javac可能在中止之前尽可能多地编译，以尽量多地向用户报告程序中的错误）。

可以用下面的图来形象地展示这一过程：






至于我们将大小写弄错了但javac却没有报错的原因，其实前面的说明已经隐含了解释：是因为javac只是将命令中的.java文件当做普通文件，又由于Windows是不区分大小写的，因此不会报错。如果换成Linux系统，将会提示文件无法找到的错误。

默认的CLASSPATH是当前目录（“.”），我们也可以设置为需要的路径，让javac据此查找类文件（这就是前面所说的为什么只是暂时将CLASSPATH设置为“.”的原因）。在这个例子中，我们设置CLASSPATH为“.;D:\workspaces\workspace_v1.1\my-test\src\test”，注意Linux中分隔符为“:”（冒号）。然后在src目录下就可以使用命令“javac main\ExceptionDemo.java”进行编译：






实际上，此时在任何目录都可以对ExceptionDemo.java进行编译，只是文件的路径要适当更改。例如我们在D盘根目录输入以下命令编译：






这是因为设置了CLASSPATH后，javac总能找到HelloWorldException类。

有时候必须使用CLASSPATH：当涉及到的类很多时，而这些类并不在同一个目录下，此时我们只能使用CLASSPATH来指定这些类的路径——我们无法同时处于多个类的“当前目录”下。

另外一个需要注意的问题是，JDK包含的Java基础类（例如java.lang包中的类）并不需要指定CLASSPATH——Java知道如何找到它们。

编译完成后，运行我们的例子，例子将抛出一个异常，并向世界问好：







注意，必须输入完整的包名和类名（不需要.class后缀），且大小写不能弄错（因为Java是区分大小写的）。完整的包名+类名在Java中称为类的完全限定名。
       Java环境配置的异常处理：

Java环境配置的异常处理：
      Windows7X64安装“jdk-8u92-windows-x64.exe”后  ，在cmd命令行执行javac，老是提示：javac不是内部命令或外部命令，也不是可运行的程序或批处理文件。但是输入
 java -version能正常显示jdk版本号，说明java命令可用。我输入Java -home后，发现指向的路径完全不对，环境变量设置没起作用，不知是何原因。
     解决方案：运行——&gt;cmd ：
在控制台中运行以下命令设置java环境变量即可解决：
set java_home=C:\Program Files\Java\jdk1.8.0_92  安装JDK的根目录
setclasspath=%JAVA_HOME%\lib\tools.jar;%JAVA_HOME%\lib\dt.jar;
set path=%JAVA_HOME%\bin; 
       注：输入不区分大小写，另外附上几个Java命令
java -help  可以查看命令
java -home  查看JDK安装路径
java -version  查看JDK版本
异常如下图：


配置正确如下图：





至此为止，我们成功地搭建起了Java开发和运行环境。

转载自：http://blog.csdn.net/antineutrino/article/details/6763565



MySQL数据库提供了很多函数包括：数学函数；字符串函数；日期和时间函数；条件判断函数；系统信息函数；加密函数；格式化函数；


以下是MySQL数据库函数用法简析：

语法：

新建：

Create function function_name(参数列表)returns返回值类型

函数体

函数名，应该合法的标识符，并且不应该与已有的关键字冲突。

一个函数应该属于某个数据库，可以使用db_name.funciton_name的形式执行当前函数所属数据库，否则为当前数据库。

参数部分，由参数名和参数类型组成。

返回值类类型

函数体由多条可用的mysql语句，流程控制，变量声明等语句构成。

多条语句应该使用begin end语句块包含。

注意，一定要有return返回值语句。

删除：

Dropfunction if existsfunction_name;

查看：

Show function status like ‘partten’

Show create functionfunction_name;

修改：

Alter functionfunction_name函数选项。

例子：

Hello world!

IF语句

IF search_conditionTHEN

statement_list

[ELSEIF search_conditionTHENstatement_list]

...

[ELSE statement_list]ENDIF;

CASE语句

CASE case_value

WHEN when_valueTHENstatement_list

[WHEN when_value THENstatement_list]

...

[ELSE statement_list]

END CASE; 

循环：

While

[begin_label:]WHILEsearch_conditionDO

statement_list

END WHILE [end_label];

如果需要在循环内提前终止 while循环，则需要使用标签；标签需要成对出现。

退出循环

退出整个循环leave 相当于break

退出当前循环iterate 相当于 continue

通过退出的标签决定退出哪个循环。
变量声明：
语法：
DECLARE var_name[,...] type [DEFAULT value]
这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个DEFAULT子句。值可以被指定为一个表达式，不需要为一个常数。如果没有DEFAULT子句，初始值为NULL。
使用
语序使用 set 和 select into语句为变量赋值。

注意在函数内是可以使用全局变量（用户自定义的变量的）@XXX 全局变量不用声明 可以直接@XXX使用。


例子：获取当前班级内，最大的学号。 



参考学生表 

create table join_student( 

stu_id int not null auto_increment, 

stu_no char(10), 

class_id int not null, 

stu_name varchar(10), 

stu_info text, 

primary key (stu_id) 

); 

计算新增学号 

drop function if existssno;

 delimiter $$ #在包含有语句块时 可以更换语句结束符“；” 为“$$” 

create function sno(c_id int)returns char(10) 

begin 

declare last_no char(10); #声明一个局部变量 用来保存当前最大的学号， 如果没有就为null

declare class_name char(10); 

select stu_no from join_student where class_id=c_id order by stu_no desc limit 1 into last_no; 

if last_no is null then #如果为空代表当前班级没有学生 从1开始，获得班级名字 

return concat ((select c_name from join_class where id=c_id into class_name),'001'); #concat() 函数的作用是连接字符串。 

else 

return concat(left(last_no,7),lpad(right(last_no,3) + 1, 3, '0')); 

end if;

 #return @last_no; 

end 

$$

delimiter ; 

随机获得学生名字。 

drop function if exists sname; 

delimiter $$ 

create function sname() returns char(2) 

begin 

declare first_name char(16) default '赵钱孙李周吴郑王冯陈褚卫蒋沈韩杨'; 

declare last_name char(10) default '甲乙丙丁戊己庚辛壬癸'; 

declare full_name char(2); 

set full_name=concat(substring(first_name,floor(rand()*16+1), 1), substring(last_name,floor(rand()*10+1), 1)); 

return full_name; 

end 

$$ 

delimiter ;


========================================================================================

mysql常用内置函数

数值函数

Abs(X)，绝对值abs(-10.9) = 10

Format(X，D)，格式化千分位数值format(1234567.456, 2) =1,234,567.46

Ceil(X),向上取整ceil(10.1) = 11

Floor(X),向下取整floor (10.1) = 10

Round(X),四舍五入去整

Mod(M,N) M%N M MOD N 求余 10%3=1

Pi(),获得圆周率

Pow(M,N) M^N

Sqrt(X)，算术平方根

Rand(),随机数

TRUNCATE(X,D) 截取D位小数

时间日期函数

Now(),current_timestamp(); 当前日期时间

Current_date();当前日期

current_time();当前时间

Date(‘yyyy-mm-dd HH;ii:ss’);获取日期部分

Time(‘yyyy-mm-dd HH;ii:ss’);获取时间部分

Date_format(‘yyyy-mm-dd HH;ii:ss’,’%D %y %a %d %m %b %j');

Unix_timestamp();获得unix时间戳

From_unixtime();//从时间戳获得时间

字符串函数

LENGTH(string ) //string长度，字节

CHAR_LENGTH(string) //string的字符个数

SUBSTRING(str ,position [,length ]) //从str的position开始,取length个字符

REPLACE(str ,search_str ,replace_str) //在str中用replace_str替换search_str

INSTR(string ,substring ) //返回substring首次在string中出现的位置

CONCAT(string [,... ]) //连接字串

CHARSET(str) //返回字串字符集

LCASE(string ) //转换成小写

LEFT(string ,length ) //从string2中的左边起取length个字符

LOAD_FILE(file_name) //从文件读取内容

LOCATE(substring , string [,start_position ]) //同INSTR,但可指定开始位置

LPAD(string ,length ,pad ) //重复用pad加在string开头,直到字串长度为length

LTRIM(string ) //去除前端空格

REPEAT(string ,count ) //重复count次

RPAD(string ,length ,pad) //在str后用pad补充,直到长度为length

RTRIM(string ) //去除后端空格

STRCMP(string1 ,string2 ) //逐字符比较两字串大小

流程函数：

CASE WHEN [condition]THEN result[WHEN [condition]THEN result ...][ELSE result]END 多分支

IF(expr1,expr2,expr3) 双分支。

聚合函数

Count()

Sum();

Max();

Min();

Avg();

Group_concat()

其他常用函数

Md5();

Default();


        MySQL字符串截取函数：left(), right(),substring(), substring_index(),还有 mid(),substr()。其中，mid(),substr() 等价于 substring() 函数，substring()的功能非常强大和灵活。
1. 字符串截取：left(str,length) 
&lt;span style="font-family:宋体;font-size:14px;"&gt;mysql&gt; select left('sqlstudy.com', 3);&lt;/span&gt;
+-------------------------+
| left('sqlstudy.com', 3) |
+-------------------------+
| sql                     |
+-------------------------+
2. 字符串截取：right(str,length) 
&lt;span style="font-family:宋体;font-size:14px;"&gt;mysql&gt; select right('sqlstudy.com', 3);&lt;/span&gt;
+--------------------------+
| right('sqlstudy.com', 3) |
+--------------------------+
| com                      |
+--------------------------+
3. 字符串截取：substring(str,pos); substring(str, pos, len) 
3.1 从字符串的第 4 个字符位置开始取，直到结束。
&lt;span style="font-family:宋体;font-size:14px;"&gt;mysql&gt; select substring('sqlstudy.com', 4);&lt;/span&gt;
+------------------------------+
| substring('sqlstudy.com', 4) |
+------------------------------+
| study.com                    |
+------------------------------+
3.2 从字符串的第 4 个字符位置开始取，只取 2 个字符。 
&lt;span style="font-family:宋体;font-size:14px;"&gt;mysql&gt; select substring('sqlstudy.com', 4, 2);&lt;/span&gt;
+---------------------------------+
| substring('sqlstudy.com', 4, 2) |
+---------------------------------+
| st                              |
+---------------------------------+
3.3 从字符串的第 4 个字符位置（倒数）开始取，直到结束。
&lt;span style="font-family:宋体;font-size:14px;"&gt;mysql&gt; select substring('sqlstudy.com', -4);&lt;/span&gt;
+-------------------------------+
| substring('sqlstudy.com', -4) |
+-------------------------------+
| .com                          |
+-------------------------------+
3.4 从字符串的第 4 个字符位置（倒数）开始取，只取 2 个字符。
&lt;span style="font-family:宋体;font-size:14px;"&gt;mysql&gt; select substring('sqlstudy.com', -4, 2);&lt;/span&gt;
+----------------------------------+
| substring('sqlstudy.com', -4, 2) |
+----------------------------------+
| .c                               |
+----------------------------------+
我们注意到在函数 substring(str,pos, len)中， pos 可以是负值，但len 不能取负值。
4. 字符串截取：substring_index(str,delim,count)
4.1 截取第二个 '.' 之前的所有字符。
&lt;span style="font-family:宋体;font-size:14px;"&gt;mysql&gt; select substring_index('www.sqlstudy.com.cn', '.', 2);&lt;/span&gt;
+------------------------------------------------+
| substring_index('www.sqlstudy.com.cn', '.', 2) |
+------------------------------------------------+
| www.sqlstudy                                   |
+------------------------------------------------+
4.2 截取第二个 '.' （倒数）之后的所有字符。
&lt;span style="font-family:宋体;font-size:14px;"&gt;mysql&gt; select substring_index('www.sqlstudy.com.cn', '.', -2);&lt;/span&gt;
+-------------------------------------------------+
| substring_index('www.sqlstudy.com.cn', '.', -2) |
+-------------------------------------------------+
| com.cn                                          |
+-------------------------------------------------+
4.3 如果在字符串中找不到 delim 参数指定的值，就返回整个字符串
&lt;span style="font-family:宋体;font-size:14px;"&gt;mysql&gt; select substring_index('www.sqlstudy.com.cn', '.coc', 1);&lt;/span&gt;
+---------------------------------------------------+
| substring_index('www.sqlstudy.com.cn', '.coc', 1) |
+---------------------------------------------------+
| www.sqlstudy.com.cn                               |
+---------------------------------------------------+
 




     说明：调用文件系统(FS)Shell命令应使用bin/hadoop fs &lt;args&gt;的形式。所有的的FS
 shell命令使用URI路径作为参数。
1、cat
说明：将路径指定文件的内容输出到stdout。
用法：hadoop fs -cat URI [URI …]
范例：
hadoop fs -cat hdfs://host1:port1/file1 hdfs://host2:port2/file2
hadoop fs -cat file:///file3/user/hadoop/file4
返回值：成功返回0，失败返回-1。
2、chgrp
说明：改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。
用法：hadoop fs -chgrp [-R] GROUP URI [URI …]
范例：hadoop fs -chgrp -R hadoop /user/hadoop/
3、chmod
说明：改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。
用法：hadoop fs -chmod [-R] URI [URI …]
范例：hadoop fs -chmod -R 744 /user/hadoop/
 
4、chown
说明：改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。
用法：hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]
范例：hadoop fs -chmod -R hadoop /user/hadoop/
5、copyFromLocal(本地到hdfs)
说明：除了限定源路径是一个本地文件外，和put命令相似。
用法：hadoop fs -copyFromLocal &lt;localsrc&gt; URI
6、copyToLocal(hdfs到本地)
说明：除了限定目标路径是一个本地文件外，和get命令类似。
用法：hadoop fs -copyToLocal [-ignorecrc] [-crc] URI &lt;localdst&gt;
7、cp
说明：将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。
用法：hadoop fs -cp URI [URI …] &lt;dest&gt;
范例：
hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2
hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir
返回值：成功返回0，失败返回-1。
8、du
说明：显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。
用法：hadoop fs -du URI [URI …]
范例：
hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1hdfs://host:port/user/hadoop/dir1
查看hbase所有文件的大小
hadoop fs -du hdfs://master:54310/hbase
返回值：成功返回0，失败返回-1。
9、dus
说明：显示文件的大小。
用法：hadoop fs -dus &lt;args&gt;
10、expunge
说明：清空回收站。
用法：hadoop fs -expunge

11、get(hdfs到本地)
说明：复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件。使用-crc选项复制文件以及CRC信息。
用法：hadoop fs -get [-ignorecrc] [-crc] &lt;src&gt; &lt;localdst&gt;
范例：
hadoop fs -get /user/hadoop/file localfile
hadoop fs -get hdfs://host:port/user/hadoop/file localfile
返回值：成功返回0，失败返回-1。
12、getmerge
说明：接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件连接成本地目标文件。addnl是可选的，用于指定在每个文件结尾添加一个换行符。
用法：hadoop fs -getmerge &lt;src&gt; &lt;localdst&gt; [addnl]
13、ls
用法：hadoop fs -ls &lt;args&gt;
说明：
(1).如果是文件，则按照如下格式返回文件信息：
文件名 &lt;副本数&gt;
文件大小修改日期修改时间权限用户ID
组ID
(2).如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：
目录名 &lt;dir&gt; 
修改日期修改时间权限用户ID
组ID
范例：
hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2hdfs://host:port/user/hadoop/dir1 /nonexistentfile
返回值：成功返回0，失败返回-1。
14、lsr
用法：hadoop fs -lsr &lt;args&gt;
说明：ls命令的递归版本。类似于Unix中的ls -R。
15、mkdir
说明：接受路径指定的uri作为参数，创建这些目录。其行为类似于Unix的mkdir -p，它会创建路径中的各级父目录。
用法：hadoop fs -mkdir &lt;paths&gt;
范例：
hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2
hadoop fs -mkdir hdfs://host1:port1/user/hadoop/dirhdfs://host2:port2/user/hadoop/dir
返回值：成功返回0，失败返回-1。
16、movefromLocal
说明：输出一个”not implemented“信息。
用法：dfs -moveFromLocal &lt;src&gt; &lt;dst&gt;
17、mv
说明：将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。
用法：hadoop fs -mv URI [URI …] &lt;dest&gt;
范例：
hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2
hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2hdfs://host:port/file3 hdfs://host:port/dir1
返回值：成功返回0，失败返回-1。
18、put
说明：从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。
用法：hadoop fs -put &lt;localsrc&gt; … &lt;dst&gt;
范例：
hadoop fs -put localfile /user/hadoop/hadoopfile
hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir
hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile
hadoop fs -put – hdfs://host:port/hadoop/hadoopfile
从标准输入中读取输入。
返回值：成功返回0，失败返回-1。
19、rm
说明：删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。
用法：hadoop fs -rm URI [URI …]
范例：
hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir
返回值：成功返回0，失败返回-1。
20、rmr
说明：delete的递归版本。
用法：hadoop fs -rmr URI [URI …]
范例：
hadoop fs -rmr /user/hadoop/dir
hadoop fs -rmr hdfs://host:port/user/hadoop/dir
返回值：成功返回0，失败返回-1。
21、setrep
说明：改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。
用法：hadoop fs -setrep [-R] &lt;path&gt;
范例：
hadoop fs -setrep -w 3 -R /user/hadoop/dir1
返回值：成功返回0，失败返回-1。
22、stat
说明：返回指定路径的统计信息。
用法：hadoop fs -stat URI [URI …]
范例：
hadoop fs -stat path
返回值：成功返回0，失败返回-1。
23、tail
用法：将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。
用法：hadoop fs -tail [-f] URI
范例：
hadoop fs -tail pathname
返回值：成功返回0，失败返回-1。
24、test
用法：hadoop fs -test -[ezd] URI
选项：
-e 检查文件是否存在。如果存在则返回0。
-z 检查文件是否是0字节。如果是则返回0。
-d 如果路径是个目录，则返回1，否则返回0。
范例：
hadoop fs -test -e filename
25、text
说明：将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。
用法：hadoop fs -text &lt;src&gt;
26、touchz
说明：创建一个0字节的空文件。
用法：hadoop fs -touchz URI [URI …]
范例：
hadoop -touchz pathname
返回值：成功返回0，失败返回-1。
 


        高性能分布式计算与存储系统。

        这个系统看名字很高大，所涉足的目前互联网最领先的技术领域。具体有什么用途？ 它主要是作为中间层，给网站页面提供缓存服务的，并且，它对付的难题，是大数据、海量数据，相信，每一个日PV超过千万级的网站，都必须会有类似的系统存在，如果，你曾经看过，博客园里的《淘宝技术发展》等类似文章，就一定不会对我接来将要提到的许多概念和术语感到陌生。对于这样大流量，需要处理大数据的网站而言，由Web的逻辑直接调用管理数据存储，是非常不科学的，实际上也是不可能的，大数据、高并发的对数据库进行读写，通常数据库都会挂掉，从而使网站也挂掉，必须要在Web和数据库之间，通过技术手段实现一种“转换”或“控制”，或“均衡”或“过渡”。这样的技术手段有许多，所实现的东东也有许多，我们用到的，就是被称为“中间层”的一个逻辑层，在这个层，将数据库的海量数据抓出来，做成缓存，运行在服务器的内存中，同理，当有新的数据到来，也先做成缓存，再想办法，持久化到数据库中，就是这样简单的思路，但实现起来，从零到有，可以说难如登天，但是，任何事物，都是在曲折中，不断发展前进的，这是中学我们就学过的哲学理论。这个系统，就被我们称简为“缓存系统”，它最大的好处，就是砍掉了每天上千万次的数据库读写操作，取代而之的，是读取服务器中提供缓存服务的进程所控制的内存，所以你知道，这里面节省了多少的资源申请、竞争、I/O……当然，后面你也会发现，它会带来许多新的问题，最显著的问题，就是数据的同步和一致性问题。

          这个系统长什么样子？






       就是这样的一张架构图，代表着可以处理每日上千万PV的系统，涉及到许多的技术，让我们一个部分一个部分解读它。

首先，从当我有一个web请求到达时，将会发生怎样的事情说起。比如，我是一个用户，我在这个网站登陆，我的“个人”页面上，将会加载许许多多的东西，有许许多多的图片、文字、消息等，我们举其中一个例子，我将要得到我的好友列表——friend list。通过常识可以知道，这个friend list，不是随机的、临时的，而肯定是一个（一组）持久化存储于数据库里的数据，我们就是一个用户请求得到他的friend list说起，来解读这张架构图。如果我的网站流量很小，每天不超过10万PV，峰值可能就几百个上千个用户，同时请求他们的friend
 list，那么，现今任何一种语言配上任何一种数据库的搭配，只要稍做处理，都可以很好的完成这个工作——从数据库中，读出该用户的friend list，然后访回给web，如果用户对好友列表作了任何修改，web马上将修改内容写入数据库，形成新的friend list。然而，当访问流量持续提升，达到千万级、甚至亿级PV的时候，刚才说的方法就不可行了。因为，同时可能有几十万甚至上百万用户，通过web请求从数据库中读（如果写将会更糟糕）上百条万数据，数据库将不堪重负，形成巨大的延迟甚至挂掉。通过上面的系统，来解决这样的问题。

          现在，我们要设计和研发的上述系统，当一个web页面提交一个获取friend list的请求后，它首先将根据一定的规则，通过负载均衡，然后到达相应的master节点。上面我们提到的是DNS负载均衡，这得众多负载均衡技术中的一种方法。也就是说，我有许许多多的master节点（上图的scalabe表明，我是可扩展的，只要有条件，可随意横向扩展节点，以提高速度、容灾、容量等指标），每个master节点的IP地址（域名）当然不一样，通过DNS负载均衡，合理地把该请求，送到相对“空闲”的master节点服务器。现在解释一下master节点服务器和slave节点服务器的功能：slave节点，主要用于”Running
 services”，即，实际处理请求的缓存服务进程，通常运行在slave节点上；master节点，主要用于分发通过负载均衡的请求（当然，master节点上也可以运行一些“缓存服务进程”，即并发流量不高、较辅助的一些服务），找到用于处理实际请求的合适的slave节点，将该请求交给它处理，再次实现了一道“负载均衡”，同时，需要分布式计算的内容，将可能同时分发到几个slave节点，之后再对结果进行合并返回（Map-Reduce原理）。

          一个friend list请求已经通过DNS负载均衡、通过master节点进行分配，到达了相应的slave节点上。我们还知道，所说的“缓存” ，正是slave节点中所运行的services进程中所管理的内存，提供同样功能的service可能会有很多份，同时运行在不同slave节点上，以提供高并发和分布式计算的功能。例如，获得friend list就是这样的service，因为这个功能太常用了，所以，在我们的系统中，这样的服务可能同时提供5份、10份甚至更多，那么我这个获取friend list的请求，究竟被分配到哪个slave节点上的service处理呢？这正是刚才提到的master节点来完成这一工作。再比如，我现在需要获取“二度关系”的列表（关于六度人脉理论，可google），所谓“二度关系”，就是好友的好友，那么我要取这样的列表，即friend’s
 every friend list，这样的请求，将会把取每个friend list分配（Map）到不同slave节点上去做（根据一定的规则），然后再进行合并（Reduce）（当然，熟悉算法的同学可能已经发现，这样去获取请求，非常的笨拙，有没有更好的方法呢？当然有！因为好友的好友，其实就是好友的friend list与我和好友的共同好友common friend list的“差集”，对吗？所以我不用去取好友的每个好友的friend list，而只用取2次就可以通过计算完成请求，这又节省了多少资源呢？假如我有100个好友，1000个，10000万个？会节省多少次计算呢？这也证明，一个良好的算法，对改善程序性能，有多么大的帮助！）

             获取friend list的请求，已经在被某个slave节点中的负责这一功能的service进程处理，它将根据一定规则，给出两种可能的处理方式：

1、 我这个用户非常活跃，经常登陆网站（一定的规则，认为缓存未到过期时间），且我这个slave节点自上次“重建缓存”（即重新从数据库中读取数据，建立缓存，后面会谈）后，没有发生过down机重启行为（又一定的规则），我也没有收到过master节点发送过来要求更新缓存（即从数据库中比较数据并更新）的Notification（通知），或是在一定条件下我这个slave节点对它掌握的缓存数据版本（版本管理系统原理，思考一下svn的工作原理）和数据库进行了一次比较（注意，比较数据版本可认为只是一个int值，且是原子操作，这和比较整条数据是否一致在性能上有天壤之别）发现是最新的数据版本，那么，我这个slave节点将直接返回缓存数据，而没有任何数据库读操作，也就是说，我这一次获取friend
 list的请求，得到的是缓存数据，当然，这个缓存数据肯定是最新的、正确的、和数据库中的持久化数据是一致的，后面会提到怎样来尽量保证这一点；

2、第1点中的“一定规则”不满足时，即我这个slave节点的缓存和数据库中的数据可能存在不一致的没有其它办法，我必须从数据库中读取数据，更新缓存，然后再返回。但同时注意，slave节点中的service服务进程，将认为此用户现在活跃，可能还会请求一些相关、类似的数据（如马上可能进行添加好友、删除好友等操作），所以去数据库读取数据的时候，将不会只读friend list，可能与用户有关的其它一部分数据，会被同时读取并更新缓存，如果负责这一部分数据的缓存服务并不是当前的service进程，或在其它slave节点，或同时还有几份service进程在工作，那么slave节点将提交“更新缓存”请求给master节点，通过master节点发出Notification给相关slave节点的相关service进程，从而，尽可能使每一次读取数据库的作用最大化，而如果稍后用户果然进行了我们猜测的行为（可认为cache命中），结果将同第1点，直接通过缓存返回数据而且保证了数据的正确和一致性。

好了，刚刚提到的都是“读操作”，相比“写操作”， 其数据一致性更容易保证，之后我们将讲述“写操作”的工作原理。现在，让我们先跳过这一部分，继续看架构图。slave节点之后，就是实际的数据存储了，使用了MySQL、Redis，MySQL主从之间的协同是DBA的工作，不在此篇讨论，Redis主要存储K-V键值对数据，比如用户id和用户昵称，是最常用的K-V对之一，通过Redis进行存储，再结合上述的工作过程，可保证这个系统的高性能。而架构图最右下角的Hadoop与MongoDB，是可选的MySQL替代方案，其实，正是未来的主要发展方向。如果slave节点中的service服务进程与Hadoop良好结合，系统的性能将更上一层楼。顺便说一句，master、slave节点都是由C++开发的。可参考陈皓的一篇文章《Why
 C++? 王者归来》。

           在上篇里，我们主要讨论了，这个系统怎样处理大数据的“读”操作，当然还有一些细节没有讲述。下篇，我们将主要讲述，“写”操作是如何被处理的。我们都知道，如果只有“读”，那几乎是不用做任何数据同步的，也不会有并发安全问题，之所以，会产生这样那样的问题，会导致缓存和数据库的数据不一致，其实根源就在于“写”操作的存在。下面，让我们看一看，当系统需要写一条数据的时候，又会发生怎样的事情？

            同样，我们还是以friend list为例。现在，我登陆了这个网站，获取了friend list之后，我添加了一个好友，那么，我的friend list必定要做修改和更新（当然，添加好友这一个动作肯定不会只有修改更新friend list这一个请求，但我们以此为例，其它请求也是类似处理），那么，这个要求修改和更新friend list的请求，和获取friend list请求类似，在被slave节点中的服务进程处理之前，也是先通过DNS负载均衡，被分配到合适的master节点，再由master节点，分配到合适的相对空闲的负责这一功能的slave点上。现在假设，前面我们已经讲过，获取friend
 list这样的请求，非常常用，所以，提供这一供能的服务进程将会有多份，比如，有10份，服务进程编号为0~9，同时运行在10个（也可能仅运行在1个~9个slave节点上！）slave节点上，具体分配请求的时候，选择哪一个slave节点和哪一份服务进程呢？这当然有许多种规则去影响分配策略，我们就举一个最简单的例子，采用用户id对10取模，得到0~9的结果，即是所选择的服务进程编号，假设我的用户id尾号为9，那么我这个请求，只会被分配到编号为9的服务进程去处理（当然，所有用户id尾号为9的都是如此），编号为9的服务进程，也只负责为数据库中用户id尾号为9的那些数据做缓存，而用户id尾号为0~8的缓存则由其它服务进程来处理。如果所需的请求是以刚才这种方式工作的，那么现在我要求修改和更新friend
 list的这个请求，将只会被分配到服务进程编号为9的进程来处理，我们称之为“单点模型”（也就是说，同一条数据只会有一份可用缓存，备份节点上的的不算），你可能已经猜到了，还会有“多点模型”——即同时有好几个服务进程都会负责同样的缓存数据，这是更复杂的情况，我们稍后再讨论。



（点击可查看大图）

          现在，我们接着说“单点模型” 。这个修改和更新friend list的请求到了编号为9的服务进程中后，如何被处理呢？缓存肯定先要被处理，之后才考虑缓存去和数据库同步一致，这大家都知道（否则还要这个系统干嘛？）大家还知道，只要涉及到并发的读写，就肯定存在并发冲突和安全问题，这又如何解决呢？我们有两种方式，来进行读写同步。

1、 第一种方式，就是传统的，加锁方式——通过加锁，可以有效地保证缓存中数据的同步和正确，但缺点也非常明显，当服务进程中同时存在读写操作的线程时，将会存在严重的锁竞争，进而重新形成性能瓶颈。好在，通常使用这种方式处理的业务需求，都经过上述的一些负载均衡、分流措施之后，锁的粒度不会太大，还是上述例子，我最多也就锁住了所有用户id尾号为9的这部分缓存数据更新，其它90%的用户则不受影响。再具体些，锁住的缓存数据可以更小，甚至仅锁住我这个用户的缓存数据，那么，锁产生的性能瓶颈影响就会更小了（为什么锁的粒度不可能小到总是直接锁住每个用户的缓存数据呢？答案很简单，你不可能有那么多的锁同时在工作，数据库也不可能为每个用户建一张表），即锁的粒度是需要平衡和调整的。好，现在继续，我要求修改和更新friend
 list的请求，已经被服务进程中的写进程在处理，它将会申请获得对这部分缓存数据的锁，然后进行写操作，之后释放锁，传统的锁工作流程。在这期间，读操作将被阻塞等待，可想而知，如果锁的粒度很大，将有多少读操作处于阻塞等待状态，那么该系统的高性能就无从谈起了。

2、有没有更好的方法呢？当然有，这就是无锁的工作方式。首先，我们的网站，是一个读操作远大于写操作的网站（如果需求相反，可能处理的方式也就相反了），也就是说，大多数时候，读操作不应该被写操作阻塞，应优先保证读操作，如果产生了写操作，再想办法使读操作“更新”一次，进而使得读写同步。这样的工作方式，其实很像版本管理工具，如svn的工作原理：即，每个人，都可以读，不会因为有人在进行写，使得读被阻塞；当我读到数据后，由于有人写，可能已经不是最新的数据了，svn在你尝试提交写的时候，进行判断，如果版本不一致，则重新读，合并，再写。我们的系统也是按类似的方式工作的：即每个线程，都可以读，但读之前先比较一下版本号，然后读缓存数据，读完之后准备返回给Web时，再次比较版本号，如果发现版本已经被更新（当然你读的数据顶多是“老”数据，但不至于是错误的数据，Why？还是参考svn，这是”Copy
 and Write”原理，即我写的那一份数据，是copy出来写的，写完再copy回去，不会在你读出的那一份上写），则必须重新读，直到读到的缓存数据版本号是最新的。前面已经说过，比较和更新版本号，可认为是原子操作（比如，利用CAS操作可以很好的完成这一点，关于CAS操作，可以google到一大堆东西），所以，整个处理流程就实现了无锁化，这样，在大数据高并发的时候，没有锁瓶颈产生。然而，你可能已经发现其中的一些问题，最显著的问题，就是可能多读不止一次数据，如果读的数据较多较大，又要产生性能瓶颈了（苦！没有办法），并且可能产生延迟，造成差的用户体验。那么，又如何来解决这些问题呢？其实，我们是根据实际的业务需求来做权衡的，如果，所要求的请求，允许一定的延迟存在，实时性要求不是最高，比如，我看我好友发的动态，这样的缓存数据，并不要求实时性非常高，稍稍有延迟是允许的，你可以想象一下，如果你的好友发了一个状态，你完全没有必要，其实也不可能在他点击“发布”之后，你的动态就得到了更新，其实只要在一小段时间内（比如10秒？）你的动态更新了，看到了他新发布了状态，就足够了。假设是这样的请求，且如果我采用第1种加锁的方式所产生的性能瓶颈更大，那么，将采用这种无锁的工作方式，即当读写有冲突时候，读操作重新读所产生的开销或延迟，是可以忍受的。比较幸运的是，同时有多个读写线程操作同一条缓存数据导致多次的重读行为，其实并不是总是发生，也就是说，我们系统的大数据并发，主要在多个进程线程同时读不同条的数据这一业务需求上，这也很容易理解，每个用户登陆，都是读他们各自的friend
 list（不同条数据，且在不同的slave节点上），只不过，这些请求是并发的（如果不进行分布式处理会冲垮服务器或数据库），但是并不总是会，许多用户都要同时读某一条friend list同时我还在更新该条friend list导致多次无效的重读行为。

           我们继续上面的friend list。现在，我的friend list已经在缓存中被修改和更新了。无论是采用方式1还是方式2进行，在这期间，如果恰好有其它线程来读我的friend list，那么总之会受到影响，如果是方式1，该请求将等待写完毕；而如果是方式2，该请求将读2次（也可能更多，但实在不常见）。这样的处理方式，应该不是最好的，但前面已经说过了，我们的系统，主要解决：大流量高并发地读写多条数据，而不是一条。接下来，该考虑和数据库同步的事情了。

           经过我修改和更新friend list后，缓存中的数据和数据库不一致了呢？显然，数据库中的数据，已经过期了，需要对其更新。现在，slave节点中的编号为9的服务进程，更新完了自己的缓存数据后（修改更新我的friend list），将“尝试”向数据库更新。注意，用词“尝试”表明该请求不一定会被马上得到满足。其实，服务进程对数据库的更新，是批量进行的，可认为是一个TaskContainer（任务容器），每间隔一段时间，或得到一定的任务数量，则成批地向数据库进行更新操作，而不是每过来一个请求，更新缓存后就更新一次数据库（你现在知道了这样做又节省了多少次数据库操作！）。那么，为什么可以这样做呢？因为，我们已经有了缓存，缓存就是我们的保障，在“单点模型”下，缓存更新后，任何读缓存的操作，都只会读到该缓存，不需要经过数据库，参看上篇中提到过此问题。所以，数据库的写更新操作，可以“聚集”，可以一定延迟之后，再进行处理。你会发现，既然如此，我就可以对这些操作进行合并、优化，比如，两个写请求都是操作同一张表，那么可以合并成一条，没错，这其实已经涉及到SQL优化的领域了。当然，你也会发现，现在缓存中的新数据还没有进行持久化，如果在这个时间点，slave节点机器down掉了，那么，这部分数据就丢失了！所以，这个延迟时间并不会太长，通常10秒已经足够了。即，每10秒，整理一下我这个服务进程中已经更新缓存未更新DB的请求，然后统一处理，如果更杞人忧天（虽然考虑数据安全性决不能说是杞人忧天，但你要明白，其实任何实时服务器发生down行为总是会有数据丢失的，只是或多或少），则延迟间隔可以更短一些，则DB压力更大一些，再次需要进行实际的考量和权衡。至此，我的friend
 list修改和更新请求，就全部完成了，虽然，可能在几十秒之前，就已经在页面上看到了变化（通过缓存返回的数据）。

            那么，读和写都已经讲述了，还有其它问题吗？问题还不少。刚才讨论的，都是“单点模型”。



（点击可查看大图）

             即，每一条数据库中的数据，都只有一份缓存数据与之对应。然而，实际上，“多点模型”是必须存在的，而且是更强大的处理方式，也带来同步和一致性的更多难题，即每一条数据，可能有多份缓存与之对应。即多个slave节点上的服务进程中，都有一份对应DB中相同数据的缓存，这个时候，又将如何同步呢？我们解决的方式，叫做“最终一致性”原则，关于最终一致性模型，又可以google到一大堆，特别要提出的是GoogleFS的多点一致性同步，就是通过“最终一致性”来解决的，通俗的讲，就是同一条数据，同一时刻，只能被一个节点修改。假设，我现在的业务，是“多点模型”，比如，我的friend
 list，是多点模型，有多份缓存（虽然实际并不是这样的），那么，我对friend list的修改和更新，将只会修改我被分配到的slave节点服务进程中的缓存，其它服务进程或slave节点的缓存，以及数据库，将必须被同步更新，这是如何做到的呢？这又要用到上篇曾提到的Notification（通知服务），这个模块虽然没有在架构图中出现，却是这个系统中最核心的一种服务（当然，它也是多份的，呵呵），即，当一条数据是多点模型时，当某一个服务进程对其进行修改和更新后，将通过向master节点提交Notificaion并通知其它服务进程或其它slave节点，告知他们的缓存已经过期，需要进行更新，这个更新，可能由所进行修改更新的服务进程，发送缓存数据给其它进程或节点，也由可能等待DB更新之后，由其它节点从DB进行更新，从而间接保证多点一致性。等等，刚才不是说，通常10秒才批量更新DB吗？那是因为在单点模型下，这样做是合理的，但在多点模型下，虽然也是批理对数据库进行更新，但这样的延迟通常非常小，可认为即时对数据库进行批量更新，然后，通过Notification通知所有有这一条数据的节点，更新他们的缓存。由此可见，多点模型，所可能产生的问题是不少的。那么，为什么要用多点模型呢？假设我有这样的业务：大数据高并发的读某一条数据，非常非常多的读，但写很少，比如一张XX门的热门图片，有很多很多的请求来自不同的用户都需要这个条数据的缓存，多点模型即是完美的选择。我许多slave节点上都有它的缓存，而很少更新，则可最大限度的享用到多点模型带来的性能提升。

            还有一些问题，不得不说一下。就是down机和定期缓存更新的问题。先说宕机，很显然，缓存是slave节点中的服务进程的内存，一旦节点宕机，缓存就丢失了，这时就需要前面我提到过的“重建缓存”，这通常是由master节点发出的，master节点负责监控各个slave节点（当然也可以是其它master节点）的运行状况，如果发现某个slave节点宕机（没有了“心跳”，如果你了解一些Hadoop，你会发现它也是这样工作的），则在slave节点重新运行之后（可能进行了重启），master节点将通知该slave节点，重建其所负责的数据的缓存，从哪重建，当然是从数据库了，这需要一定的时间（在我们拥有百万用户之后，重建一个slave节点所负责的数据的缓存通常需要几分钟），那么，从宕机到slave节点重建缓存完毕这一段时间，服务由谁提供呢？显然备份节点就出马了。其实在单点模型下，如果考虑了备份节点，则其实所有的请求都是多点模型。只不过备份节点并不是总是会更新它的缓存，而是定期，或收到Notification时，才会进行更新。master节点在发现某个slave节点宕机后，可以马上指向含有同样数据的备份节点，保证缓存服务不中断。那么，备份节点的缓存数据是否是最新的呢？有可能不是。虽然，通常每次对数据库完成批量更新后，都会通知备份节点，去更新这些缓存，但还是有可能存在不一致的情况。所以，备份节点的工作方式，是特别的，即对于每次请求的缓存都采用Pull（拉）方式，如何Pull？前面提到的版本管理系统再次出马，即每次读之前，先比较版本，再读，写也是一样的。所以，备份节点的性能，并不会很高，而且，通常需要同时负责几个slave节点的数据的备份，所以，存在被冲垮的可能性，还需要slave节点尽快恢复，然后把服务工作重新还给它。

            再说定期缓存更新的问题。通常，所有的slave节点，都会被部署在夜深人静的某个时候（如02:00~06:00），用户很少的时候，定期进行缓存更新，以尽可能保证数据的同步和一致性，且第二天上午，大量请求到达时，基本都能从缓存返回最新数据。而备份节点，则可能每30分钟，就进行一次缓存更新。咦？前面你不是说，备份节点上每次读都要Pull，比较版本并更新缓存，才会返回吗？是的，那为什么还要定期更新呢？答案非常简单，因为如果大部分缓存都是最新的数据，只比较版本而没有实际的更新操作，所消耗的性能很小很小，所以定期更新，在发生slave节点宕机转由备份节点工作的时候，有很大的帮助。

          最后，再说一下Push（推送）方式，即，每次有数据改动，都强制去更新所有缓存。这种方式很消耗性能，但更能保证实时性。而通常我们使用的，都是Pull（拉）方式，即无论是定期更新缓存，还是收到Notification（虽然收通知是被“推”了一把）后更新缓存，其实都是拉，把新的数据拉过来，就好了。在实际的系统中，两种方式都有，还是那句话，看需求，再决定处理方式。





         面对越来越多的客户，千万级以上PV的网站，普通布局的系统已经无法满足客户需求，设计出一套高并发的分布式处理系统，就显得尤为重要。系统需要包含业务逻辑的处理、交易、注册、资讯、各种计算、存储、日志、备份等方面内容，可用于类微博，SNS，广告推送，邮件等有大量线上并发请求的场景。

        如何提高系统的大流量高并发？说起来很简单，就是“分”，如何“分”，简单的说就是把不同的业务分拆到不同的服务器上去跑（垂直拆分），相同的业务压力分拆到不同的服务器去跑（水平拆分），并时刻不要忘记备份、扩展、意外处理等讨厌的问题。说起来都比较简单，但设计和实现起来，就会比较困难。

        首先来看，我们的数据应该如何存储和取用。根据我们之前确定的“分”的方法，先确定以下2点：

（1）我们的分布式系统，按不同的业务，存储不同的数据；（2）同样的业务，同一个数据应存储多份，其中有的存储提供读写，而有的存储只提供读。

        （1）应该容易理解，比如说，我这套系统用于微博（就假想我们做一个山寨的推特吧，给他个命名就叫“山推” 好了，以下都叫山推，Stwi），那么，“我关注的人”这一个业务的数据，肯定和“我发了的推文”这个业务的数据是分开存储的，那么我们现在把，每一个业务所负责的数据的存储，称为一个group。即以group的方式，来负责各个业务的数据的存储。接下来说（2），现在我们已经知道，数据按业务拆到group里面去存取，那么一个group里面又应该有哪些角色呢？自然的，应该有一台主要的机器，作为group的核心，我们称它为Group
 Master，是的，它就是这个group的主要代表。这个group的数据，在Group Master上应该都能找到，进行读写。另外，我们还需要一些辅助角色，我们称它们为Group Slaves，这些slave机器做啥工作呢？它们负责去Group Master处拿数据，并尽量保持和它同步，并提供读服务。请注意我的用词，“尽量”，稍后将会解释。现在我们已经有了一个group的基本轮廓：



         一个group提供对外的接口（废话否则怎么存取数据），group的底层可以是实际的File System，甚至是HDFS。Group Master和Group Slave可以共享同一个File System（用于不能丢数据的强一致性系统），也可以分别指向不同的File System（用于弱一致性，允许停写服务和系统宕机时丢数据的系统），但总之应认为这个”File System”是无状态，有状态的是Group Master和各个Group Slave。

       下面来说一个group如何工作，同步等核心问题。首先，一个group的Group Master和Group Slave间应保持强一致性还是弱一致性（最终一致性）应取决于具体的业务需求，以我们的“山推”来说，Group Master和Group Slave并不要求保持强一致性，而弱一致性（最终一致性）即能满足要求，为什么？因为对于“山推”来讲，一个Group Master写了一个数据，而另一个Group Slave被读到一个“过期”（因为Group Master已经写，但此Group Slave还未更新此数据）的数据通常并不会带来大问题，比如，我在“山推”上发了一个推文，“关注我的人”并没有即时同步地看到我的最新推文，并没有太大影响，只要“稍后”它们能看到最新的数据即可，这就是所谓的最终一致性。但当Group
 Master挂掉时，写服务将中断一小段时间由其它Group Slave来顶替，稍后还要再讲这个问题。假如我们要做的系统不是山推，而是淘宝购物车，支付宝一类的，那么弱一致性（最终一致性）则很难满足要求，同时写服务挂掉也是不能忍受的，对于这样的系统，应保证“强一致性”，保证不能丢失任何数据。

       接下来还是以我们的“山推“为例，看看一个group如何完成数据同步。假设，现在我有一个请求要写一个数据，由于只有Group Master能写，那么Group Master将接受这个写请求，并加入写的队列，然后Group Master将通知所有Group Slave来更新这个数据，之后这个数据才真正被写入File System。那么现在就有一个问题，是否应等所有Group Slave都更新了这个数据，才算写成功了呢？这里涉及一些NWR的概念，我们作一个取舍，即至少有一个Group Slave同步成功，才能返回写请求的成功。这是为什么呢？因为假如这时候Group
 Master突然挂掉了，那么我们至少可以找到一台Group Slave保持和Group Master完全同步的数据并顶替它继续工作，剩下的、其它的Group Slave将“异步”地更新这个新数据，很显然，假如现在有多个读请求过来并到达不同的Group Slave节点，它们很可能读到不一样的数据，但最终这些数据会一致，如前所述。我们做的这种取舍，叫“半同步”模式。那之前所说的强一致性系统应如何工作呢？很显然，必须得等所有Group Slave都同步完成才能返回写成功，这样Group Master挂了，没事，其它Group
 Slave顶上就行，不会丢失数据，但是付出的代价就是，等待同步的时间。假如我们的group是跨机房、跨地区分布的，那么等待所有Group Slave同步完成将是很大的性能挑战。所以综合考虑，除了对某些特别的系统，采用“最终一致性”和“半同步”工作的系统，是符合高并发线上应用需求的。而且，还有一个非常重要的原因，就是通常线上的请求都是读&gt;&gt;写，这也正是“最终一致性”符合的应用场景。

         刚才我们曾提到，如果Group Master宕机挂掉，至少可以找到一个和它保持同不的Group Slave来顶替它继续工作，其它的Group Slave则“尽量”保持和Group Master同步，如前文所述。那么这是如何做到的呢？这里涉及到“分布式选举”的概念，如Paxos协议，通过分布式选举，总能找到一个最接近Group Master的Group Slave，来顶替它，从而保证系统的可持续工作。当然，在此过程中，对于最终一致性系统，仍然会有一小段时间的写服务中断。现在继续假设，我们的“山推”已经有了一些规模，而负责“山推”推文的这个group也有了五台机器，并跨机房，跨地区分布，按照上述设计，无论哪个机房断电或机器故障，都不会影响这个group的正常工作，只是会有一些小的影响而已。

         那么对于这个group，还剩2个问题，一是如何知道Group Master挂掉了呢？二是在图中我们已经看到Group Slave是可扩展的，那么新加入的Group Slave应如何去“偷”数据从而逐渐和其它节点同步呢？对于问题一，我们的方案是这样的，另外提供一个类似“心跳”的服务（由谁提供呢，后面我们将讲到的Global Master将派上用场），group内所有节点无论是Group Master还是Group Slave都不停地向这个“心跳”服务去申请一个证书，或认为是一把锁，并且这个锁是有时间的，会过期。“心跳”服务定期检查Group
 Master的锁和其有效性，一旦过期，如果Group Master工作正常，它将锁延期并继续工作，否则说明Group Master挂掉，由其它Group Slave竞争得到此锁（分布式选举），从而变成新的Group Master。对于问题二，则很简单，新加入的Group Slave不断地“偷”老数据，而新数据总由于Group Master通知其更新，最终与其它所有结点同步。（当然，“偷”数据所用的时间并不乐观，通常在小时级别）


          我们完成了在此分布式系统中，一个group的设计。那么接下来，我们设计系统的其他部分。如前文所述，我们的业务及其数据以group为单位，显然在此系统中将存在many many的groups（别告诉我你的网站总共有一个业务，像我们的“山推”，那业务是一堆一堆地），那么由谁来管理这些groups呢？由Web过来的请求，又将如何到达指定的group，并由该group处理它的请求呢？这就是我们要讨论的问题。

          我们引入了一个新的角色——Global Master，顾名思义，它是管理全局的一个节点，它主要完成如下工作：（1）管理系统全局配置，发送全局控制信息；（2）监控各个group的工作状态，提供心跳服务，若发现宕机，通知该group发起分布式选举产生新的Group Master；（3）处理Client端首次到达的请求，找出负责处理该请求的group并将此group的信息（location）返回，则来自同一个前端请求源的该类业务请求自第二次起不需要再向Global Master查询group信息（缓存机制）；（4）保持和Global
 Slave的强一致性同步，保持自身健康状态并向全局的“心跳”服务验证自身的状态。

         现在我们结合图来逐条解释上述工作，显然，这个系统的完整轮廓已经初现。



         首先要明确，不管我们的系统如何“分布式”，总之会有至少一个最主要的节点，术语可称为primary node，如图所示，我们的系统中，这个节点叫Global Master，也许读过GFS + Bigtable论文的同学知道，在GFS + Bigtable里，这样的节点叫Config Master，虽然名称不一样，但所做的事情却差不多。这个主要的Global Master可认为是系统状态健康的标志之一，只要它在正常工作，那么基本可以保证整个系统的状态是基本正常的（什么？group或其他结点会不正常不工作？前面已经说过，group内会通过“分布式选举”来保证自己组内的正常工作状态，不要告诉我group内所有机器都挂掉了，那个概率我想要忽略它），假如Global
 Master不正常了，挂掉了，怎么办？显然，图中的Global Slave就派上用场了，在我们设计的这个“山推”系统中，至少有一个Global Slave，和Global Master保持“强一致性”的完全同步，当然，如果有不止一个Global Slave，它们也都和Global Master保持强一致性完全同步，这样有个好处，假如Global Master挂掉，不用停写服务，不用进行分布式选举，更不会读服务，随便找一个Global Slave顶替Global Master工作即可。这就是强一致性最大的好处。那么有的同学就会问，为什么我们之前的group，不能这么搞，非要搞什么最终一致性，搞什么分布式选举（Paxos协议属于既难理解又难实现的坑爹一族）呢？我告诉你，还是压力，压力。我们的系统是面向日均千万级PV以上的网站（“山推”嘛，推特是亿级PV，我们千万级也不过分吧），但系统的压力主要在哪呢？细心的同学就会发现，系统的压力并不在Global
 Master，更不会在Global Slave，因为他们根本不提供数据的读写服务！是的，系统的压力正是在各个group，所以group的设计才是最关键的。同时，细心的同学也发现了，由于Global Master存放的是各个group的信息和状态，而不是用户存取的数据，所以它更新较少，也不能认为读&gt;&gt;写，这是不成立的，所以，Global Slave和Global Master保持强一致性完全同步，正是最好的选择。所以我们的系统，一台Global Master和一台Global Slave，暂时可以满足需求了。

         现在已经了解Global Master的大概用途，那么，一个来自Client端的请求，如何到达真正的业务group去呢？在这里，Global Master将提供“首次查询”服务，即，新请求首次请求指定的group时，通过Global Master获得相应的group的信息，以后，Client将使用该信息直接尝试访问对应的group并提交请求，如果group信息已过期或是不正确，group将拒绝处理该请求并让Client重新向Global Master请求新的group信息。显然，我们的系统要求Client端缓存group的信息，避免多次重复地向Global
 Master查询group信息。这里其实又挖了许多烂坑等着我们去跳，首先，这样的工作模式满足基本的Ddos攻击条件，这得通过其他安全性措施来解决，避免group总是收到不正确的Client请求而拒绝为其服务；其次，当出现大量“首次”访问时，Global Master尽管只提供查询group信息的读服务，仍有可能不堪重负而挂掉，所以，这里仍有很大的优化空间，比较容易想到的就是采用DNS负载均衡，因为Global Master和其Global Slave保持完全同步，所以DNS负载均衡可以有效地解决“首次”查询时Global
 Master的压力问题；再者，这个工作模式要求Client端缓存由Global Master查询得到的group的信息，万一Client不缓存怎么办？呵呵，不用担心，Client端的API也是由我们设计的，之后才面向Web前端。

          之后要说的，就是图中的“Global Heartbeat”，这又是个什么东西呢？可认为这是一个管理Global Master和Global Slave的节点，Global Master和各个Global Slave都不停向Global Heartbeat竞争成为Global Master，如果Global Master正常工作，定期更新其状态并延期其获得的锁，否则由Global Slave替换之，原理和group内的“心跳”一样，但不同的是，此处Global Master和Global Slave是强一致性的完全同步，不需要分布式选举。有同学可能又要问了，假如Global
 Heartbeat挂掉了呢？我只能告诉你，这个很不常见，因为它没有任何压力，而且挂掉了必须人工干预才能修复。在GFS + Bigtable里，这个Global Heartbeat叫做Lock Service。


          现在接着设计我们的“山推”系统。有了前面两篇的铺垫，我们的系统现在已经有了五脏六腑，剩下的工作就是要让其羽翼丰满。那么，是时候，放出我们的“山推”系统全貌了：



（1）整个系统由N台机器组合而成，其中Global Master一台，Global Slave一台到多台，两者之间保持强一致性并完全同步，可由Global Slave随时顶替Global Master工作，它们被Global Heartbeat（一台）来管理，保证有一个Global Master正常工作；Global Heartbeat由于无压力，通常认为其不能挂掉，如果它挂掉了，则必须人工干预才能恢复正常；

（2）整个系统由多个groups合成，每一个group负责相应业务的数据的存取，它们是数据节点，是真正抗压力的地方，每一个group由一个Group Master和一个到多个Group Slave构成，Group Master作为该group的主节点，提供读和写，而Group Slave则只提供读服务且保证这些Group Slave节点中，至少有一个和Group Master保持完全同步，剩余的Group Slave和Group Master能够达到最终一致，它们之间以“半同步”模式工作保证最终一致性；

（3）每一个group的健康状态由Global Master来管理，Global Master向group发送管理信息，并保证有一个Group Master正常工作，若Group Master宕机，在该group内通过分布式选举产生新的Group Master顶替原来宕机的机器继续工作，但仍然有一小段时间需要中断写服务来切换新的Group Master；

（4）每一个group的底层是实际的存储系统，File system，它们是无状态的，即，由分布式选举产生的Group Master可以在原来的File system上继续工作；

（5）Client的上端可认为是Web请求，Client在“首次”进行数据读写时，向Global Master查询相应的group信息，并将其缓存，后续将直接与相应的group进行通信；为避免大量“首次”查询冲垮Global Master，在Client与Global Master之间增加DNS负载均衡，可由Global Slave分担部分查询工作；

（6）当Client已经拥有足够的group信息时，它将直接与group通信进行工作，从而真正的压力和流量由各个group分担，并处理完成需要的工作。

         好了，现在我们的“山推”系统设计完成了，但是要将它编码实现，还有很远的路要走，细枝末节的问题也会暴露更多。如果该系统用于线上计算，如有大量的Map-Reduce运行于group中，系统将会更复杂，因为此时不光考虑的数据的存储同步问题，操作也需要同步。现在来检验下我们设计的“山推”系统，主要分布式指标：

        一致性：如前文所述，Global机器强一致性，Group机器最终一致性；

       可用性：Global机器保证了HA（高可用性），Group机器则不保证，但满足了分区容错性；

       备份Replication：Global机器采用完全同步，Group机器则是半同步模式，都可以进行横向扩展；

       故障恢复：如前文所述，Global机器完全同步，故障可不受中断由slave恢复工作，但Group机器采用分布式选举和最终一致性，故障时有较短时间的写服务需要中断并切换到slave机器，但读服务可不中断。

       还有其他一些指标，这里就不再多说了。还有一些细节，需要提一下，比如之前的评论中有同学提到，group中master挂时，由slave去顶替，但这样一来该group内其他所有slave需要分担之前成这新master的这个slave的压力，有可能继续挂掉而造成雪崩。针对此种情况，可采用如下做法：即在一个group内，至少还存在一个真正做“备份”用途的slave，平时不抗压力，只同步数据，这样当出现上述情况时，可由该备份slave来顶替成为新master的那个slave，从而避免雪崩效应。不过这样一来，就有新的问题，由于备份slave平时不抗压力，加入抗压力后必然产生一定的数据迁移，数据迁移也是一个较麻烦的问题。常采用的分摊压力做法如一致性Hash算法（环状Hash），可将新结点加入对整个group的影响降到较小的程度。

        另外，还有一个较为棘手的问题，就是系统的日志处理，主要是系统宕机后如何恢复之前的操作日志。比较常见的方法是对日志作快照（Snapshot）和回放点（checkpoint），并采用Copy-on-write方式定期将日志作snapshot存储，当发现宕机后，找出对应的回放点并恢复之后的snapshot，但此时仍可能有新的写操作到达，并产生不一致，这里主要依靠Copy-on-write来同步。

        最后再说说图中的Client部分。显然这个模块就是面向Web的接口，后面连接我们的“山推”系统，它可以包含诸多业务逻辑，最重要的，是要缓存group的信息。在Client和Web之间，还可以有诸如Nginx之类的反向代理服务器存在，做进一步性能提升，这已经超出了本文的范畴，但我们必须明白的是，一个高并发高性能的网站，对性能的要求是从起点开始的，何为起点，即用户的浏览器。

        现在，让我们来看看GFS的设计：



           很明显，这么牛的系统我是设计不出来的，我们的“山推”，就是在学习GFS + Bigtable的主要思想。说到这，也必须提一句，可能我文章中，名词摆的有点多了，如NWR，分布式选举，Paxos包括Copy-on-write等，有兴趣的同学可自行google了解。因为说实在的，这些概念我也没法讲透彻，只是一知半解。另外，大家可参考一些分布式项目的设计，如Cassandra，包括淘宝的Oceanbase等，以加深理解。

      


          译文出自：Linux
 story-FOREST 




         对于每个系统管理员或网络管理员来说，每天要监控和调试 Linux 系统性能问题都是非常困难的工作。我已经有5年 Linux 管理员的工作经历，知道如何监控系统使其保持正常运行。为此，我们编写了对于 Linux/Unix 系统管理员非常有用的并且最常用的20个命令行系统监视工具。这些命令可以在所有版本的 Linux 下使用去监控和查找系统性能的实际原因。这些监控命令足够你选择适合你的监控场景。

1. top —Linux系统进程监控

top 命令是性能监控程序，它可以在很多 Linux/Unix 版本下使用，并且它也是 Linux 系统管理员经常使用的监控系统性能的工具。Top 命令可以定期显示所有正在运行和实际运行并且更新到列表中，它显示出 CPU 的使用、内存的使用、交换内存、缓存大小、缓冲区大小、过程控制、用户和更多命令。它也会显示内存和 CPU 使用率过高的正在运行的进程。当我们对
 Linux 系统需要去监控和采取正确的行动时，top 命令对于系统管理员是非常有用的。让我们看下 top 命令的实际操作。

# top



2. vmstat — 虚拟内存统计

vmstat 命令是用于显示虚拟内存、内核线程、磁盘、系统进程、I/O 模块、中断、CPU 活跃状态等更多信息。在默认的情况下，Linux 系统是没有 vmstat 这个命令的，如果你要使用它，必须安装一个包名叫 sysstat 的程序包。命令格式常用用法如下：







1

2

3

4




#
 vmstat

procs
 -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----

 r 
 b   swpd   free  inact active   si   so    bi    bo   in  

cs us sy id wa st

 1 

0      
0 
810420  
97380  
70628    
0    
0   
115     
4   
89   
79  
1  
6 
90  
3  
0








3. lsof — 打开文件列表

lsof 命令对于很多 Linux/Unix 系统都可以使用，主要以列表的形式显示打开的文件和进程。

打开的文件主要包括磁盘文件、网络套接字、管道、设备和进程。使用这个命令的主要原因是一个一个盘不能卸载并且显示文件正在使用或者打开的错误信息。这个命令很容易看出哪些文件正在使用。这个命令最常用的格式：







1

2

3

4

5

6

7

8

9

10

11

12




#
 lsof

 

COMMAND    
 PID      USER   FD      TYPE     DEVICE     SIZE       NODE NAME

init         
1     

root  cwd       DIR      104,2    

4096          
2 
/

init         
1     

root  rtd       DIR      104,2    

4096          
2 
/

init         
1     

root  txt       REG      104,2   

38652   
17710339 
/sbin/init

init         
1     

root  mem       REG      104,2  

129900     
196453 
/lib/ld-2.5.so

init         
1     

root  mem       REG      104,2 

1693812     
196454 
/lib/libc-2.5.so

init         
1     

root  mem       REG      104,2   

20668     
196479 
/lib/libdl-2.5.so

init         
1     

root  mem       REG      104,2  

245376     
196419 
/lib/libsepol.so.1

init         
1     

root  mem       REG      104,2   

93508     
196431 
/lib/libselinux.so.1

init         
1     

root   10u     FIFO       0,17                

953 
/dev/initctl








4. tcpdump — 网络数据包分析器

tcpdump 是一种使用最广泛的命令行网络数据包分析器或数据包嗅探程序，主要用于捕获和过滤 TCP/IP 包收到或者转移在一个网络的特定借口信息。它也提供了一个选项参数去保存将捕获的包在一个文件中用于以后分析使用，tcpdump 几乎在所有的
 Linux 版本中都是可用的。







1

2

3

4

5

6

7




#
 tcpdump -i eth0

 

tcpdump:
 verbose output suppressed, use

-v or -vv for

full protocol decode

listening
 on eth0, link-type EN10MB (Ethernet), capture size 96

bytes

22:08:59.617628

IP tecmint.com.ssh &gt; 115.113.134.3.static-mumbai.vsnl.net.in.28472:
 P 2532133365:2532133481(116)
 ack 3561562349

win 9648

22:09:07.653466

IP tecmint.com.ssh &gt; 115.113.134.3.static-mumbai.vsnl.net.in.28472:
 P 116:232(116)
 ack 1

win 9648

22:08:59.617916

IP 115.113.134.3.static-mumbai.vsnl.net.in.28472

&gt; tecmint.com.ssh: . ack 116

win 64347








5. netstat — 网络统计

netstat 命令是一个监控网络数据包传入和传出的统计界面的命令行工具。它对于许多系统管理员去监控网络性能和解决网络相关问题是一个非常有用的工具。







1

2

3

4

5

6

7




#
 tcpdump -i eth0

 

tcpdump:
 verbose output suppressed, use

-v or -vv for

full protocol decode

listening
 on eth0, link-type EN10MB (Ethernet), capture size 96

bytes

22:08:59.617628

IP tecmint.com.ssh &gt; 115.113.134.3.static-mumbai.vsnl.net.in.28472:
 P 2532133365:2532133481(116)
 ack 3561562349

win 9648

22:09:07.653466

IP tecmint.com.ssh &gt; 115.113.134.3.static-mumbai.vsnl.net.in.28472:
 P 116:232(116)
 ack 1

win 9648

22:08:59.617916

IP 115.113.134.3.static-mumbai.vsnl.net.in.28472

&gt; tecmint.com.ssh: . ack 116

win 64347








6. htop — 进程监控

htop 是一个更加先进的交互式的实时监控工具。htop 与 top 命令非常相似，但是他有一些非常丰富的功能，如用户友好界面管理进程、快捷键、横向和纵向进程等更多的。htop 是一个第三方工具并不包括在
 Linux 系统中，你需要使用包管理工具进行安装。







1




#
 htop










7. iotop — 监控 Linux 磁盘 I/O

iotop 也是和 top 和 htop 命令相似，但是它会有一个报告功能去监控和显示实时的磁盘I/O 输入和输出和程序进程。这个工具对于查找精确的高的磁盘读/写过程是非常有用的。







1




#
 iotop








8. iostat — 输入/输出统计

iostat 是收集和展示系统输入和输出存储设备统计的简单工具。这个工具通常用于查找存储设备性能问题，包括设备、本地磁盘、例如 NFS 远程磁盘。







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15




#
 iostat

 

Linux
2.6.18-238.9.1.el5
 (tecmint.com)         09/13/2012

 

avg-cpu: 
 %user   %nice %system %iowait  %steal   %idle

           2.60   

3.65    
1.04    
4.29    
0.00   
88.42

 

Device:           
 tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn

cciss/c0d0      
17.79      

545.80       
256.52  
855159769  
401914750

cciss/c0d0p1     
0.00        

0.00         
0.00       
5459       
3518

cciss/c0d0p2    
16.45      

533.97       
245.18  
836631746  
384153384

cciss/c0d0p3     
0.63        

5.58         
3.97    
8737650    
6215544

cciss/c0d0p4     
0.00        

0.00         
0.00          
8          
0

cciss/c0d0p5     
0.63        

3.79         
5.03    
5936778    
7882528

cciss/c0d0p6     
0.08        

2.46         
2.34    
3847771    
3659776








9. IPTraf —实时IP局域网监控

IPTraf 是一个基于开源的 Linux 系统实时网络（IP 网络）监测的工具。它能收集到各种各样的信息，如通过网络对 IP 流量监测，包括 TCP 标志信息、ICMP 详细细节、TCP/UDP 流量故障、TCP 连接的数据包和拜恩计数。并且它还收集 TCP，UDP，ICMP，IP，非 IP，IP 校验错误，界面活性等一般信息和详细信息的接口统计数据。



10. Psacct 或者 Acct — 监视用户活动

Psacct 或者 Acct 是用于监测每个用户对系统的活跃状态的一个非常有用的工具。在后台有两个守护进程在运行，一个是密切关注系统上每个用户的整体活动，另一个进程关注有哪些资源被它们消耗。

   这个工具对于系统管理员是非常有用的去跟踪每个用户的活动，可以知道用户正在做什么，发出了什么样的命令，占用了多少资源，多长时间活跃在系统上。

11. Monit — 程序和服务监测

这是一个免费的开源的基于 web 程序的自动监控和管理系统进程、程序、文件、目录、权限、校验文件系统。它监控的服务包括 Apache、MYSQL、Mail、FTP、Nginx 等等。系统状态是可以从命令行或者自己的网络接口来查看。



12. NetHogs — 监视每个进程的网络带宽

NetHogs 是一个开源的漂亮的小程序（类似于 Linux 上面的 top 命令），在您的系统上保持每个进程的网络活动状态。它也保持了一个程序或者应用实时的网络流量带宽使用情况。



13. iftop — 网络带宽监控

iftop 是另一个基于终端的开源的系统监测工具，主要功能是通过你自己系统上的网络接口显示一个经常更新的网络带宽利用率的列表（即源主机和目的主机）。iftop 监控的是网络的使用情况，而 top 监控的是 CPU 的使用情况。iftop 监视一个选定的接口并且显示两台主机之间当前宽带的使用情况。



14. Monitorix — 系统和网络监控

Monitorix 是一个尽可能多的在 Linux/Unix 上一个轻量级监控工具，主要设计是监控正在运行的系统和网络资源。它有一个内置的 HTTP web 服务去定期收集系统和网络信息并显示成图片。它可以监视系统的平均负载使用、内存的分配、磁盘驱动器、系统服务、网络端口、邮件统计（Sendmail、Postfix、Dovecot 等等）、MYSQL 数据库等等更多的服务。它的主要目的是监控整个系统的性能，并且有助于监测故障、瓶颈、异常活动等状况。



15. Arpwatch — 以太网活动监控器

Arpwatch是一种用来监视 Linux 网络的以太网的网络流量的地址解析（网络地址转换）的一个程序。它一直随着网络时间戳的变化监视以太网流量和产生日志的 IP 和 MAC 地址对。当一个 IP 地址或 MAC 地址对发生变化的时候，它会发送电子邮件通知管理员。

并且它在检测 ARP 攻击是非常有用的。

16. Suricata — 网络安全监控

Suricata 是一个高性能的开源的网络安全与入侵检测与预防 Linux、FreeBSD、Windows等操作系统的监控工具。它是一个非营利基金 OISF（Open
 Information Security Foundation）拥有的。

17. VnStat PHP — 监测网络带宽

VnStat PHP 是一个 web 前端应用最流行的社交工具叫“vnstat”。 VnStat
 PHP 使用了很好的图形模式监控网络流量的使用情况。它显示了每时、每天、每月的总结报告中的网络流量使用情况。

18. Nagios — 网络/服务器监控

Nagios 是一个领先的开源的强大的监控系统，网络/系统管理员在他们影响主要业务流程之前识别和解决服务器相关的问题。Nagios 可以监控远程 Linux、Windows、开关、单窗口的路由器和打印机。它能显示你的网络和服务器关键的告警，有利于在错误反生之前帮助你解决问题。

19. Nmon — 监控Linux系统性能

Nmon（即奈吉尔性能监视器）工具用来监视 Linux 系统的所有资源包括：CPU、内存、磁盘使用率、网络上的进程、NFS、内核等等。这个工具有两个模式：即在线模式和捕捉模式。在线模式适用于实时监控，捕捉模式用于存储输出为 CSV 格式后的处理。

  

20. Collectl — 一体化性能检测工具

Collectl 是另一个功能强大的基于命令行的监控工具，它可用于收集有关系统资源的信息，包括 CPU 使用率、内存、网络、节点、进程、NFS、TCP 套接等等。



        我们想知道你使用什么样的监控程序来监控你的服务器性能？如果我们错过你想要的任何工具，请通过评论告知我们，并且不要忘记分享它。
    
   


        MySQL凭借着出色的性能、低廉的成本、丰富的资源，已经成为绝大多数互联网公司的首选关系型数据库。虽然性能出色，但所谓“好马配好鞍”，如何能够更好的使用它，已经成为开发工程师的必修课，我们经常会从职位描述上看到诸如“精通MySQL”、“SQL语句优化”、“了解数据库原理”等要求。我们知道一般的应用系统，读写比例在10:1左右，而且插入操作和一般的更新操作很少出现性能问题，遇到最多的，也是最容易出问题的，还是一些复杂的查询操作，所以查询语句的优化显然是重中之重。

         之前在核心业务系统部做慢查询的优化工作，共计十余个系统，累计解决和积累了上百个慢查询案例。随着业务的复杂性提升，遇到的问题千奇百怪，五花八门，匪夷所思。本文旨在以开发工程师的角度来解释数据库索引的原理和如何优化慢查询。

一个慢查询引发的思考







1

2

3

4

5

6

7

8

9

10




select

   count(*)

from

   task

where

   status=2

   andoperator_id=20839

   andoperate_time&gt;1371169729

   andoperate_time&lt;1371174603

   andtype=2;








系统使用者反应有一个功能越来越慢，于是工程师找到了上面的SQL。

并且兴致冲冲的找到了我，“这个SQL需要优化，给我把每个字段都加上索引”

我很惊讶，问道“为什么需要每个字段都加上索引？”

“把查询的字段都加上索引会更快”工程师信心满满

“这种情况完全可以建一个联合索引，因为是最左前缀匹配，所以operate_time需要放到最后，而且还需要把其他相关的查询都拿来，需要做一个综合评估。”

“联合索引？最左前缀匹配？综合评估？”工程师不禁陷入了沉思。

多数情况下，我们知道索引能够提高查询效率，但应该如何建立索引？索引的顺序如何？许多人却只知道大概。其实理解这些概念并不难，而且索引的原理远没有想象的那么复杂。

MySQL索引原理

索引目的

索引的目的在于提高查询效率，可以类比字典，如果要查“mysql”这个单词，我们肯定需要定位到m字母，然后从下往下找到y字母，再找到剩下的sql。如果没有索引，那么你可能需要把所有单词看一遍才能找到你想要的，如果我想找到m开头的单词呢？或者ze开头的单词呢？是不是觉得如果没有索引，这个事情根本无法完成？

索引原理

除了词典，生活中随处可见索引的例子，如火车站的车次表、图书的目录等。它们的原理都是一样的，通过不断的缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是我们总是通过同一种查找方式来锁定数据。

数据库也是一样，但显然要复杂许多，因为不仅面临着等值查询，还有范围查询(&gt;、&lt;、between、in)、模糊查询(like)、并集查询(or)等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？最简单的如果1000条数据，1到100分成第一段，101到200分成第二段，201到300分成第三段……这样查第250条数据，只要找第三段就可以了，一下子去除了90%的无效数据。但如果是1千万的记录呢，分成几段比较好？稍有算法基础的同学会想到搜索树，其平均复杂度是lgN，具有不错的查询性能。但这里我们忽略了一个关键的问题，复杂度模型是基于每次相同的操作成本来考虑的，数据库实现比较复杂，数据保存在磁盘上，而为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。

磁盘IO与预读

前面提到了访问磁盘，那么这里先简单介绍一下磁盘IO和预读，磁盘读取数据靠的是机械运动，每次读取数据花费的时间可以分为寻道时间、旋转延迟、传输时间三个部分，寻道时间指的是磁臂移动到指定磁道所需要的时间，主流磁盘一般在5ms以下；旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘7200转，表示每分钟能转7200次，也就是说1秒钟能转120次，旋转延迟就是1/120/2 = 4.17ms；传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略不计。那么访问一次磁盘的时间，即一次磁盘IO的时间约等于5+4.17
 = 9ms左右，听起来还挺不错的，但要知道一台500 -MIPS的机器每秒可以执行5亿条指令，因为指令依靠的是电的性质，换句话说执行一次IO的时间可以执行40万条指令，数据库动辄十万百万乃至千万级数据，每次9毫秒的时间，显然是个灾难。下图是计算机硬件延迟的对比图，供大家参考：



考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。

索引的数据结构

前面讲了生活中索引的例子，索引的基本原理，数据库的复杂性，又讲了操作系统的相关知识，目的就是让大家了解，任何一种数据结构都不是凭空产生的，一定会有它的背景和使用场景，我们现在总结一下，我们需要这种数据结构能够做些什么，其实很简单，那就是：每次查找数据时把磁盘IO次数控制在一个很小的数量级，最好是常数数量级。那么我们就想到如果一个高度可控的多路搜索树是否能满足需求呢？就这样，b+树应运而生。

详解b+树



如上图，是一颗b+树，关于b+树的定义可以参见B+树，这里只说一些重点，浅蓝色的块我们称之为一个磁盘块，可以看到每个磁盘块包含几个数据项（深蓝色所示）和指针（黄色所示），如磁盘块1包含数据项17和35，包含指针P1、P2、P3，P1表示小于17的磁盘块，P2表示在17和35之间的磁盘块，P3表示大于35的磁盘块。真实的数据存在于叶子节点即3、5、9、10、13、15、28、29、36、60、75、79、90、99。非叶子节点只不存储真实的数据，只存储指引搜索方向的数据项，如17、35并不真实存在于数据表中。

b+树的查找过程

如图所示，如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短（相比磁盘的IO）可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。真实的情况是，3层的b+树可以表示上百万的数据，如果上百万的数据查找只需要三次IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次IO，那么总共需要百万次的IO，显然成本非常非常高。

b+树性质

1.通过上面的分析，我们知道IO次数取决于b+数的高度h，假设当前数据表的数据为N，每个磁盘块的数据项的数量是m，则有h=㏒(m+1)N，当数据量N一定的情况下，m越大，h越小；而m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的，如果数据项占的空间越小，数据项的数量越多，树的高度越低。这就是为什么每个数据项，即索引字段要尽量的小，比如int占4字节，要比bigint8字节少一半。这也是为什么b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于1时将会退化成线性表。

2.当b+树的数据项是复合的数据结构，比如(name,age,sex)的时候，b+数是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F)这样的数据来检索的时候，b+树会优先比较name来确定下一步的所搜方向，如果name相同再依次比较age和sex，最后得到检索的数据；但当(20,F)这样的没有name的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去哪里查询。比如当(张三,F)这样的数据来检索时，b+树可以用name来指定搜索方向，但下一个字段age的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是F的数据了，
 这个是非常重要的性质，即索引的最左匹配特性。

慢查询优化

关于MySQL索引原理是比较枯燥的东西，大家只需要有一个感性的认识，并不需要理解得非常透彻和深入。我们回头来看看一开始我们说的慢查询，了解完索引原理之后，大家是不是有什么想法呢？先总结一下索引的几大基本原则

建索引的几大原则

1.最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。

2.=和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式

3.尽量选择区分度高的列作为索引,区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录

4.索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’);

5.尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可

回到开始的慢查询

根据最左匹配原则，最开始的sql语句的索引应该是status、operator_id、type、operate_time的联合索引；其中status、operator_id、type的顺序可以颠倒，所以我才会说，把这个表的所有相关查询都找到，会综合分析；
比如还有如下查询







1

2




select*fromtaskwherestatus
 = 0 andtype
 = 12 limit 10;

selectcount(*)fromtaskwherestatus
 = 0 ;








那么索引建立成(status,type,operator_id,operate_time)就是非常正确的，因为可以覆盖到所有情况。这个就是利用了索引的最左匹配的原则

查询优化神器 – explain命令

关于explain命令相信大家并不陌生，具体用法和字段含义可以参考官网explain-output，这里需要强调rows是核心指标，绝大部分rows小的语句执行一定很快（有例外，下面会讲到）。所以优化语句基本上都是在优化rows。

慢查询优化基本步骤

0.先运行看看是否真的很慢，注意设置SQL_NO_CACHE
1.where条件单表查，锁定最小返回记录表。这句话的意思是把查询语句的where都应用到表中返回的记录数最小的表开始查起，单表每个字段分别查询，看哪个字段的区分度最高
2.explain查看执行计划，是否与1预期一致（从锁定记录较少的表开始查询）
3.order by limit 形式的sql语句让排序的表优先查
4.了解业务方使用场景
5.加索引时参照建索引的几大原则
6.观察结果，不符合预期继续从0分析

几个慢查询案例

下面几个例子详细解释了如何分析和优化慢查询

复杂语句写法

很多情况下，我们写SQL只是为了实现功能，这只是第一步，不同的语句书写方式对于效率往往有本质的差别，这要求我们对mysql的执行计划和索引原则有非常清楚的认识，请看下面的语句







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28




select

   distinctcert.emp_id

from

   cm_log
 cl 

innerjoin

   (

      select

         emp.idasemp_id,

         emp_cert.idascert_id

      from

         employee
 emp 

      leftjoin

         emp_certificate
 emp_cert 

            onemp.id
 = emp_cert.emp_id 

      where

         emp.is_deleted=0

   )
 cert 

      on(

         cl.ref_table='Employee'

         andcl.ref_oid=
 cert.emp_id

      )

      or(

         cl.ref_table='EmpCertificate'

         andcl.ref_oid=
 cert.cert_id

      )

where

   cl.last_upd_date
 &gt;='2013-11-07
 15:03:00'

   andcl.last_upd_date&lt;='2013-11-08
 16:00:00';








0.先运行一下，53条记录 1.87秒，又没有用聚合语句，比较慢







1




53rowsinset(1.87sec)








1.explain







1

2

3

4

5

6

7

8




+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+

|
 id | select_type | table      | type  | possible_keys                   | key                   | key_len | ref               | rows  | Extra                          |

+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+

| 1|
 PRIMARY     | cl         | range | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date     |8      |
 NULL              |   379|
 Using where; Using temporary   |

| 1|
 PRIMARY     | &lt;derived2&gt; | ALL   | NULL                            | NULL                  | NULL    | NULL              |63727|
 Using where; Using join buffer |

| 2|
 DERIVED     | emp        | ALL   | NULL                            | NULL                  | NULL    | NULL              |13317|
 Using where                    |

| 2|
 DERIVED     | emp_cert   | ref   | emp_certificate_empid           | emp_certificate_empid |4      |
 meituanorg.emp.id |     1|
 Using index                    |

+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+








简述一下执行计划，首先mysql根据idx_last_upd_date索引扫描cm_log表获得379条记录；然后查表扫描了63727条记录，分为两部分，derived表示构造表，也就是不存在的表，可以简单理解成是一个语句形成的结果集，后面的数字表示语句的ID。derived2表示的是ID = 2的查询构造了虚拟表，并且返回了63727条记录。我们再来看看ID = 2的语句究竟做了写什么返回了这么大量的数据，首先全表扫描employee表13317条记录，然后根据索引emp_certificate_empid关联emp_certificate表，rows
 = 1表示，每个关联都只锁定了一条记录，效率比较高。获得后，再和cm_log的379条记录根据规则关联。从执行过程上可以看出返回了太多的数据，返回的数据绝大部分cm_log都用不到，因为cm_log只锁定了379条记录。

如何优化呢？可以看到我们在运行完后还是要和cm_log做join,那么我们能不能之前和cm_log做join呢？仔细分析语句不难发现，其基本思想是如果cm_log的ref_table是EmpCertificate就关联emp_certificate表，如果ref_table是Employee就关联employee表，我们完全可以拆成两部分，并用union连接起来，注意这里用union，而不用union all是因为原语句有“distinct”来得到唯一的记录，而union恰好具备了这种功能。如果原语句中没有distinct不需要去重，我们就可以直接使用union
 all了，因为使用union需要去重的动作，会影响SQL性能。
优化过的语句如下







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28




select

   emp.id

from

   cm_log
 cl 

innerjoin

   employee
 emp 

      oncl.ref_table
 = 'Employee'

      andcl.ref_oid
 = emp.id  

where

   cl.last_upd_date
 &gt;='2013-11-07
 15:03:00'

   andcl.last_upd_date&lt;='2013-11-08
 16:00:00'

   andemp.is_deleted
 = 0  

union

select

   emp.id

from

   cm_log
 cl 

innerjoin

   emp_certificate
 ec 

      oncl.ref_table
 = 'EmpCertificate'

      andcl.ref_oid
 = ec.id  

innerjoin

   employee
 emp 

      onemp.id
 = ec.emp_id  

where

   cl.last_upd_date
 &gt;='2013-11-07
 15:03:00'

   andcl.last_upd_date&lt;='2013-11-08
 16:00:00'

   andemp.is_deleted
 = 0








4.不需要了解业务场景，只需要改造的语句和改造之前的语句保持结果一致

5.现有索引可以满足，不需要建索引

6.用改造后的语句实验一下，只需要10ms 降低了近200倍！







1

2

3

4

5

6

7

8

9

10




+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+

|
 id | select_type  | table      | type   | possible_keys                   | key               | key_len | ref                   | rows | Extra       |

+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+

| 1|
 PRIMARY      | cl         | range  | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date |8      |
 NULL                  |  379|
 Using where |

| 1|
 PRIMARY      | emp        | eq_ref | PRIMARY                         | PRIMARY           |4      |
 meituanorg.cl.ref_oid |    1|
 Using where |

| 2|
 UNION        | cl         | range  | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date |8      |
 NULL                  |  379|
 Using where |

| 2|
 UNION        | ec         | eq_ref | PRIMARY,emp_certificate_empid   | PRIMARY           |4      |
 meituanorg.cl.ref_oid |    1|            
 |

| 2|
 UNION        | emp        | eq_ref | PRIMARY                         | PRIMARY           |4      |
 meituanorg.ec.emp_id  |    1|
 Using where |

|
 NULL | UNION RESULT | &lt;union1,2&gt;
 | ALL    | NULL                            | NULL              | NULL    | NULL                  | NULL |             |

+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+








明确应用场景

举这个例子的目的在于颠覆我们对列的区分度的认知，一般上我们认为区分度越高的列，越容易锁定更少的记录，但在一些特殊的情况下，这种理论是有局限性的







1

2

3

4

5

6

7

8

9

10

11




select

   *

from

   stage_poi
 sp 

where

   sp.accurate_result=1

   and(

      sp.sync_status=0

      orsp.sync_status=2

      orsp.sync_status=4

   );








0.先看看运行多长时间,951条数据6.22秒，真的很慢







1




951rowsinset(6.22sec)








1.先explain，rows达到了361万，type = ALL表明是全表扫描







1

2

3

4

5




+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+

|
 id | select_type | table | type | possible_keys | key  | key_len | ref  | rows    | Extra       |

+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+

| 1|
 SIMPLE      | sp    | ALL  | NULL          | NULL | NULL    | NULL | 3613155|
 Using where |

+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+








2.所有字段都应用查询返回记录数，因为是单表查询 0已经做过了951条

3.让explain的rows 尽量逼近951

看一下accurate_result = 1的记录数







1

2

3

4

5

6

7

8




select
 count(*),accurate_result from stage_poi  group by accurate_result;

+----------+-----------------+

|
 count(*) | accurate_result |

+----------+-----------------+

|    1023|             
 -1|

| 2114655|              0|

|  972815|              1|

+----------+-----------------+








我们看到accurate_result这个字段的区分度非常低，整个表只有-1,0,1三个值，加上索引也无法锁定特别少量的数据

再看一下sync_status字段的情况







1

2

3

4

5

6

7




select
 count(*),sync_status from stage_poi  group by sync_status;

+----------+-------------+

|
 count(*) | sync_status |

+----------+-------------+

|    3080|          0|

| 3085413|          3|

+----------+-------------+








同样的区分度也很低，根据理论，也不适合建立索引

问题分析到这，好像得出了这个表无法优化的结论，两个列的区分度都很低，即便加上索引也只能适应这种情况，很难做普遍性的优化，比如当sync_status 0、3分布的很平均，那么锁定记录也是百万级别的

4.找业务方去沟通，看看使用场景。业务方是这么来使用这个SQL语句的，每隔五分钟会扫描符合条件的数据，处理完成后把sync_status这个字段变成1,五分钟符合条件的记录数并不会太多，1000个左右。了解了业务方的使用场景后，优化这个SQL就变得简单了，因为业务方保证了数据的不平衡，如果加上索引可以过滤掉绝大部分不需要的数据

5.根据建立索引规则，使用如下语句建立索引







1




alter
 table stage_poi add index idx_acc_status(accurate_result,sync_status);








6.观察预期结果,发现只需要200ms，快了30多倍。







1




952rowsinset(0.20sec)








我们再来回顾一下分析问题的过程，单表查询相对来说比较好优化，大部分时候只需要把where条件里面的字段依照规则加上索引就好，如果只是这种“无脑”优化的话，显然一些区分度非常低的列，不应该加索引的列也会被加上索引，这样会对插入、更新性能造成严重的影响，同时也有可能影响其它的查询语句。所以我们第4步调差SQL的使用场景非常关键，我们只有知道这个业务场景，才能更好地辅助我们更好的分析和优化查询语句。

无法优化的语句







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37




select

   c.id,

   c.name,

   c.position,

   c.sex,

   c.phone,

   c.office_phone,

   c.feature_info,

   c.birthday,

   c.creator_id,

   c.is_keyperson,

   c.giveup_reason,

   c.status,

   c.data_source,

   from_unixtime(c.created_time)ascreated_time,

   from_unixtime(c.last_modified)aslast_modified,

   c.last_modified_user_id 

from

   contact
 c  

innerjoin

   contact_branch
 cb 

      on c.id
 = cb.contact_id  

innerjoin

   branch_user
 bu 

      on cb.branch_id
 = bu.branch_id 

      andbu.statusin(

         1,

      2) 

   innerjoin

      org_emp_info
 oei 

         on oei.data_id
 = bu.user_id 

         andoei.node_left
 &gt;= 2875 

         andoei.node_right
 &lt;= 10802 

         andoei.org_category
 = - 1  

   orderby

      c.created_timedesc limit
 0 ,

      10;








还是几个步骤

0.先看语句运行多长时间，10条记录用了13秒，已经不可忍受







1




10rowsinset(13.06sec)








1.explain







1

2

3

4

5

6

7

8




+----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+

|
 id | select_type | table | type   | possible_keys                       | key                     | key_len | ref                      | rows | Extra                                        |

+----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+

| 1|
 SIMPLE      | oei   | ref    | idx_category_left_right,idx_data_id | idx_category_left_right |5      |const                   |8849|
 Using where; Using temporary; Using filesort |

| 1|
 SIMPLE      | bu    | ref    | PRIMARY,idx_userid_status           | idx_userid_status       |4      |
 meituancrm.oei.data_id   |   76|
 Using where; Using index                     |

| 1|
 SIMPLE      | cb    | ref    | idx_branch_id,idx_contact_branch_id | idx_branch_id           |4      |
 meituancrm.bu.branch_id  |    1|                                             
 |

| 1|
 SIMPLE      | c     | eq_ref | PRIMARY                             | PRIMARY                 |108    |
 meituancrm.cb.contact_id |    1|                                             
 |

+----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+








从执行计划上看，mysql先查org_emp_info表扫描8849记录，再用索引idx_userid_status关联branch_user表，再用索引idx_branch_id关联contact_branch表，最后主键关联contact表。

rows返回的都非常少，看不到有什么异常情况。我们在看一下语句，发现后面有order by + limit组合，会不会是排序量太大搞的？于是我们简化SQL，去掉后面的order by 和 limit，看看到底用了多少记录来排序







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25




select

  count(*)

from

   contact
 c  

innerjoin

   contact_branch
 cb 

      on c.id
 = cb.contact_id  

innerjoin

   branch_user
 bu 

      on cb.branch_id
 = bu.branch_id 

      andbu.statusin(

         1,

      2) 

   innerjoin

      org_emp_info
 oei 

         on oei.data_id
 = bu.user_id 

         andoei.node_left
 &gt;= 2875 

         andoei.node_right
 &lt;= 10802 

         andoei.org_category
 = - 1  

+----------+

|count(*)
 |

+----------+

|  
 778878 |

+----------+

1
 row inset(5.19
 sec)








发现排序之前居然锁定了778878条记录，如果针对70万的结果集排序，将是灾难性的，怪不得这么慢，那我们能不能换个思路，先根据contact的created_time排序，再来join会不会比较快呢？

于是改造成下面的语句，也可以用straight_join来优化







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43




select

c.id,

c.name,

c.position,

c.sex,

c.phone,

c.office_phone,

c.feature_info,

c.birthday,

c.creator_id,

c.is_keyperson,

c.giveup_reason,

c.status,

c.data_source,

from_unixtime(c.created_time)ascreated_time,

from_unixtime(c.last_modified)aslast_modified,

c.last_modified_user_id

from

contact
 c

where

exists
 (

select

1

from

contact_branch
 cb

innerjoin

branch_user
 bu

oncb.branch_id
 = bu.branch_id

andbu.statusin(

1,

2)

innerjoin

org_emp_info
 oei

onoei.data_id
 = bu.user_id

andoei.node_left
 &gt;= 2875

andoei.node_right
 &lt;= 10802

andoei.org_category
 = - 1

where

c.id
 = cb.contact_id

)

orderby

c.created_timedesclimit
 0 ,

10;








验证一下效果 预计在1ms内，提升了13000多倍！







1




10rowsinset(0.00sec)








本以为至此大工告成，但我们在前面的分析中漏了一个细节，先排序再join和先join再排序理论上开销是一样的，为何提升这么多是因为有一个limit！大致执行过程是：mysql先按索引排序得到前10条记录，然后再去join过滤，当发现不够10条的时候，再次去10条，再次join，这显然在内层join过滤的数据非常多的时候，将是灾难的，极端情况，内层一条数据都找不到，mysql还傻乎乎的每次取10条，几乎遍历了这个数据表！

用不同参数的SQL试验下







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44




select

   sql_no_cache  
 c.id,

   c.name,

   c.position,

   c.sex,

   c.phone,

   c.office_phone,

   c.feature_info,

   c.birthday,

   c.creator_id,

   c.is_keyperson,

   c.giveup_reason,

   c.status,

   c.data_source,

   from_unixtime(c.created_time)ascreated_time,

   from_unixtime(c.last_modified)aslast_modified,

   c.last_modified_user_id   

from

   contact
 c   

where

   exists
 (

      select

         1       

      from

         contact_branch
 cb         

      innerjoin

         branch_user
 bu                     

            on cb.branch_id
 = bu.branch_id                     

            andbu.statusin(

               1,

            2)               

         innerjoin

            org_emp_info
 oei                           

               on oei.data_id
 = bu.user_id                           

               andoei.node_left
 &gt;= 2875                           

               andoei.node_right
 &lt;= 2875                           

               andoei.org_category
 = - 1                

         where

            c.id
 = cb.contact_id           

      )       

   orderby

      c.created_timedesc limit
 0 ,

      10;

Emptyset(2min18.99
 sec)








2 min 18.99 sec！比之前的情况还糟糕很多。由于mysql的nested loop机制，遇到这种情况，基本是无法优化的。这条语句最终也只能交给应用系统去优化自己的逻辑了。

通过这个例子我们可以看到，并不是所有语句都能优化，而往往我们优化时，由于SQL用例回归时落掉一些极端情况，会造成比原来还严重的后果。所以，第一：不要指望所有语句都能通过SQL优化，第二：不要过于自信，只针对具体case来优化，而忽略了更复杂的情况。

慢查询的案例就分析到这儿，以上只是一些比较典型的案例。我们在优化过程中遇到过超过1000行，涉及到16个表join的“垃圾SQL”，也遇到过线上线下数据库差异导致应用直接被慢查询拖死，也遇到过varchar等值比较没有写单引号，还遇到过笛卡尔积查询直接把从库搞死。再多的案例其实也只是一些经验的积累，如果我们熟悉查询优化器、索引的内部原理，那么分析这些案例就变得特别简单了。

            说明： 本文出自美团技术博客。



参考文献如下：

1.《高性能MySQL》
2.《数据结构与算法分析》


         在应用系统开发初期，由于开发数据库数据比较少，对于查询SQL语句，复杂视图的的编写等体会不出SQL语句各种写法的性能优劣，但是如果将应用系统提交实际应用后，随着数据库中数据的增加，系统的响应速度就成为目前系统需要解决的最主要的问题之一。系统优化中一个很重要的方面就是SQL语句的优化。对于海量数据，劣质SQL语句和优质SQL语句之间的速度差别可以达到上百倍，可见对于一个系统不是简单地能实现其功能就可，而是要写出高质量的SQL语句，提高系统的可用性。

        在多数情况下，Oracle使用索引来更快地遍历表，优化器主要根据定义的索引来提高性能。但是，如果在SQL语句的where子句中写的SQL代码不合理，就会造成优化器删去索引而使用全表扫描，一般就这种SQL语句就是所谓的劣质SQL语句。在编写SQL语句时我们应清楚优化器根据何种原则来删除索引，这有助于写出高性能的SQL语句。

       下面就某些SQL语句的where子句编写中需要注意的问题作详细介绍。在这些where子句中，即使某些列存在索引，但是由于编写了劣质的SQL，系统在运行该SQL语句时也不能使用该索引，而同样使用全表扫描，这就造成了响应速度的极大降低。

1. 操作符优化

(a) IN 操作符

用IN写出来的SQL的优点是比较容易写及清晰易懂，这比较适合现代软件开发的风格。但是用IN的SQL性能总是比较低的，从Oracle执行的步骤来分析用IN的SQL与不用IN的SQL有以下区别：

ORACLE试图将其转换成多个表的连接，如果转换不成功则先执行IN里面的子查询，再查询外层的表记录，如果转换成功则直接采用多个表的连接方式查询。由此可见用IN的SQL至少多了一个转换的过程。一般的SQL都可以转换成功，但对于含有分组统计等方面的SQL就不能转换了。

推荐方案：在业务密集的SQL当中尽量不采用IN操作符，用EXISTS 方案代替。

(b) NOT IN操作符

此操作是强列不推荐使用的，因为它不能应用表的索引。

推荐方案：用NOT EXISTS 方案代替

(c) IS NULL 或IS NOT NULL操作（判断字段是否为空）

判断字段是否为空一般是不会应用索引的，因为索引是不索引空值的。不能用null作索引，任何包含null值的列都将不会被包含在索引中。即使索引有多列这样的情况下，只要这些列中有一列含有null，该列就会从索引中排除。也就是说如果某列存在空值，即使对该列建索引也不会提高性能。任何在where子句中使用is null或is not null的语句优化器是不允许使用索引的。

推荐方案：用其它相同功能的操作运算代替，如：a is not null 改为 a&gt;0 或a&gt;’’等。不允许字段为空，而用一个缺省值代替空值，如申请中状态字段不允许为空，缺省为申请。

(d) &gt; 及 &lt; 操作符（大于或小于操作符）

大于或小于操作符一般情况下是不用调整的，因为它有索引就会采用索引查找，但有的情况下可以对它进行优化，如一个表有100万记录，一个数值型字段A，30万记录的A=0，30万记录的A=1，39万记录的A=2，1万记录的A=3。那么执行A&gt;2与A&gt;=3的效果就有很大的区别了，因为A&gt;2时ORACLE会先找出为2的记录索引再进行比较，而A&gt;=3时ORACLE则直接找到=3的记录索引。

(e) LIKE操作符

LIKE操作符可以应用通配符查询，里面的通配符组合可能达到几乎是任意的查询，但是如果用得不好则会产生性能上的问题，如LIKE ‘%5400%’ 这种查询不会引用索引，而LIKE ‘X5400%’则会引用范围索引。

一个实际例子：用YW_YHJBQK表中营业编号后面的户标识号可来查询营业编号 YY_BH LIKE ‘%5400%’ 这个条件会产生全表扫描，如果改成YY_BH LIKE ’X5400%’ OR YY_BH LIKE ’B5400%’ 则会利用YY_BH的索引进行两个范围的查询，性能肯定大大提高。

带通配符(%)的like语句：

同样以上面的例子来看这种情况。目前的需求是这样的，要求在职工表中查询名字中包含cliton的人。可以采用如下的查询SQL语句:







1




select

* from

employee where

last_name like

'%cliton%';








这里由于通配符(%)在搜寻词首出现，所以Oracle系统不使用last_name的索引。在很多情况下可能无法避免这种情况，但是一定要心中有底，通配符如此使用会降低查询速度。然而当通配符出现在字符串其他位置时，优化器就能利用索引。在下面的查询中索引得到了使用:







1




select

* from

employee where

last_name like

'c%';








(f) UNION操作符

UNION在进行表链接后会筛选掉重复的记录，所以在表链接后会对所产生的结果集进行排序运算，删除重复的记录再返回结果。实际大部分应用中是不会产生重复的记录，最常见的是过程表与历史表UNION。如：







1

2

3




select

* from

gc_dfys 

union 

select

* from

ls_jg_dfys








这个SQL在运行时先取出两个表的结果，再用排序空间进行排序删除重复的记录，最后返回结果集，如果表数据量大的话可能会导致用磁盘进行排序。

推荐方案：采用UNION ALL操作符替代UNION，因为UNION ALL操作只是简单的将两个结果合并后就返回。







1

2

3




select

* from

gc_dfys 

union

all 

select

* from

ls_jg_dfys








(g) 联接列

对于有联接的列，即使最后的联接值为一个静态值，优化器是不会使用索引的。我们一起来看一个例子，假定有一个职工表(employee)，对于一个职工的姓和名分成两列存放(FIRST_NAME和LAST_NAME)，现在要查询一个叫比尔.克林顿(Bill Cliton)的职工。

下面是一个采用联接查询的SQL语句：







1




select

* from

employss where

first_name||''||last_name
 ='Beill
 Cliton';








上面这条语句完全可以查询出是否有Bill Cliton这个员工，但是这里需要注意，系统优化器对基于last_name创建的索引没有使用。当采用下面这种SQL语句的编写，Oracle系统就可以采用基于last_name创建的索引。







1




where

first_name ='Beill'

and 
last_name ='Cliton';








(h) Order by语句

ORDER BY语句决定了Oracle如何将返回的查询结果排序。Order by语句对要排序的列没有什么特别的限制，也可以将函数加入列中(象联接或者附加等)。任何在Order by语句的非索引项或者有计算表达式都将降低查询速度。

仔细检查order by语句以找出非索引项或者表达式，它们会降低性能。解决这个问题的办法就是重写order by语句以使用索引，也可以为所使用的列建立另外一个索引，同时应绝对避免在order by子句中使用表达式。

(i) NOT

我们在查询时经常在where子句使用一些逻辑表达式，如大于、小于、等于以及不等于等等，也可以使用and(与)、or(或)以及not(非)。NOT可用来对任何逻辑运算符号取反。下面是一个NOT子句的例子:







1




where

not 
(status ='VALID')








如果要使用NOT，则应在取反的短语前面加上括号，并在短语前面加上NOT运算符。NOT运算符包含在另外一个逻辑运算符中，这就是不等于(&lt;&gt;)运算符。换句话说，即使不在查询where子句中显式地加入NOT词，NOT仍在运算符中，见下例:







1




where

status &lt;&gt;'INVALID';








对这个查询，可以改写为不使用NOT：







1




select

* from

employee where

salary&lt;3000 or

salary&gt;3000;








虽然这两种查询的结果一样，但是第二种查询方案会比第一种查询方案更快些。第二种查询允许Oracle对salary列使用索引，而第一种查询则不能使用索引。

2. SQL书写的影响

(a) 同一功能同一性能不同写法SQL的影响。

如一个SQL在A程序员写的为  Select * from zl_yhjbqk

B程序员写的为 Select * from dlyx.zl_yhjbqk（带表所有者的前缀）

C程序员写的为 Select * from DLYX.ZLYHJBQK（大写表名）

D程序员写的为 Select *  from DLYX.ZLYHJBQK（中间多了空格）

以上四个SQL在ORACLE分析整理之后产生的结果及执行的时间是一样的，但是从ORACLE共享内存SGA的原理，可以得出ORACLE对每个SQL 都会对其进行一次分析，并且占用共享内存，如果将SQL的字符串及格式写得完全相同，则ORACLE只会分析一次，共享内存也只会留下一次的分析结果，这不仅可以减少分析SQL的时间，而且可以减少共享内存重复的信息，ORACLE也可以准确统计SQL的执行频率。

(b) WHERE后面的条件顺序影响

WHERE子句后面的条件顺序对大数据量表的查询会产生直接的影响。如：







1

2




Select

* from

zl_yhjbqk where

dy_dj = '1KV以下'

and 
xh_bz=1 

Select

* from

zl_yhjbqk where

xh_bz=1 and

dy_dj = '1KV以下'








以上两个SQL中dy_dj（电压等级）及xh_bz（销户标志）两个字段都没进行索引，所以执行的时候都是全表扫描，第一条SQL的dy_dj = ’1KV以下’条件在记录集内比率为99%，而xh_bz=1的比率只为0.5%，在进行第一条SQL的时候99%条记录都进行dy_dj及xh_bz的比较，而在进行第二条SQL的时候0.5%条记录都进行dy_dj及xh_bz的比较，以此可以得出第二条SQL的CPU占用率明显比第一条低。

(c) 查询表顺序的影响

在FROM后面的表中的列表顺序会对SQL执行性能影响，在没有索引及ORACLE没有对表进行统计分析的情况下，ORACLE会按表出现的顺序进行链接，由此可见表的顺序不对时会产生十分耗服物器资源的数据交叉。（注：如果对表进行了统计分析，ORACLE会自动先进小表的链接，再进行大表的链接）

3. SQL语句索引的利用

(a) 对条件字段的一些优化

采用函数处理的字段不能利用索引，如：







1

2




substr(hbs_bh,1,4)=’5400’，优化处理：hbs_bh
like

‘5400%’

trunc(sk_rq)=trunc(sysdate)，
 优化处理：sk_rq&gt;=trunc(sysdate) and

sk_rq&lt;trunc(sysdate+1)








进行了显式或隐式的运算的字段不能进行索引，如：ss_df+20&gt;50，优化处理：ss_df&gt;30







1

2




‘X’
 || hbs_bh&gt;’X5400021452’，优化处理：hbs_bh&gt;’5400021542’

sk_rq+5=sysdate，优化处理：sk_rq=sysdate-5








hbs_bh=5401002554，优化处理：hbs_bh=’ 5401002554’，注：此条件对hbs_bh 进行隐式的to_number转换，因为hbs_bh字段是字符型。

条件内包括了多个本表的字段运算时不能进行索引，如：







1

2




ys_df&gt;cx_df，无法进行优化 

qc_bh
 || kh_bh=’5400250000’，优化处理：qc_bh=’5400’ and

kh_bh=’250000’








4. 更多方面SQL优化资料分享

（1） 选择最有效率的表名顺序(只在基于规则的优化器中有效)：

ORACLE 的解析器按照从右到左的顺序处理FROM子句中的表名，FROM子句中写在最后的表(基础表 driving table)将被最先处理，在FROM子句中包含多个表的情况下,你必须选择记录条数最少的表作为基础表。如果有3个以上的表连接查询, 那就需要选择交叉表(intersection table)作为基础表, 交叉表是指那个被其他表所引用的表.

（2） WHERE子句中的连接顺序：

ORACLE采用自下而上的顺序解析WHERE子句,根据这个原理,表之间的连接必须写在其他WHERE条件之前, 那些可以过滤掉最大数量记录的条件必须写在WHERE子句的末尾.

（3） SELECT子句中避免使用 ‘ * ‘：

ORACLE在解析的过程中, 会将’*’ 依次转换成所有的列名, 这个工作是通过查询数据字典完成的, 这意味着将耗费更多的时间。

（4） 减少访问数据库的次数：

ORACLE在内部执行了许多工作: 解析SQL语句, 估算索引的利用率, 绑定变量 , 读数据块等。

（5） 在SQL*Plus , SQL*Forms和Pro*C中重新设置ARRAYSIZE参数, 可以增加每次数据库访问的检索数据量 ,建议值为200。

（6） 使用DECODE函数来减少处理时间：

使用DECODE函数可以避免重复扫描相同记录或重复连接相同的表.

（7） 整合简单,无关联的数据库访问：

如果你有几个简单的数据库查询语句,你可以把它们整合到一个查询中(即使它们之间没有关系) 。

（8） 删除重复记录：

最高效的删除重复记录方法 ( 因为使用了ROWID)例子：







1




DELETE 

FROM  
EMP E  WHERE 

E.ROWID &gt; (SELECT

MIN(X.ROWID)
FROM 

EMP X  WHERE 

X.EMP_NO = E.EMP_NO)。








（9） 用TRUNCATE替代DELETE：

当删除表中的记录时,在通常情况下, 回滚段(rollback segments ) 用来存放可以被恢复的信息. 如果你没有COMMIT事务,ORACLE会将数据恢复到删除之前的状态(准确地说是恢复到执行删除命令之前的状况) 而当运用TRUNCATE时, 回滚段不再存放任何可被恢复的信息.当命令运行后,数据不能被恢复.因此很少的资源被调用,执行时间也会很短. (译者按: TRUNCATE只在删除全表适用,TRUNCATE是DDL不是DML) 。

（10） 尽量多使用COMMIT：

只要有可能,在程序中尽量多使用COMMIT, 这样程序的性能得到提高,需求也会因为COMMIT所释放的资源而减少，COMMIT所释放的资源:
a. 回滚段上用于恢复数据的信息.
b. 被程序语句获得的锁
c. redo log buffer 中的空间
d. ORACLE为管理上述3种资源中的内部花费

（11） 用Where子句替换HAVING子句：

避免使用HAVING子句, HAVING 只会在检索出所有记录之后才对结果集进行过滤. 这个处理需要排序,总计等操作. 如果能通过WHERE子句限制记录的数目,那就能减少这方面的开销. (非oracle中)on、where、having这三个都可以加条件的子句中，on是最先执行，where次之，having最后，因为on是先把不符合条件的记录过滤后才进行统计，它就可以减少中间运算要处理的数据，按理说应该速度是最快的，where也应该比having快点的，因为它过滤数据后才进行sum，在两个表联接时才用on的，所以在一个表的时候，就剩下where跟having比较了。在这单表查询统计的情况下，如果要过滤的条件没有涉及到要计算字段，那它们的结果是一样的，只是where可以使用rushmore技术，而having就不能，在速度上后者要慢如果要涉及到计算的字
 段，就表示在没计算之前，这个字段的值是不确定的，根据上篇写的工作流程，where的作用时间是在计算之前就完成的，而having就是在计算后才起作 用的，所以在这种情况下，两者的结果会不同。在多表联接查询时，on比where更早起作用。系统首先根据各个表之间的联接条件，把多个表合成一个临时表 后，再由where进行过滤，然后再计算，计算完后再由having进行过滤。由此可见，要想过滤条件起到正确的作用，首先要明白这个条件应该在什么时候起作用，然后再决定放在那里。

（12） 减少对表的查询：

在含有子查询的SQL语句中,要特别注意减少对表的查询.例子：







1




SELECT 

TAB_NAME FROM

TABLES WHERE

(TAB_NAME,DB_VER) = ( SELECT

TAB_NAME,DB_VER FROM 

TAB_COLUMNS  WHERE 

VERSION = 604)








（13） 通过内部函数提高SQL效率：

复杂的SQL往往牺牲了执行效率. 能够掌握上面的运用函数解决问题的方法在实际工作中是非常有意义的。

（14） 使用表的别名(Alias)：

当在SQL语句中连接多个表时, 请使用表的别名并把别名前缀于每个Column上.这样一来,就可以减少解析的时间并减少那些由Column歧义引起的语法错误。

（15） 用EXISTS替代IN、用NOT EXISTS替代NOT IN：

在许多基于基础表的查询中,为了满足一个条件,往往需要对另一个表进行联接.在这种情况下, 使用EXISTS(或NOT EXISTS)通常将提高查询的效率. 在子查询中,NOT IN子句将执行一个内部的排序和合并. 无论在哪种情况下,NOT IN都是最低效的 (因为它对子查询中的表执行了一个全表遍历). 为了避免使用NOT IN ,我们可以把它改写成外连接(Outer Joins)或NOT EXISTS。
例子：







1

2




（高效）SELECT

* FROM 

EMP (基础表)  WHERE 

EMPNO &gt; 0  AND 

EXISTS (SELECT

‘X' 
 FROM DEPT  WHERE  DEPT.DEPTNO = EMP.DEPTNO  AND  LOC = ‘MELB') 

(低效)SELECT 

* FROM 

EMP (基础表)  WHERE 

EMPNO &gt; 0  AND 

DEPTNO IN(SELECT

DEPTNO  FROM 

DEPT  WHERE 

LOC = ‘MELB')








（16） 识别’低效执行’的SQL语句：

虽然目前各种关于SQL优化的图形化工具层出不穷,但是写出自己的SQL工具来解决问题始终是一个最好的方法：







1

2

3

4

5

6

7

8

9




SELECT 

EXECUTIONS , DISK_READS, BUFFER_GETS, 

ROUND((BUFFER_GETS-DISK_READS)/BUFFER_GETS,2)
 Hit_radio, 

ROUND(DISK_READS/EXECUTIONS,2)
 Reads_per_run, 

SQL_TEXT 

FROM 

V$SQLAREA 

WHERE 

EXECUTIONS&gt;0 

AND 

BUFFER_GETS &gt; 0 

AND 

(BUFFER_GETS-DISK_READS)/BUFFER_GETS &lt; 0.8 

ORDER

BY  
4 DESC;








（17） 用索引提高效率：

索引是表的一个概念部分,用来提高检索数据的效率，ORACLE使用了一个复杂的自平衡B-tree结构. 通常,通过索引查询数据比全表扫描要快. 当ORACLE找出执行查询和Update语句的最佳路径时, ORACLE优化器将使用索引. 同样在联结多个表时使用索引也可以提高效率. 另一个使用索引的好处是,它提供了主键(primary key)的唯一性验证.。那些LONG或LONG RAW数据类型, 你可以索引几乎所有的列. 通常, 在大型表中使用索引特别有效. 当然,你也会发现, 在扫描小表时,使用索引同样能提高效率.
 虽然使用索引能得到查询效率的提高,但是我们也必须注意到它的代价. 索引需要空间来存储,也需要定期维护, 每当有记录在表中增减或索引列被修改时, 索引本身也会被修改. 这意味着每条记录的INSERT , DELETE , UPDATE将为此多付出4 , 5 次的磁盘I/O . 因为索引需要额外的存储空间和处理,那些不必要的索引反而会使查询反应时间变慢.。定期的重构索引是有必要的：







1




ALTER 

INDEX 
&lt;INDEXNAME&gt; REBUILD &lt;TABLESPACENAME&gt;








（18） 用EXISTS替换DISTINCT：

当提交一个包含一对多表信息(比如部门表和雇员表)的查询时,避免在SELECT子句中使用DISTINCT. 一般可以考虑用EXIST替换, EXISTS 使查询更为迅速,因为RDBMS核心模块将在子查询的条件一旦满足后,立刻返回结果. 例子：







1

2

3

4




(低效): 

SELECT 

DISTINCT  
DEPT_NO,DEPT_NAME  FROM 

DEPT D , EMP E WHERE 

D.DEPT_NO = E.DEPT_NO 

(高效): 

SELECT 

DEPT_NO,DEPT_NAME  FROM 

DEPT D  WHERE 

EXISTS ( SELECT

‘X'  FROM 

EMP E  WHERE

E.DEPT_NO = D.DEPT_NO);








（19） sql语句用大写的；因为oracle总是先解析sql语句，把小写的字母转换成大写的再执行。

（20） 在java代码中尽量少用连接符“＋”连接字符串！

（21） 避免在索引列上使用NOT，通常我们要避免在索引列上使用NOT, NOT会产生在和在索引列上使用函数相同的影响. 当ORACLE”遇到”NOT,他就会停止使用索引转而执行全表扫描。

（22） 避免在索引列上使用计算
WHERE子句中，如果索引列是函数的一部分．优化器将不使用索引而使用全表扫描．举例:







1

2

3

4




低效： 

SELECT

… FROM 

DEPT  WHERE

SAL * 12 &gt; 25000; 

高效: 

SELECT

… FROM

DEPT WHERE

SAL &gt; 25000/12;








（23） 用&gt;=替代&gt;







1

2

3

4




高效: 

SELECT

* FROM 

EMP  WHERE 

DEPTNO &gt;=4 

低效: 

SELECT

* FROM

EMP WHERE

DEPTNO &gt;3








两者的区别在于, 前者DBMS将直接跳到第一个DEPT等于4的记录而后者将首先定位到DEPTNO=3的记录并且向前扫描到第一个DEPT大于3的记录。

（24） 用UNION替换OR (适用于索引列)

通常情况下, 用UNION替换WHERE子句中的OR将会起到较好的效果. 对索引列使用OR将造成全表扫描. 注意, 以上规则只针对多个索引列有效. 如果有column没有被索引, 查询效率可能会因为你没有选择OR而降低. 在下面的例子中, LOC_ID 和REGION上都建有索引.







1

2

3

4

5

6

7

8

9

10

11

12




高效: 

SELECT

LOC_ID , LOC_DESC , REGION 

FROM

LOCATION 

WHERE

LOC_ID = 10 

UNION 

SELECT

LOC_ID , LOC_DESC , REGION 

FROM

LOCATION 

WHERE

REGION = “MELBOURNE” 

低效: 

SELECT

LOC_ID , LOC_DESC , REGION 

FROM

LOCATION 

WHERE

LOC_ID = 10 OR

REGION = “MELBOURNE”








如果你坚持要用OR, 那就需要返回记录最少的索引列写在最前面.

（25） 用IN来替换OR

这是一条简单易记的规则，但是实际的执行效果还须检验，在ORACLE8i下，两者的执行路径似乎是相同的．







1

2

3

4




低效: 

SELECT….
FROM

LOCATION WHERE

LOC_ID = 10 OR

LOC_ID = 20 OR

LOC_ID = 30 

高效 

SELECT…
FROM

LOCATION WHERE

LOC_IN  IN

(10,20,30);








（26） 避免在索引列上使用IS NULL和IS NOT NULL

避免在索引中使用任何可以为空的列，ORACLE将无法使用该索引．对于单列索引，如果列包含空值，索引中将不存在此记录. 对于复合索引，如果每个列都为空，索引中同样不存在此记录. 如果至少有一个列不为空，则记录存在于索引中．举例: 如果唯一性索引建立在表的A列和B列上, 并且表中存在一条记录的A,B值为(123,null) , ORACLE将不接受下一条具有相同A,B值（123,null）的记录(插入). 然而如果所有的索引列都为空，ORACLE将认为整个键值为空而空不等于空. 因此你可以插入1000 条具有相同键值的记录,当然它们都是空!
 因为空值不存在于索引列中,所以WHERE子句中对索引列进行空值比较将使ORACLE停用该索引.







1

2

3

4




低效:
 (索引失效) 

SELECT

… FROM 

DEPARTMENT  WHERE 

DEPT_CODE IS

NOT 
NULL; 

高效:
 (索引有效) 

SELECT

… FROM 

DEPARTMENT  WHERE 

DEPT_CODE &gt;=0;








（27） 总是使用索引的第一个列：

如果索引是建立在多个列上, 只有在它的第一个列(leading column)被where子句引用时,优化器才会选择使用该索引. 这也是一条简单而重要的规则，当仅引用索引的第二个列时,优化器使用了全表扫描而忽略了索引。

（28） 用UNION-ALL 替换UNION ( 如果有可能的话)：

当SQL 语句需要UNION两个查询结果集合时,这两个结果集合会以UNION-ALL的方式被合并, 然后在输出最终结果前进行排序. 如果用UNION ALL替代UNION, 这样排序就不是必要了. 效率就会因此得到提高. 需要注意的是，UNION ALL 将重复输出两个结果集合中相同记录. 因此各位还是要从业务需求分析使用UNION ALL的可行性. UNION 将对结果集合排序,这个操作会使用到SORT_AREA_SIZE这块内存. 对于这块内存的优化也是相当重要的. 下面的SQL可以用来查询排序的消耗量







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16




低效： 

SELECT 

ACCT_NUM, BALANCE_AMT 

FROM 

DEBIT_TRANSACTIONS 

WHERE

TRAN_DATE = '31-DEC-95' 

UNION 

SELECT

ACCT_NUM, BALANCE_AMT 

FROM

DEBIT_TRANSACTIONS 

WHERE

TRAN_DATE = '31-DEC-95' 

高效: 

SELECT

ACCT_NUM, BALANCE_AMT 

FROM

DEBIT_TRANSACTIONS 

WHERE

TRAN_DATE = '31-DEC-95' 

UNION

ALL 

SELECT

ACCT_NUM, BALANCE_AMT 

FROM

DEBIT_TRANSACTIONS 

WHERE

TRAN_DATE = '31-DEC-95'








（29） 用WHERE替代ORDER BY：

ORDER BY 子句只在两种严格的条件下使用索引.
ORDER BY中所有的列必须包含在相同的索引中并保持在索引中的排列顺序.
ORDER BY中所有的列必须定义为非空.
WHERE子句使用的索引和ORDER BY子句中所使用的索引不能并列.
例如:
表DEPT包含以下列:







1

2

3




DEPT_CODE
 PK NOT

NULL 

DEPT_DESC
NOT

NULL 

DEPT_TYPE
NULL














1

2

3

4




低效:
 (索引不被使用) 

SELECT

DEPT_CODE FROM 

DEPT  ORDER

BY  
DEPT_TYPE 

高效:
 (使用索引) 

SELECT

DEPT_CODE  FROM 

DEPT  WHERE 

DEPT_TYPE &gt; 0








（30） 避免改变索引列的类型:

当比较不同数据类型的数据时, ORACLE自动对列进行简单的类型转换.
假设 EMPNO是一个数值类型的索引列.







1




SELECT

…  FROM

EMP  WHERE 

EMPNO = ‘123'








实际上,经过ORACLE类型转换, 语句转化为:







1




SELECT

…  FROM

EMP  WHERE 

EMPNO = TO_NUMBER(‘123')








幸运的是,类型转换没有发生在索引列上,索引的用途没有被改变.
现在,假设EMP_TYPE是一个字符类型的索引列.







1




SELECT

…  FROM

EMP  WHERE

EMP_TYPE = 123








这个语句被ORACLE转换为:







1




SELECT

…  FROM

EMP  WHERE

TO_NUMBER(EMP_TYPE)=123








因为内部发生的类型转换, 这个索引将不会被用到! 为了避免ORACLE对你的SQL进行隐式的类型转换, 最好把类型转换用显式表现出来. 注意当字符和数值比较时, ORACLE会优先转换数值类型到字符类型。

分析







1




select  

emp_name   form   employee   where  

salary   &gt;   3000








在此语句中若salary是Float类型的，则优化器对其进行优化为Convert(float,3000)，因为3000是个整数，我们应在编程时使用3000.0而不要等运行时让DBMS进行转化。同样字符和整型数据的转换。
（31） 需要当心的WHERE子句:

某些SELECT 语句中的WHERE子句不使用索引. 这里有一些例子.
在下面的例子里, (1)‘!=’ 将不使用索引. 记住, 索引只能告诉你什么存在于表中, 而不能告诉你什么不存在于表中. (2) ‘ | |’是字符连接函数. 就象其他函数那样, 停用了索引. (3) ‘+’是数学函数. 就象其他数学函数那样, 停用了索引. (4)相同的索引列不能互相比较,这将会启用全表扫描.

（32） a. 如果检索数据量超过30%的表中记录数.使用索引将没有显著的效率提高. b. 在特定情况下, 使用索引也许会比全表扫描慢, 但这是同一个数量级上的区别. 而通常情况下,使用索引比全表扫描要块几倍乃至几千倍!

（33） 避免使用耗费资源的操作：

带有DISTINCT,UNION,MINUS,INTERSECT,ORDER BY的SQL语句会启动SQL引擎执行耗费资源的排序(SORT)功能. DISTINCT需要一次排序操作, 而其他的至少需要执行两次排序. 通常, 带有UNION, MINUS , INTERSECT的SQL语句都可以用其他方式重写. 如果你的数据库的SORT_AREA_SIZE调配得好, 使用UNION , MINUS, INTERSECT也是可以考虑的, 毕竟它们的可读性很强。

（34） 优化GROUP BY：

提高GROUP BY 语句的效率, 可以通过将不需要的记录在GROUP BY 之前过滤掉.下面两个查询返回相同结果但第二个明显就快了许多.







1

2

3

4

5

6

7

8

9

10

11

12




低效: 

SELECT

JOB , AVG(SAL) 

FROM

EMP 

GROUP

by 
JOB 

HAVING

JOB = ‘PRESIDENT' 

OR
 JOB = ‘MANAGER' 

高效: 

SELECT

JOB , AVG(SAL) 

FROM

EMP 

WHERE

JOB = ‘PRESIDENT' 

OR
 JOB = ‘MANAGER' 

GROUP

by 
JOB









       随着Linux在企业中的不断飞速的应用，为了企业中更好的运维，熟悉日常运维的技巧能更好的满足企业的发展，同时让我们的运维更加轻松，不再觉得运维是苦逼的活，真正去锻炼去成长去磨练。
那我们需要注意什么呢：
1) Linux行业目标
      我们要明白学习Linux运维的目的，相信大家都是为了能找到一份非常好的工作，一个高薪的工作，不断的练习，不断的成长。
通过工作，让我们的生活更加的完整和充实。
2) Linux学习路线
      在明白自己的大的目标之后，我们需要分解大目标，接下来就是真正去行动，去朝着小目标努力，有哪些小目标呢？
计算机基础知识—&gt;硬件认识—&gt;windows系统日常操作—&gt;Linux系统入门—&gt;Linux目录及权限学习—&gt;linux必备20个命令(ls pwd cd cat useradd groupadd rm cp chown chmod vi find grep ps free top sed awk if for case wc yum rpm tar unzip more head tail等)
—&gt;Linux简单服务器搭建（掌握tar常见文件解压方式，掌握安装软件的方法：
yum install方式安装；源码编译安装三步，configure、make、make install）
—&gt;Apache服务构建—&gt;Mysql服务搭建—&gt;PHP服务器搭建—&gt;LAMP架构整合discuz论坛—&gt;Kickstart自动化系统安装—&gt;cacti监控部署—&gt;Shell脚本编程（包括各种语句的学习，if for awk for while sed等）—&gt;Linux高级服务器搭建—&gt;Nginx WEB服务器搭建—&gt;Tomcat服务器搭建—&gt;resin服务器搭建—&gt;Nginx均衡java服务器—&gt;LNMP架构搭建（yum/源码）—&gt;Nginx动静分离—&gt;LVS+Keepalived负载均衡部署—&gt;LVS+Keepalived+Nginx+Tomcat均衡架构部署—&gt;高级Shell编写—&gt;自动化运维学习（KVM、Puppet、ZABBIX、Ansible、Mysql+DRBD等）
3) 编辑器命令技巧
熟悉命令行及vi编辑器的查找，匹配删除、跳转等等，例如在shell命令行里ctrl +a跳转到最前，ctrl +e跳转到最末尾。

在vi编辑器里面：
Shift + ^跳转到开头，shift + $跳转到末尾。
匹配/word字符，删除光标所在字符按x即可，跳转到文本最末行按G，跳转到文本首行按gg。
同时删除光标行至文本最后一行：dG
删除光标行至文本第一行：dgg
4) 系统运行状态监测
可以使用free m查看内存剩余大小，通常看




1


-/+ buffers/cache:        881        112（该值大约为真实内存值）





3967为剩余的真实内存！ 
可以使用df h查看到tmpfs内存文件系统，加速静态文件及图片：


     tmfts为内存文件系统，通常用于加速静态资源文件，改容量为物理内存的1/2，可以扩容，但是重启后，内容丢失，所以在生产环境使用要注意！！！
查看本地网卡流量技巧：




1


yum install iftop -y




iftop  -i eth0查看结果如下图：




1
2
3
4
5
6
7


中间这两个左右箭头，表示的是流量的方向。
TX：发送流量。
RX：接收流量。
TOTAL：总流量。
Cumm：运行iftop到目前时间的总流量。
peak：流量峰值。
rates：分别表示过去 2s 10s 40s 的平均流量。




Linux企业运维高效技巧心得及分享
查看磁盘IO负载技巧：
vmstat 1 5 (每秒输出结果，总共输出5次)，如下图：



r: 运行队列中进程数量
b: 等待IO的进程数量
Memory（内存）：
bi: 每秒读取的块数
bo: 每秒写入的块数
wa: 等待IO时间
注意*一般判断系统负载是否过高，IO磁盘读写是否超高，我们可以查看r、b和wa的时间，当然是越小表是性能资源还有很多剩余，如果过大，我们就需要查看是由于什么操作导致的。
可以结合iostat查看更容易判断是不是磁盘读写导致IO很高。如下图：



一般判断%util的值，如果持续超过75%以上就需要注意了，检查相关服务的访问是否异常，然后去一一解决。
服务后台启动：
常见的程序放在后台运行方法主要有：
screen 后台运行：
在命令行执行screen 回车，进入一个随机的screen后台，可以输入命令，然后按ctrl +a+d保存退出即可，这时程序已经在后台运行。
Screen ls可以查看当前运行screen后台列表,执行screen r 加PID可以进入相应的后台，再次退出还需要按ctrl+a+d

如何想要删除screen，需要执行kill -9 3215 ，然后执行screen -wipe 即可删除。
 
除了screen之外，我们还可以使用nohup来后台运行程序：




1


nohup  sh  auto_nginx.sh




即程序已经在后台运行，可以在当前目录查看tail fn 10 nohup.out可以看到程序执行的相关信息，如果需要结束就直接kill 进程就OK。



         随着IT运维的不断发展，尤其的Linux的飞速发展，越来越多的企业开始使用Linux操作系统平台，例如CentOS、RedHat、Ubuntu、Fedora等等，成千上亿个网站涌现在当今互联网，互联网已经成为必不可少的工具。

         目前用的最多的Linux下主流网站架构：

LVS+KEEPALIVED（heartbeat）+Squid+Nginx/Apache+JAVA/PHP+MySQL/MariaDB等

         下面是一个简单的拓扑图，供大家实验参考：



       一般网站总体分为四层，依次为前端负载均衡、中间代理、后端服务、数据库层。

       当然除了这个整体的流程，不同的公司扩展的东西也非常多，各种系统不断的往这个架构里面添加，形成一个非常庞大、复杂的系统。那接下来我们对每个层级运维人员需要注意的细节：

1)  LVS负载均衡层

        LVS负载均衡层主要用来抵御大流量及转发数据功能，一般基于TCP/IP 四层协议进行转发，根据不同的内部环境使用的转发方式也不一样，通常DR模式效率比较高，LVS+keepalived结合，可以使用keepalived去管理我们整个配置文件，让负责均衡变得简单实用，可以各种策划来检查后端Nginx或者Squid服务是否正常。

        LVS简单工作原理：用户请求LVS VIP，LVS根据转发方式和算法，将请求转发给后端服务器，后端服务器接受到请求，返回给用户，对于用户来说，看不到WEB后端具体的应用。

运维人员在维护LVS中，需要密切关注LVS当前转发连接数及系统LVS日志。通过监控平台监控VIP、真实IP的情况、连接数的情况。

2)  Nginx反向代理层

       Nginx是目前主流的高性能WEB服务器，Nginx因为非常不错的稳定性、丰富的功能集、示例配置文件和低系统资源的消耗等优点，目前发展势头非常火爆。

       Nginx主要基于7层应用，能够实现各种规则转发，反向代理我们后端的JAVA、PHP动态服务器，同时Nginx本身处理静态页面的能力官方理论并发5w/s，同时Nginx还可以作为缓存服务器存储我们静态页面缓存，性能跟squid不相上下。

       作为IT运维人员在日常运维中，需要长期的关注网站的整体运行情况，分析网站瓶颈，不断优化Nginx的相关参数，并确保Nginx跟后端服务连接是否有异常等。

3)  后端服务层

       后端存放我们真正的网站和后台服务，通过前端Nnginx调用，后端常见的服务解析软件，如果是jsp语言的话，容器为Tomcat、Resin、Weblogic等等。

       如果是PHP程序，我们就需要安装PHP环境来解析php代码，然后通过前端Nginx反向代理提供给用户访问。

在日常的运维中，需要注意后端服务层的监控，及连接数的问题，要实时关注并监控后端服务的正常，配置多实例，冗余案例。

4)  数据库层

       目前互联网主流数据库有Mysql、Mariadb、mongodb、Oracle等等，对于数据库是整个架构的核心层，而且数据是企业生存之本，所以数据库的架构和维护也是至关重要的。中大型的互联网公司都有自己专职的DBA人员负责Mysql的运行和维护。

       对于IT运维人员在维护数据库时需要密切关注数据库并发数、连接池等变化，关注数据库主从、读写分离状态及日志的变化情况，并制定完整的备份机制完成数据库的备份，有问题及时处理。

    人们一直在推动MySQL发展到它的极限。这里是100条调节和优化MySQL安装的技巧。一些技巧是针对特定的安装环境的，但这些思路是通用的。我已经把他们分成几类，来帮助你掌握更多MySQL的调节和优化技巧。

MySQL 
服务器硬件和操作系统调节:
1. 拥有足够的物理内存来把整个InnoDB文件加载到内存中——在内存中访问文件时的速度要比在硬盘中访问时快的多。
2. 不惜一切代价避免使用Swap交换分区 – 交换时是从硬盘读取的，它的速度很慢。
3. 使用电池供电的RAM（注：RAM即随机存储器）。
4. 使用高级的RAID（注：Redundant Arraysof Inexpensive Disks，即磁盘阵列）
 – 最好是RAID10或更高。
5. 避免RAID5（注：一种存储性能、数据安全和存储成本兼顾的存储解决方案） – 确保数据库完整性的校验是要付出代价的。
6. 将操作系统和数据分区分开，不仅仅是逻辑上，还包括物理上 – 操作系统的读写操作会影响数据库的性能。
7. 把MySQL临时空间和复制日志与数据放到不同的分区 – 当数据库后台从磁盘进行读写操作时会影响数据库的性能。
8. 更多的磁盘空间等于更快的速度。
9. 更好更快的磁盘。
10. 使用SAS（注： Serial AttachedSCSI，即串行连接SCSI）代替SATA（注：SATA，即串口硬盘）。
11. 较小的硬盘 比 较大的硬盘快，尤其是在RAID配置的情况下。
12. 使用电池支持的高速缓存RAID控制器。
13. 避免使用软件磁盘阵列。
14. 考虑为数据分区使用固态IO卡 (不是磁盘驱动器) –
 这些卡能够为几乎任何数量的数据支持2GB/s的写入速度。
15. 在Linux中设置swappiness的值为0–
 在数据库服务器中没有理由缓存文件，这是一个服务器或台式机的优势。
16. 如果可以的话，使用  noatime和nodirtime挂载文件系统 – 没有理由更新访问数据库文件的修改时间。
17. 使用 XFS文件系统 – 一种比ext3更快、更小的文件系统，并且有许多日志选项，而且ext3已被证实与MySQL有双缓冲问题。
18. 调整 XFS文件系统日志和缓冲变量 – 为了最高性能标准。
19. 在 Linux系统中,
使用 NOOP 
或者 DEADLINE IO定时调度程序 – 同 NOOP和 DEADLINE定时调度程序相比，这个
 CFQ和 ANTICIPATORY定时调度程序 显得非常慢。
20. 使用64位的操作系统 – 对于MySQL，会有更大的内存支持和使用。
21. 删除服务器上未使用的安装包和守护进程 – 更少的资源占用。
22. 把使用MySQL的host和你的MySQL host放到一个hosts文件中
 – 没有DNS查找。
23. 切勿强制杀死一个MySQL进程 – 你会损坏数据库和正在运行备份的程序。
24. 把服务器贡献给MySQL– 后台进程和其他服务能够缩短数据库占用CPU的时间。
MySQL 
配置:
25. 当写入时，使用 innodb_flush_method=O_DIRECT来避免双缓冲。
26. 避免使用 O_DIRECT和 EXT3
文件系统– 你将序列化所有要写入的。
27. 分配足够的 innodb_buffer_pool_size来加载整个 InnoDB文件到内存中– 少从磁盘中读取。
28. 不要将 innodb_log_file_size参数设置太大， 这样可以更快同时有更多的磁盘空间– 丢掉多的日志通常是好的，在数据库崩溃后可以降低恢复数据库的时间。
29. 不要混用 innodb_thread_concurrency和 thread_concurrency参数– 这2个值是不兼容的。
30. 分配一个极小的数量给 max_connections参数 – 太多的连接会用尽RAM并锁定MySQL服务。
31. 保持 thread_cache在一个相对较高的数字，大约 16– 防止打开连接时缓慢。
32. 使用skip-name-resolve参数 – 去掉DNS查找。
33.如果你的查询都是重复的，并且数据不常常发生变化，那么可以使用查询缓存。但是如果你的数据经常发生变化，那么使用查询缓存会让你感到失望。
34.增大temp_table_size值，以防止写入磁盘
35.增大max_heap_table_size值，以防止写入磁盘
36.不要把sort_buffer_size值设置的太高，否则的话你的内存将会很快耗尽
37.根据key_read_requests和key_reads值来决定key_buffer的大小，一般情况下key_read_requests应该比key_reads值高，否则你不能高效的使用key_buffer
38.将innodb_flush_log_at_trx_commit设置为0将会提高性能，但是如果你要保持默认值（1）的话，那么你就要确保数据的完整性，同时你也要确保复制不会滞后。
39.你要有一个测试环境，来测试你的配置，并且在不影响正常生产的情况下，可以常常进行重启。
MySQL模式优化:
40. 保持你的数据库整理性。
41. 旧数据归档 – 删除多余的行返回或搜索查询。
42. 将您的数据加上索引.
43. 不要过度使用索引，比较与查询.
44. 压缩文字和BLOB数据类型 – 以节省空间和减少磁盘读取次数.
45. UTF 8和UTF16都低于latin1执行效率.
46. 有节制地使用触发器.
47. 冗余数据保持到最低限度 – 不重复不必要的数据.
48. 使用链接表，而不是扩展行.
49. 注意数据类型，在您的真实数据中，尽可能使用最小的一个.
50. 如果其他数据经常被用于查询时，而BLOB / TEXT数据不是，就把BLOB / TEXT数据从其他数据分离出来.
51.检查和经常优化表.
52. 经常重写InnoDB表优化.
53. 有时，当添加列时删除索引，然后在添加回来索引，这样就会更快.
54. 针对不同的需求，使用不同的存储引擎.
55. 使用归档存储引擎日志表或审计表-这是更有效地写道.
56.  会话数据存储在缓存（memcache）的而不是MySQL中 – 缓存允许自动自动填值的，并阻止您创建难以读取和写入到MySQL的时空数据.
57.存储可变长度的字符串时使用VARCHAR而不是CHAR–节省空间，因为固定长度的CHAR，而VARCHAR长度不固定（UTF8不受此影响）.
58. 逐步进行模式的变化 – 一个小的变化，可以有巨大的影响.
59.在开发环境中测试所有模式，反映生产变化.
60. 不要随意更改你的配置文件中的值，它可以产生灾难性的影响.
61. 有时候，在MySQL的configs少即是多.
62.有疑问时使用一个通用的MySQL配置文件.
 
查询优化:
63. 使用慢查询日志去发现慢查询。
64. 使用执行计划去判断查询是否正常运行。
65. 总是去测试你的查询看看是否他们运行在最佳状态下 –久而久之性能总会变化。
66. 避免在整个表上使用count(*),它可能锁住整张表。
67. 使查询保持一致以便后续相似的查询可以使用查询缓存。
68. 在适当的情形下使用GROUP BY而不是DISTINCT。
69. 在WHERE, GROUP BY和ORDER BY子句中使用有索引的列。
70. 保持索引简单,不在多个索引中包含同一个列。
71. 有时候MySQL会使用错误的索引,对于这种情况使用USE
 INDEX。
72. 检查使用SQL_MODE=STRICT的问题。
73. 对于记录数小于5的索引字段，在UNION的时候使用LIMIT不是是用OR.
74. 为了 避免在更新前SELECT，使用INSERT ONDUPLICATE KEY或者INSERT
 IGNORE ,不要用UPDATE去实现。
75. 不要使用 MAX,使用索引字段和ORDER BY子句。
76. 避免使用ORDER BY RAND().
77. LIMIT M，N实际上可以减缓查询在某些情况下，有节制地使用。
78. 在WHERE子句中使用UNION代替子查询。
79. 对于UPDATES（更新），使用 SHARE MODE（共享模式），以防止独占锁。
80. 在重新启动的MySQL，记得来温暖你的数据库，以确保您的数据在内存和查询速度快。
81. 使用DROP TABLE，CREATE TABLEDELETE FROM从表中删除所有数据。
82. 最小化的数据在查询你需要的数据，使用*消耗大量的时间。
83. 考虑持久连接，而不是多个连接，以减少开销。
84. 基准查询，包括使用服务器上的负载，有时一个简单的查询可以影响其他查询。
85. 当负载增加您的服务器上，使用SHOW PROCESSLIST查看慢的和有问题的查询。
86. 在开发环境中产生的镜像数据中 测试的所有可疑的查询。

MySQL 备份过程:

87. 从二级复制服务器上进行备份。
88. 在进行备份期间停止复制，以避免在数据依赖和外键约束上出现不一致。
89. 彻底停止MySQL，从数据库文件进行备份。
90. 如果使用 MySQL dump进行备份，请同时备份二进制日志文件 – 确保复制没有中断。
91. 不要信任LVM快照 – 这很可能产生数据不一致，将来会给你带来麻烦。
92. 为了更容易进行单表恢复，以表为单位导出数据 – 如果数据是与其他表隔离的。
93. 当使用mysqldump时请使用 –opt。
94. 在备份之前检查和优化表。
95. 为了更快的进行导入，在导入时临时禁用外键约束。
96. 为了更快的进行导入，在导入时临时禁用唯一性检测。
97. 在每一次备份后计算数据库，表以及索引的尺寸，以便更够监控数据尺寸的增长。
98. 通过自动调度脚本监控复制实例的错误和延迟。
99. 定期执行备份。
100. 定期测试你的备份，并执行MySQL监控。
 

数据库表表面上存在索引和防错机制，然而一个简单的查询就会耗费很长时间。Web应用程序或许在开发环境中运行良好，但在产品环境中表现同样糟糕。如果你是个数据库管理员，你很有可能已经在某个阶段遇到上述情况。因此，本文将介绍对MySQL进行性能优化的技巧和窍门。
1.存储引擎的选择
如果数据表需要事务处理，应该考虑使用InnoDB，因为它完全符合ACID特性。如果不需要事务处理，使用默认存储引擎MyISAM是比较明智的。并且不要尝试同时使用这两个存储引擎。思考一下：在一个事务处理中，一些数据表使用InnoDB，而其余的使用MyISAM。结果呢?整个subject将被取消，只有那些在事务处理中的被带回到原始状态，其余的被提交的数据转存，这将导致整个数据库的冲突。然而存在一个简单的方法可以同时利用两个存储引擎的优势。目前大多数MySQL套件中包括InnoDB、编译器和链表，但如果你选择MyISAM，你仍然可以单独下载InnoDB，并把它作为一个插件。很简单的方法，不是吗?
2.计数问题
如果数据表采用的存储引擎支持事务处理(如InnoDB)，你就不应使用COUNT(*)计算数据表中的行数。这是因为在产品类数据库使用COUNT(*)，最多返回一个近似值，因为在某个特定时间，总有一些事务处理正在运行。如果使用COUNT(*)显然会产生bug，出现这种错误结果。
3.反复测试查询
查询最棘手的问题并不是无论怎样小心总会出现错误，并导致bug出现。恰恰相反，问题是在大多数情况下bug出现时，应用程序或数据库已经上线。的确不存在针对该问题切实可行的解决方法，除非将测试样本在应用程序或数据库上运行。任何数据库查询只有经过上千个记录的大量样本测试，才能被认可。
4.避免全表扫描
通常情况下，如果MySQL(或者其他关系数据库模型)需要在数据表中搜索或扫描任意特定记录时，就会用到全表扫描。此外，通常最简单的方法是使用索引表，以解决全表扫描引起的低效能问题。然而，正如我们在随后的问题中看到的，这存在错误部分。
5.使用”EXPLAIN”进行查询
当需要调试时，EXPLAIN是一个很好的命令，下面将对EXPLAIN进行深入探讨。
首先，创建一个简单的数据表：
代码如下:

CREATETABLE'awesome_pcq'(
'emp_id'INT(10)NOTNULL
DEFAULT'0',
'full_name'VARCHAR(100)NOTNULL,
'email_id'VARCHAR(100)NOTNULL,
'password'VARCHAR(50)NOTNULL,
'deleted'TINYINT(4)NOTNULL,
PRIMARYKEY('emp_id')
) COLLATE='utf8_general_ci'
ENGINE=InnoDB
ROW_FORMAT=DEFAULT 

这个数据表一目了然，共有五列，最后一列“deleted”是一个Boolean类变量flag来检查帐号是活动的还是已被删除。接下来，您需要用样本记录填充这个表(比如，100个雇员记录)。正如你看到的，主键是“emp_id”。因此，使用电子邮件地址和密码字段，我们可以很容易地创建一个查询，以验证或拒绝登录请求，如下(实例一)：
代码如下:

SELECTCOUNT(*)FROMawesome_pcqWHERE
email_id='blahblah'ANDpassword='blahblah'ANDdeleted=0 
之前我们提到，要避免使用COUNT(*)。代码纠正如下(实例二)：
代码如下:

SELECTemp_idFROMawesome_pcqWHERE
email_id='blahblah'ANDpassword='blahblah'ANDdeleted=0 
现在回想一下，在实例一中，代码查询定位并返回“email_id”和“password”等于给定值的行数。在实例二中，进行了同样的查询，不同的是明确要求列出“emp_id”所有满足给定的标准的值。哪个查询更费时?
很显然，这两个实例都是同样费时的数据库查询，因为无意间，两个实例查询都进行了全表扫描。为了更好地读懂指令，执行如下代码：
代码如下:

EXPLAINSELECTemp_idFROMawesome_pcqWHERE
email_id='blahblah'ANDpassword='blahblah'ANDdeleted=0 
在输出时，集中在倒数第二列：“rows”。假设我们已经将表填充了100个记录，它会在第一行显示100，这是MySQL需要进行扫描用来计算查询的结果的行数。这说明了什么?这需要全表扫描。为了克服这个弊端，则需要添加索引。
6.添加索引
先从重要的说起：给每一个可能遇到的次要问题创建索引并不明智。过多的索引会导致效能减慢和资源占用。在进一步讨论之前，在实例中创建一个样本索引：
代码如下:

ALTERTABLE'awesome_pcq'ADDINDEX'LoginValidate'('email_id')

接下来，再次运行该查询：
代码如下:

EXPLAINSELECTemp_idFROMawesome_pcqWHERE
email_id='blahblah'ANDpassword='blahblah'ANDdeleted=0 

请注意运行后的值。不是100，而是1。因此，为了给出查询结果，MySQL只扫描了1行，多亏先前创建的索引。你可能会注意到，索引只在电子邮件地址字段创建，而查询对其他字段同样进行了搜索。这表明MySQL先执行了一个cros-check，检查是否有在WHERE子句中的定义的值有索引指定，如果有这样的值就执行相应的操作。

但是，它不是每次重复将减少到一个。例如，如果不是唯一的索引字段(如employee names列可以有两行相同的值)，即使创建索引，也将有多个记录留下。但它仍然比全表扫描好。并且，在WHERE子句中指定列的顺序没有在这个过程中发挥作用。例如，如果在上面的查询中，改变字段的顺序，使电子邮件地址出现在最后，MySQL仍将遍历索引列的基础上。那么，就要在索引上动脑筋，注意如何避免大量的全表扫描，并获得更好的结果。不过，这需要经历一个很长的过程。
 

    索引是快速搜索的关键。MySQL索引的建立对于MySQL的高效运行是很重要的。下面介绍几种常见的MySQL索引类型。
在数据库表中，对字段建立索引可以大大提高查询速度。假如我们创建了一个 mytable表：
代码如下:
CREATE TABLE mytable(  ID INT NOT NULL,    username VARCHAR(16) NOT NULL  );
我们随机向里面插入了10000条记录，其中有一条：5555, admin。
在查找username="admin"的记录 SELECT * FROM mytable WHERE username='admin';时，如果在username上已经建立了索引，MySQL无须任何扫描，即准确可找到该记录。相反，MySQL会扫描所有记录，即要查询10000条记录。
索引分单列索引和组合索引。单列索引，即一个索引只包含单个列，一个表可以有多个单列索引，但这不是组合索引。组合索引，即一个索包含多个列。
MySQL索引类型包括：
一、普通索引
这是最基本的索引，它没有任何限制。它有以下几种创建方式：
1.创建索引
代码如下:
CREATE INDEX indexName ONmytable(username(length));
如果是CHAR，VARCHAR类型，length可以小于字段实际长度；如果是BLOB和TEXT类型，必须指定length，下同。
2.修改表结构
代码如下:

ALTER mytable ADD INDEX [indexName] ON (username(length)) -- 创建表的时候直接指定
CREATETABLE mytable(   ID INT NOT NULL,    usernameVARCHAR(16) NOT NULL,   INDEX [indexName](username(length))   ); 
--删除索引的语法：
DROPINDEX [indexName] ON mytable;
二、唯一索引
它与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。它有以下几种创建方式：
复制代码 代码如下:
CREATE UNIQUE INDEX indexName ONmytable(username(length))
-- 修改表结构
ALTER mytable ADD UNIQUE [indexName] ON (username(length))
-- 创建表的时候直接指定
CREATE TABLE mytable(   ID INT NOT NULL,    usernameVARCHAR(16) NOT NULL,   UNIQUE [indexName] (username(length))  );
三、主键索引
它是一种特殊的唯一索引，不允许有空值。一般是在建表的时候同时创建主键索引：
代码如下:
CREATE TABLE mytable(  ID INT NOT NULL,    username VARCHAR(16) NOT NULL,  PRIMARY KEY(ID)   );
当然也可以用 ALTER命令。记住：一个表只能有一个主键。
四、组合索引
为了形象地对比单列索引和组合索引，为表添加多个字段：
代码如下:
CREATE TABLE mytable(  ID INT NOT NULL,    username VARCHAR(16) NOT NULL,  city VARCHAR(50) NOT NULL,   age INT NOT NULL  ); 
为了进一步榨取MySQL的效率，就要考虑建立组合索引。就是将 name, city, age建到一个索引里：
代码如下:
ALTER TABLE mytable ADD INDEXname_city_age (name(10),city,age);[code]
建表时，usernname长度为 16，这里用 10。这是因为一般情况下名字的长度不会超过10，这样会加速索引查询速度，还会减少索引文件的大小，提高INSERT的更新速度。
如果分别在 usernname，city，age上建立单列索引，让该表有3个单列索引，查询时和上述的组合索引效率也会大不一样，远远低于我们的组合索引。虽然此时有了三个索引，但MySQL只能用到其中的那个它认为似乎是最有效率的单列索引。
建立这样的组合索引，其实是相当于分别建立了下面三组组合索引：
usernname,city,age  usernname,city   usernname 为什么没有 city，age这样的组合索引呢？这是因为MySQL组合索引“最左前缀”的结果。简单的理解就是只从最左面的开始组合。并不是只要包含这三列的查询都会用到该组合索引，下面的几个SQL就会用到这个组合索引：
[code]
SELECT * FROM mytable WHREE username="admin" AND city="郑州"  SELECT * FROM mytable WHREE username="admin"
而下面几个则不会用到：
代码如下:

SELECT * FROM mytable WHREE age=20 AND city="郑州"  SELECT * FROM mytableWHREE city="郑州"
五、建立索引的时机
到这里我们已经学会了建立索引，那么我们需要在什么情况下建立索引呢？一般来说，在WHERE和JOIN中出现的列需要建立索引，但也不完全如此，因为MySQL只对&lt;，&lt;=，=，&gt;，&gt;=，BETWEEN，IN，以及某些时候的LIKE才会使用索引。例如：
代码如下:
SELECT t.Name  FROM mytablet LEFT JOIN mytable m    ON t.Name=m.username WHERE m.age=20 ANDm.city='郑州'
此时就需要对city和age建立索引，由于mytable表的userame也出现在了JOIN子句中，也有对它建立索引的必要。
刚才提到只有某些时候的LIKE才需建立索引。因为在以通配符%和_开头作查询时，MySQL不会使用索引。例如下句会使用索引：
 代码如下:

SELECT * FROM mytable WHERE username like'admin%'

而下句就不会使用：
代码如下:
SELECT * FROM mytable WHEREt Namelike'%admin'
因此，在使用LIKE时应注意以上的区别。
六、索引的不足之处
上面都在说使用索引的好处，但过多的使用索引将会造成滥用。因此索引也会有它的缺点：
1.虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。
2.建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件的会膨胀很快。
索引只是提高效率的一个因素，如果你的MySQL有大数据量的表，就需要花时间研究建立最优秀的索引，或优化查询语句。
七、使用索引的注意事项
使用索引时，有以下一些技巧和注意事项：
1.索引不会包含有NULL值的列
只要列中包含有NULL值都将不会被包含在索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。
2.使用短索引
对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个CHAR(255)的列，如果在前10个或20个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。
3.索引列排序
MySQL查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。
4.like语句操作
一般情况下不鼓励使用like操作，如果非使用不可，如何使用也是一个问题。like“%aaa%”
 不会使用索引而like“aaa%”可以使用索引。
5.不要在列上进行运算
代码如下:

select * from users where YEAR(adddate)&lt;2007; 
将在每个行上进行运算，这将导致索引失效而进行全表扫描，因此我们可以改成:
代码如下:
select * from users whereadddate&lt;‘2007-01-01';
6.不使用NOT IN和&lt;&gt;操作
以上，就对其中MySQL索引类型进行了介绍。希望对大家有所帮助。
 

1.第一种查询方法：
     SELECT table_name, column_name from information_schema.columns WHERE column_name LIKE 'Name';
2.第二种查询方法：
    SELECT column_name from information_schema.columns WHERE column_name LIKE ’%searchTerm%’ AND table_schema = ‘DateDB’
3.第三种查询方法：
    SELECT column_name from information_schema.columns WHERE column_name LIKE ’%searchTerm%’ AND table_schema = ‘DateDB’ AND table_name = ‘DBTable’




        对于多数应用来说，MySQL都是作为最关键的数据存储中心的，所以，如何让MySQL提供HA服务，是我们不得不面对的一个问题。当master当机的时候，我们如何保证数据尽可能的不丢失，如何保证快速的获知master当机并进行相应的故障转移处理，都是需要我们好好思考的。这里，将结合这段时间做的MySQL proxy以及toolsets相关工作，说说我们现阶段以及后续会在项目中采用的MySQL HA方案。


Replication


        要保证MySQL数据不丢失，replication是一个很好的解决方案，而MySQL也提供了一套强大的replication机制。只是我们需要知道，为了性能考量，replication是采用的asynchronous模式，也就是写入的数据并不会同步更新到slave上面，如果这时候master当机，我们仍然可能会面临数据丢失的风险。

        为了解决这个问题，我们可以使用semi-synchronous replication，semi-synchronous replication的原理很简单，当master处理完一个事务，它会等待至少一个支持semi-synchronous的slave确认收到了该事件并将其写入relay-log之后，才会返回。这样即使master当机，最少也有一个slave获取到了完整的数据。

       但是，semi-synchronous并不是100%的保证数据不会丢失，如果master在完成事务并将其发送给slave的时候崩溃，仍然可能造成数据丢失。只是相比于传统的异步复制，semi-synchronous replication能极大地提升数据安全。更为重要的是，它并不慢，MHA的作者都说他们在facebook的生产环境中使用了semi-synchronous（这里），所以我觉得真心没必要担心它的性能问题，除非你的业务量级已经完全超越了facebook或者google。在这篇文章里面已经提到，MySQL
 5.7之后已经使用了Loss-Less Semi-Synchronous replication，所以丢数据的概率已经很小了。

       如果真的想完全保证数据不会丢失，现阶段一个比较好的办法就是使用gelera，一个MySQL集群解决方案，它通过同时写三份的策略来保证数据不会丢失。笔者没有任何使用gelera的经验，只是知道业界已经有公司将其用于生产环境中，性能应该也不是问题。但gelera对MySQL代码侵入性较强，可能对某些有代码洁癖的同学来说不合适了:-)

      我们还可以使用drbd来实现MySQL数据复制，MySQL官方文档有一篇文档有详细介绍，但笔者并未采用这套方案，MHA的作者写了一些采用drdb的问题，在这里，仅供参考。

      在后续的项目中，笔者会优先使用semi-synchronous replication的解决方案，如果数据真的非常重要，则会考虑使用gelera。


Monitor


      前面我们说了使用replication机制来保证master当机之后尽可能的数据不丢失，但是我们不能等到master当了几分钟才知道出现问题了。所以一套好的监控工具是必不可少的。

      当master当掉之后，monitor能快速的检测到并做后续处理，譬如邮件通知管理员，或者通知守护程序快速进行failover。

      通常，对于一个服务的监控，我们采用keepalived或者heartbeat的方式，这样当master当机之后，我们能很方便的切换到备机上面。但他们仍然不能很即时的检测到服务不可用。笔者的公司现阶段使用的是keepalived的方式，但后续笔者更倾向于使用zookeeper来解决整个MySQL集群的monitor以及failover。

      对于任何一个MySQL实例，我们都有一个对应的agent程序，agent跟该MySQL实例放到同一台机器上面，并且定时的对MySQL实例发送ping命令检测其可用性，同时该agent通过ephemeral的方式挂载到zookeeper上面。这样，我们可以就能知道MySQL是否当机，主要有以下几种情况：

机器当机，这样MySQL以及agent都会当掉，agent与zookeeper连接自然断开MySQL当掉，agent发现ping不通，主动断开与zookeeper的连接Agent当掉，但MySQL未当

      上面三种情况，我们都可以认为MySQL机器出现了问题，并且zookeeper能够立即感知。agent与zookeeper断开了连接，zookeeper触发相应的children changed事件，监控到该事件的管控服务就可以做相应的处理。譬如如果是上面前两种情况，管控服务就能自动进行failover，但如果是第三种，则可能不做处理，等待机器上面crontab或者supersivord等相关服务自动重启agent。

      使用zookeeper的好处在于它能很方便的对整个集群进行监控，并能即时的获取整个集群的变化信息并触发相应的事件通知感兴趣的服务，同时协调多个服务进行相关处理。而这些是keepalived或者heartbeat做不到或者做起来太麻烦的。

     使用zookeeper的问题在于部署起来较为复杂，同时如果进行了failover，如何让应用程序获取到最新的数据库地址也是一个比较麻烦的问题。

     对于部署问题，我们要保证一个MySQL搭配一个agent，幸好这年头有了docker，所以真心很简单。而对于第二个数据库地址更改的问题，其实并不是使用了zookeeper才会有的，我们可以通知应用动态更新配置信息，VIP，或者使用proxy来解决。

     虽然zookeeper的好处很多，但如果你的业务不复杂，譬如只有一个master，一个slave，zookeeper可能并不是最好的选择，没准keepalived就够了。


Failover


     通过monitor，我们可以很方便的进行MySQL监控，同时在MySQL当机之后通知相应的服务做failover处理，假设现在有这样的一个MySQL集群，a为master，b，c为其slave，当a当掉之后，我们需要做failover，那么我们选择b，c中的哪一个作为新的master呢？

     原则很简单，哪一个slave拥有最近最多的原master数据，就选哪一个作为新的master。我们可以通过show slave status这个命令来获知哪一个slave拥有最新的数据。我们只需要比较两个关键字段Master_Log_File以及Read_Master_Log_Pos，这两个值代表了slave读取到master哪一个binlog文件的哪一个位置，binlog的索引值越大，同时pos越大，则那一个slave就是能被提升为master。这里我们不讨论多个slave可能会被提升为master的情况。

     在前面的例子中，假设b被提升为master了，我们需要将c重新指向新的master b来开始复制。我们通过CHANGE MASTER TO来重新设置c的master，但是我们怎么知道要从b的binlog的哪一个文件，哪一个position开始复制呢？


GTID


     为了解决这一个问题，MySQL 5.6之后引入了GTID的概念，即uuid:gid，uuid为MySQL server的uuid，是全局唯一的，而gid则是一个递增的事务id，通过这两个东西，我们就能唯一标示一个记录到binlog中的事务。使用GTID，我们就能非常方便的进行failover的处理。

     仍然是前面的例子，假设b此时读取到的a最后一个GTID为3E11FA47-71CA-11E1-9E33-C80AA9429562:23，而c的为3E11FA47-71CA-11E1-9E33-C80AA9429562:15，当c指向新的master
 b的时候，我们通过GTID就可以知道，只要在b中的binlog中找到GTID为3E11FA47-71CA-11E1-9E33-C80AA9429562:15这个event，那么c就可以从它的下一个event的位置开始复制了。虽然查找binlog的方式仍然是顺序查找，稍显低效暴力，但比起我们自己去猜测哪一个filename和position，要方便太多了。

      google很早也有了一个Global Transaction ID的补丁，不过只是使用的一个递增的整形，LedisDB就借鉴了它的思路来实现failover，只不过google貌似现在也开始逐步迁移到MariaDB上面去了。

      MariaDB的GTID实现跟MySQL 5.6是不一样的，这点其实比较麻烦，对于我的MySQL工具集go-mysql来说，意味着要写两套不同的代码来处理GTID的情况了。后续是否支持MariaDB再看情况吧。


Pseudo GTID


      GTID虽然是一个好东西，但是仅限于MySQL 5.6+，当前仍然有大部分的业务使用的是5.6之前的版本，笔者的公司就是5.5的，而这些数据库至少长时间也不会升级到5.6的。所以我们仍然需要一套好的机制来选择master binlog的filename以及position。

      最初，笔者打算研究MHA的实现，它采用的是首先复制relay log来补足缺失的event的方式，但笔者不怎么信任relay log，同时加之MHA采用的是perl，一个让我完全看不懂的语言，所以放弃了继续研究。

     幸运的是，笔者遇到了orchestrator这个项目，这真的是一个非常神奇的项目，它采用了一种Pseudo GTID的方式，核心代码就是这个







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23




create
 database if not exists meta;

 

drop
 event if exists meta.create_pseudo_gtid_view_event;

 

delimiter
 ;;

create
 event if not exists

  meta.create_pseudo_gtid_view_event

  on
 schedule every 10 second starts current_timestamp

  on
 completion preserve

  enable

  do

    begin

    set
 @pseudo_gtid := uuid();

    set
 @_create_statement := concat('create or replace view meta.pseudo_gtid_view as select '', @pseudo_gtid, '' as pseudo_gtid_unique_val from dual');

    PREPARE
 st FROM @_create_statement;

    EXECUTE
 st;

    DEALLOCATE
 PREPARE st;

  end

;;

 

delimiter
 ;

 

set
 global event_scheduler := 1;








        它在MySQL上面创建了一个事件，每隔10s，就将一个uuid写入到一个view里面，而这个是会记录到binlog中的，虽然我们仍然不能像GTID那样直接定位到一个event，但也能定位到一个10s的区间了，这样我们就能在很小的一个区间里面对比两个MySQL的binlog了。

       继续上面的例子，假设c最后一次出现uuid的位置为s1，我们在b里面找到该uuid，位置为s2，然后依次对比后续的event，如果不一致，则可能出现了问题，停止复制。当遍历到c最后一个binlog event之后，我们就能得到此时b下一个event对应的filename以及position了，然后让c指向这个位置开始复制。

      使用Pseudo GTID需要slave打开log-slave-update的选项，考虑到GTID也必须打开该选项，所以个人感觉完全可以接受。

      在《MySQL High Availability》这本书中，作者使用了另一种GTID的做法，每次commit的时候，需要在一个表里面记录gtid，然后就通过这个gtid来找到对应的位置信息，有兴趣的同学，可以了解一下！

 
1.  数据的备份，恢复和还原准备：
（1）将要导入的.sql文件移至bin文件下，这样的路径比较方便
（2）同上面导出的第1步
（3） 进入MySQL：mysql
 -u 用户名 -p
　　 如我输入的命令行:mysql -u root-p (输入同样后会让你输入MySQL的密码)
（4）在MySQL-Front中新建你要建的数据库，这时是空数据库，如新建一个名为news的目标数据库
（5）输入：mysql&gt;use目标数据库名
　　如我输入的命令行:mysql&gt;usenews;
（6）导入文件：mysql&gt;source导入的文件名;
　　如我输入的命令行：mysql&gt;sourcenews.sql;
MySQL备份和还原,都是利用mysqldump、mysql和source命令来完成的。
 
1.  Win32下MySQL的备份与还原
（1）备份
　　开始菜单 |运行 | cmd |利用“cd\\Program
 Files\\MySQL\\MySQL Server 5.0\\bin”命令进入bin文件夹 |利用“mysqldump
-u用户名 -p databasename &gt;exportfilename”导出数据库到文件，如mysqldump
 -u root -p voice&gt;voice.sql，然后输入密码即可开始导出。
（2）还原
　　进入MySQL Command Line Client，输入密码，进入到“mysql&gt;”，输入命令"showdatabases;"，回车，看看有些什么数据库;建立你要还原的数据库，输入"create
 database voice;"，回车;切换到刚建立的数据库，输入"usevoice;"，回车;导入数据，输入"source
 voice.sql;"，回车，开始导入，再次出现"mysql&gt;"并且没有提示错误即还原成功。
2.  Linux下MySQL的备份与还原
（1）备份
[root@localhost ~]# cd/var/lib/mysql (进入到MySQL库目录，根据自己的MySQL的安装情况调整目录)
[root@localhost mysql]#mysqldump -u root -p Test&gt;Test0809.sql，输入密码即可。
（2）还原
　　方法一：
[root@localhost ~]# mysql-u root -p回车，输入密码，进入MySQL的控制台"mysql&gt;"，同1.2还原。
　　方法二：
[root@localhost ~]# cd/var/lib/mysql (进入到MySQL库目录，根据自己的MySQL的安装情况调整目录)
[root@localhost mysql]#mysql -u root -pTest
moodle_bak.sql是需要恢复的文件名
 

1.      数据接收报错，数据引擎支持不好，需要优化mysql配置：
问题如下：
SQLException: Incorrect key file for table './bd_jrdb/Bond_CreditGrading.MYI'; try to repair it
SQLException: Incorrect key file for table './bd_jrdb/LC_IPODeclaration.MYI'; try to repair it
需要修复下
问题如下：
表：LC_SMAttendInfo。Could not commit JDBC transaction; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Communications link failure during commit(). Transaction resolutionunknown.
表：Bond_RedemptionBBn。Could not commit JDBC transaction; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Communications link failure during commit(). Transaction resolution unknown.
另外Mysql配置文件需要优化，例如my.cnf关键配置如下
innodb_log_file_size=512M
innodb_log_buffer_size=16M
max_connections = 2000
table_open_cache = 8000
max_connect_errors = 1844674407370954751
max_allowed_packet = 1G
wait_timeout = 86400 
修复完以后，重启：service myqld restart.使之生效！

1.      对于数据库表try to repair it异常的处理：
       报错如下所示：
        java.sql.SQLException:Incorrect key file for table './bd_jrdb/JYDB_DeleteRec.MYI'; try to repair it
      在数据库输入如下命令查看错误：CHECKTABLE NI_DynamicNews;
      检测错误结果如图1.9：


                                                                                                       图1.9
         测试发现有如下错误，使用修复：REPAIRTABLE NI_DynamicNews;
         修复结果如图1.10：


                                                                                                    图1.10

    原因：
同一个ip在短时间内产生太多（超过mysql数据库max_connect_errors的最大值）中断的数据库连接而导致的阻塞；
   解决方法：
   1、提高允许的max_connection_errors数量：
　　① 进入Mysql数据库查看max_connection_errors： show variables like '%max_connect_errors%'; 
　   ② 修改max_connection_errors的数量为1000： set global max_connect_errors = 1000; 
　　③ 查看是否修改成功：show variables like '%max_connect_errors%'; 

   2、使用mysqladmin flush-hosts 命令清理一下hosts文件（不知道mysqladmin在哪个目录下可以使用命令查找：whereis mysqladmin）；
　　① 在查找到的目录下使用命令修改：mysqladmin --socket=/tmp/kkimdb.sock --port=3306 -uhyman -p flush-hosts
　　备注： 配置有master/slave主从数据库的要把主库和从库都修改一遍的（我就吃了这个亏明明很容易的几条命令结果折腾了大半天）； 
      mysql&gt; flush hosts; 也可以
      max_connect_errors是一个MySQL中与安全有关的计数器值，它负责阻止过多尝试失败的客户端以防止暴力破解密码的情况。max_connect_errors的值与性能并无太大关系。
      默认情况下，my.cnf文件中可能没有此行，如果需要设置此数值，手动添加即可。
[root@www ~]# vi /etc/my.cnf
max_connect_errors = 1000
      配置说明
      当此值设置为10时，意味着如果某一客户端尝试连接此MySQL服务器，但是失败（如密码错误等等）10次，则MySQL会无条件强制阻止此客户端连接。
如果希望重置此计数器的值，则必须重启MySQL服务器或者执行 mysql&gt; flush hosts; 命令。 
      当这一客户端成功连接一次MySQL服务器后，针对此客户端的max_connect_errors会清零。
影响与错误形式
     如果max_connect_errors的设置过小，则网页可能提示无法连接数据库服务器；
而通过SSH的mysql命令连接数据库，则会返回 ERROR 1129 (00000): Host 'gateway' is blocked because of many connection errors; unblock with 'mysqladmin flush-hosts' 错误。 
     功能与作用
     一般来说建议数据库服务器不监听来自网络的连接，仅仅通过sock连接，这样可以防止绝大多数针对mysql的攻击；如果必须要开启mysql的网络连接，则最好设置此值，以防止穷举密码的攻击手段。





（一）   myisam存储引擎 

 myisam的引擎的物理结构：包括三种文件：.frm.myd .myi三种。 
 myisam特有的特性：
（1）    可以支持将数据文件和索引文件放在不同的地方，以达到性能优化的目的。
[root@rhel131mysql]# mkdir -p /tmp/mysql/data
[root@rhel131 mysql]# chown-R mysql.mysql /tmp/mysql/data
[root@rhel131 mysql]# mkdir-p /tmp/mysql/index
[root@rhel131 mysql]# chown-R mysql.mysql /tmp/mysql/index
mysql&gt;createtable t(id int) engine=myisam data directory='/tmp/mysql/data'indexdirectory='/tmp/mysql/index';
Query OK, 0 rows affected(0.10 sec)
mysql&gt;showtable status like 't' \G;
***************************1. row ***************************
          Name: t
        Engine:MyISAM
        Version: 10
     Row_format: Fixed
           Rows: 0
 Avg_row_length: 0
    Data_length: 0
Max_data_length: 1970324836974591
   Index_length: 1024
      Data_free: 0
 Auto_increment: NULL
    Create_time: 2013-10-24 12:46:07
    Update_time: 2013-10-24 12:46:07
     Check_time: NULL
      Collation: latin1_swedish_ci
       Checksum: NULL
 Create_options:
        Comment:
1 row in set (0.01 sec)
ERROR:
No query specified
查看一个产生的文件：
[root@rhel131 test]# pwd
/usr/local/mysql/data/test
[root@rhel131 test]# ll
total 16
-rw-r--r--  1 mysqlmysql   65 Jul 11 00:17 db.opt
-rw-rw----  1 mysqlmysql 8556 Oct 24 12:46 t.frm
lrwxrwxrwx  1 mysqlmysql   21 Oct 24 12:46 t.MYD -&gt;/tmp/mysql/data/t.MYD
lrwxrwxrwx  1 mysql mysql  22 Oct 24 12:46 t.MYI -&gt;/tmp/mysql/index/t.MYI
数据文件和索引文件都是软链接文件，接到了我指定的目录。
（2）     灵活的自动增长列类型
对已存在的表增加一个自动增长的列：
mysql&gt;altertable t add column id1 int not null auto_increment,add primary key (id1);
Query OK, 0 rows affected(0.10 sec)
Records: 0  Duplicates:0  Warnings: 0
mysql&gt;desct;
+-------+---------+------+-----+---------+----------------+
| Field |Type    | Null | Key | Default|Extra          |
+-------+---------+------+-----+---------+----------------+
| id    |int(11) | YES  |     |NULL   |               |
| id1   | int(11)| NO   | PRI | NULL    |auto_increment |
+-------+---------+------+-----+---------+----------------+
2 rows in set (0.00 sec)
mysql&gt;insertinto t (id) values(1);
Query OK, 1 row affected(0.03 sec)
mysql&gt;insertinto t (id) values(1);
Query OK, 1 row affected(0.01 sec)
mysql&gt;insertinto t (id) values(1);
Query OK, 1 row affected(0.00 sec)
mysql&gt;insertinto t (id) values(1);
Query OK, 1 row affected(0.00 sec)
mysql&gt;select* from t;
+------+-----+
| id   | id1 |
+------+-----+
|    1|   1 |
|    1|   2 |
|    1|   3 |
|    1|   4 |
+------+-----+
4 rows in set (0.00 sec)
（3）     不支持事务的特性：
mysql&gt;droptable t;
Query OK, 0 rows affected(0.00 sec)
mysql&gt;createtable t(id int) engine=myisam;
Query OK, 0 rows affected(0.02 sec)
查询是否自动提交，将自动提交给关闭
mysql&gt; show variableslike '%autocommit%';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
|autocommit    | ON    |
+---------------+-------+
1 row in set (0.01 sec)
mysql&gt;setsession autocommit=off;
Query OK, 0 rows affected(0.01 sec)
mysql&gt;showvariables like '%autocommit%';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
|autocommit    | OFF   |
+---------------+-------+
1 row in set (0.01 sec)
插入一条数据
mysql&gt;insertinto t values(1);
Query OK, 1 row affected(0.00 sec)
mysql&gt;select* from t;
+------+
| id   |
+------+
|    1 |
+------+
1 row in set (0.01 sec)
做回滚操作后，如果支持事务操作的话，应该后取消我刚刚插入的一笔数据。
mysql&gt;rollback;
Query OK, 0 rows affected,1 warning (0.00 sec)
不过做了回滚之后，这笔数据还在，说明myisam不支持事务操作，在插入数据后直接写到了磁盘上。
mysql&gt;select* from t;
+------+
| id   |
+------+
|    1 |
+------+
1 row in set (0.00 sec)

（二）InnoDB存储引擎

 ·遵循ACID，支持commit，rollback和故障恢复，是事务安全的
·行级锁定，Oracle-style读一致性改善了多用户并发操作性能
·支持FOREIGNKEY参照完整性
·轻松地与其他存储引擎表组合，例如与MEMORY表的JOIN
      InnoDB在内存中维持着自己的缓冲区，用来缓存数据和索引。InnoDB的数据和索引存放在表空间中，表空间可以是共享的，也可以是独享的。
（1）     独享表空间
      开启独享表空间模式之后InnoDB以table_name.idb命名在数据库目录之中保存新创建的表，数据和索引都保存在.idb文件，.frm仍然会创建用来保存元数据。 
      即使使用独享表空间，共享表空间也会存在，因为需要它存放一些undo信息和其他元数据信息。因此不能像MyISAM表文件那样在数据库目录间随便移动.idb文件，因为共享表空间里会保存数据库名，数据库间的移动应使用RENAMETABLE语句：RENAMETABLEdb1.tbl_nameTO
 db2.tbl_name;    
      .idb文件的恢复方法：
1、 ALTERTABLE tbl_nameDISCARD TABLESPACE;丢弃现有表空间文件
2、 复制备份的.idb文件至正确的目录
3、 ALTERTABLE tbl_name IMPORTTABLESPACE;使用新的idb文件
（2）     InnoDB启动选项和系统变量




Variables


Description




innodb


控制InnoDB的加载。OFF，ON，FORCE




innodb_additional_mem_pool_size


InnoDB用于保存数据字典信息和其他内部数据结构的内存区大小，默认8M。超出时，使用操作系统内存并向error
 log写错误信息




innodb_autoextend_increment


自动增长的共享表空间写满时的增长大小




innodb_autoinc_lock_mode


auto_increment自动增长值的锁定模式




innodb_buffer_pool_size


InnoDB缓存数据和索引的缓冲区大小。默认是128M，数据库专用服务器可以将其设置为物理内存的80%




innodb_change_buffering


开启修改缓存的类型 inserts, deletes,  purges, changes, all, none




innodb_checksums


开启校验和




innodb_commit_concurrency


同时刻可以进行提交操作的线程数。值为0允许任意多事务同时提交




innodb_concurrency_tickets


线程通过innodb_thread_concurrency并发线程数验证后，可以得到一个innodb_concurrency_tickets数量的访问次数，在该次数范围内不需要再进行并发线程数验证。




innodb_data_file_path


指定数据文件，格式为
file_name:file_size[:autoextend[:max:max_file_size]]
autoextend和max选项只能用于最后一个数据文件




innodb_data_home_dir


数据文件根目录




innodb_doublewrite


启用后，InnoDB分两次存储数据，第一次写入buffer，第二次实际写入数据文件




innodb_fast_shutdown


InnoDB关闭模式，默认1为快速关闭（正常关闭）




innodb_file_format


新创建的InnoDB表的文件格式，Antelope和Barracuda




innodb_file_format_check


InnoDB是否检查共享表空间的file format tag，tag大于当前InnoDB所支持版本时InnoDB启动出错，反之，InnoDB向tag写入当前innodb_file_format_max的值




innodb_file_format_max


向共享表空间file format tag写入的值




innodb_file_per_table


开启独享表空间




innodb_flush_log_at_trx_commit


0：每隔一秒将日志写入log  file并flush到磁盘
1：每次事务提交将日志写入log  file并flush到磁盘，默认
2：每次事务提交将日志写入log  file，每隔一秒flush到磁盘




innodb_flush_method


设置flush模式
fdatasync：InnoDB使用fsync()函数去更新日志和数据文件。默认。
O_DSYNC：InnoDB使用O_SYNC模式打开并更新日志文件，用fsync()函数去更新数据文件。
O_DIRECT：InnoDB使用O_DIRECT模式（跳过文件系统cache）打开数据文件，用fsync()函数去更新日志和数据文件




innodb_force_recovery


恢复模式 0-6




innodb_lock_wait_timeout


事务等待锁定的超时时间，仅对行锁定有效




innodb_log_buffer_size


InnoDB日志缓冲区大小




innodb_log_file_size


日志组中日志文件的大小，默认5MB，必须小于4GB




innodb_log_files_in_group


日志组中的日志成员数




innodb_log_group_home_dir


InnoDB日志根目录




innodb_max_dirty_pages_pct


脏数据所占的最大百分比




innodb_max_purge_lag


限制每次删除更新操作影响的最大行数，超过该值操作会被延迟




innodb_mirrored_log_groups


日志的镜像拷贝数量




innodb_old_blocks_pct


InnoDB缓冲区中old sublist百分比




innodb_old_blocks_time


在old sublist必须停留超过该变量的时间，才能移到new sublist中




innodb_open_files


限制同时打开.idb文件数，只有在独享表空间模式有效




innodb_purge_batch_size


redo中记录触发清除操作的粒度




innodb_purge_threads


InnoDB清除操作的线程数，默认0表示由InnoDB主线程完成清除操作。




innodb_read_ahead_threshold


顺序读取超过该值指定的页数才会预取




innodb_read_io_threads


读操作的I/O线程数，默认为4




innodb_replication_delay


innodb_thread_concurrency满时，slave端复制线程的延迟时间(ms)




innodb_rollback_on_timeout


默认情况下事务超时仅回滚最后一条语句。启用该选项后InnoDB中止并回滚整个事务




innodb_spin_wait_delay


两次尝试获取自旋锁的延迟时间




innodb_stats_on_metadata


启用后，InnoDB会在那些metadata statement（如SHOW 
 TABLE STATUS、SHOW INDEX）或访问INFORMATION_SCHEMA中TABLES、STATISTICS表时更新统计信息。




innodb_stats_sample_pages


ANALYZE TABLE计算时索引页采样数量




--innodb-status-file


InnoDB是否在数据目录中创建名为innodb_status.&lt;pid&gt;的状态文件，启用后，InnoDB每隔一段时间写入SHOW
 ENGINE INNODB STATUS的输出结果




innodb_strict_mode


遇到异常时返回错误而不是提示警告信息




innodb_support_xa


启用XA事务支持




innodb_sync_spin_loops


自旋锁超时等待时间




innodb_table_locks


InnoDB内部获取表锁




innodb_thread_concurrency


并发访问InnoDB的线程数，超过数量限制时进入FIFO队列等待。




innodb_thread_sleep_delay


并发访问数超过限制后线程的等待时间




innodb_use_native_aio


使用本地AIO




innodb_use_sys_malloc


InnoDB使用操作系统内存分配器还是InnoDB自己的




innodb_version


InnoDB版本号




innodb_write_io_threads


写操作的I/O线程数




（3）     InnoDB数据文件、日志文件的添加、删除、重置大小
加入新数据文件：
修改配置文件中innodb_data_file_path，加入新的数据文件名，大小。autoextend选项只能用在最后一个数据文件中。
实验中遇到的问题：
      原配置：innodb_data_file_path=ibdata1:10M:autoextend
      修改后：innodb_data_file_path=ibdata1: 10M;ibdata2:20M:autoextend
      重新启动服务器，showengines发现innodb没有启动起来，查阅错误日志出现如下提示：
InnoDB:Error:data file/app/mysql/var/ibdata1 is of a different size
InnoDB:227968pages(rounded down to MB)
InnoDB:thanspecified inthe .cnf file 640 pages!
原来，是因为第一个数据文件没有了autoextend选项，而该文件的实际大小超过了设置的10M，导致了文件大小不匹配的错误。
解决：innodb_data_file_path =ibdata1:3562M;ibdata2:20M:autoextend将ibdata1的大小设置为其实际大小，不能多也不能少（错误提示中的page=64MB，du命令计算出的结果有偏差），之后重启服务器，InnoDB成功启动，ibdata2也自动创建了。 
删除数据文件： 
不能直接在配置文件将innodb_data_file_path的数据文件移除，重启后错误日志会有如下提示：
      InnoDB:Error: tablespace size stored in header is 229248pages, but
InnoDB:thesum of data filesizes is only 227968 pages
InnoDB:Cannotstart InnoDB.The tail of the system tablespace is
InnoDB:missing.Have youedited innodb_data_file_path in my.cnf in an
InnoDB:inappropriateway,removing ibdata files from there?
InnoDB:Youcan setinnodb_force_recovery=1 in my.cnf to force
InnoDB:astartup if you aretrying to recover a badly corrupt database.
说明InnoDB会在数据文件头部存放表空间的大小，检查innodb_data_file_path中的数据文件容量之和不匹配就会出现上述的错误提示
             手册中推荐的方法：
1、 mysqldump转储InnoDB表
2、 关闭服务器
3、 删除（或移动至其他目录）所有存在的表空间文件，包括ibdata，ib_log和InnoDB表的所有.frm文件
4、 my.cnf重新配置表空间
5、 重启服务器
6、 导入转储文件
日志文件的修改可以参考：
http://www.mysqlperformanceblog.com/2011/07/09/how-to-change-innodb_log_file_size-safely/
（4）     InnoDB锁定机制
共享锁：允许事务读取行
排他锁：允许事务更新、删除行
意向共享锁：需要共享锁而资源被占用，则在表上加意向共享锁
意向排他锁：需要排他锁而资源被占用，则在表上加意向排他锁 




 


共享锁（S）


排他锁（X）


意向共享锁（IS）


意向排他锁（IX）




共享锁（S）


√


×


√


×




排他锁（X）


×


×


×


×




意向共享锁（IS）


√


×


√


√




意向排他锁（IX）


×


×


√


√




 
读一致性：在一个事务中，每次读取的结果都是一致的。




session  a


session  b




SET  autocommit=0;


SET  autocommit=0;




mysql&gt;  SELECT * FROM t1;
Empty  set (0.00 sec)


 




 


mysql&gt;  insert into t1 values(1,'a');




mysql&gt;  SELECT * FROM t1;
Empty  set (0.00 sec)


 




 


mysql&gt;  commit;




mysql&gt;  SELECT * FROM t1;
Empty  set (0.00 sec)


 




mysql&gt;  commit;


 




mysql&gt;  select * from t1;
+------+------+
|  id   | name |
+------+------+
|    1 | a     |
+------+------+


 




      看到，会话a在发出第一句SELECT后事务即算开始，一直到commit之前，无论会话b是否提交读取结果都保持一致。此时若想查看最新数据可以使用SELECT*
 FROM t1 LOCK INSHARE MODE;
再做一个测试：
CREATETABLE `t1` (
 `id` int(11) DEFAULTNULL,
 `name` varchar(20)DEFAULT NULL
)ENGINE=InnoDBDEFAULTCHARSET=utf8




session  a


session  b




SET  autocommit=0;


SET  autocommit=0;




mysql&gt;  select * from t1;
+------+------+
|  id   | name |
+------+------+
|    1 | a     |
|    2 | b     |
|    3 | c     |
+------+------+


mysql&gt;  select * from t1;
+------+------+
|  id   | name |
+------+------+
|    1 | a     |
|     2 | b    |
|    3 | c     |
+------+------+




mysql&gt;  update t1 set name='x' where id=3;
Query  OK, 1 row affected (0.00 sec)


 




 


mysql&gt;  update t2 set name='y' where id=2;
等待中




mysql&gt;  commit;


 




 


Query  OK, 1 row affected (11.34 sec)




在这个测试中，发现insert语句等待，说明此时session a的操作触发了表锁定，InnoDB不是行锁定？怎么锁表了呢？原来，只有当InnoDB使用了索引，才会触发行锁，因此加上索引再做一次测试：
CREATETABLE `t2` (
 `id`int(11) DEFAULTNULL,
 `name`varchar(20)DEFAULT NULL,
 KEY`id` (`id`)
)ENGINE=InnoDB DEFAULTCHARSET=utf8




session  a


session  b




SET  autocommit=0;


SET  autocommit=0;




mysql&gt;  select * from t2;
+------+------+
|  id   | name |
+------+------+
|    1 | a     |
|    2 | b     |
|    3 | c     |
+------+------+


mysql&gt;  select * from t2;
+------+------+
|  id   | name |
+------+------+
|    1 | a     |
|    2 | b     |
|    3 | c     |
+------+------+




mysql&gt;  update t2 set name='y' where id=1;
Query  OK, 1 row affected (0.00 sec)


 




 


mysql&gt;  update t2 set name='z' where id=2;
Query  OK, 1 row affected (0.00 sec)




 


mysql&gt;  update t2 set name='i' where id=1;
等待了




mysql&gt;  update t2 set name='j' where id=2;
ERROR  1213 (40001): Deadlock found when trying to get lock; try restarting  transaction


 




      看到，加了索引以后，update操作使用了行锁，行锁的一个弊端就是可能导致死锁。这么看来，InnoDB的行锁并不是随心所欲的使用，必须要考虑到索引，上面的这个例子中如果更新操作的where条件是name这个不加索引的列，一样触发表锁。而有些操作，使用索引还是需要全表扫描，优化器会放弃使用索引，这样的操作也会导致表锁定。
（5）     InnoDB与MyISAM的对比




 


InnoDB


MyISAM




存储


数据和索引存放在同一文件


数据和索引分开存储




可以共享表空间，也可以独享表空间


每张表分开存储




不存储行数，count(*)全表扫描


会存储行数




事务


事务安全，ACID


不支持事务




行级锁定


表级锁定




通过设置autocommit决定


每句语句自动提交




缓存


innodb_buffer  / log_pool_size
innodb_additional_mem_pool_size


key_buffer_size




缓存数据、索引，日志，数据字典


缓存索引




其他


不支持REPAIR TABLE，可以CHECK


支持CHECK，REPAIR  TABLE




具备故障恢复能力


通过myisamchk进行故障恢复




性能对比测试




插入速度


45.96 s


25.82 s




 查询行数


2.11 s


0.00 s




全表扫描


10.44 s


7.25 s




更新（索引查找）


1.49 s


0.62 s




更新（非索引查找）


7.45 s


9.44 s




数据容量


561 MB


502 MB




 
总结一下：
      MyISAM的前身ISAM在设计时就被设定为适合处理读大于写的环境，所以在测试数据中，我们也看到MyISAM在全表扫描和插入数据时的优异表现。而MyISAM会单独保存表的行数，因此查询行数时直接取出就好了。
      InnoDB则被设计成适用于高并发的事务处理环境，因此它在事务方面功能强大，并提供专门的事务日志，采用了表空间的概念。
（6）     InnoDB索引
聚集索引：
InnoDB将表中数据按主键顺序构造成一颗B+树，叶子节点存放着整张表的行记录数据（索引组织表，即叶子节点就是数据页）。因为数据页只能按一棵B+树排序，因此每张表只能有一个聚集索引（因此也只能有一个PRIMARYKEY）。
Oracle中，表的默认类型是堆表，数据按插入顺序存放，堆表上的索引存储rowid。 
辅助索引：
叶子节点除了包含索引键值外，还包含了聚集索引键值。一张表可以存在多个辅助索引。通过辅助索引查找时，InnoDB通过辅助索引叶子节点的指针获取主键，再通过主键索引找到完整的行。
一个比较有意思的实验：




CREATE TABLE `t1` (
  `a` int(11) NOT  NULL,
  `b` varchar(20)  DEFAULT NULL,
  PRIMARY KEY (`a`),
  KEY `b` (`b`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
 
INSERT INTO t1 VALUES(1,'d');
INSERT INTO t1 VALUES(2,'c');
INSERT INTO t1 VALUES(3,'b');
INSERT INTO t1 VALUES(4,'a');




      此时执行SELECT *FROM t1的结果会是如何呢？恐怕多数人答案都应该是：
+---+------+
|a |b    |
+---+------+
|1 |d    |
|2 |c    |
|3 |b    |
|4 |a    |
+---+------+
      但事实确实：
                    mysql&gt;SELECT* FROM t1;
+---+------+
|a |b    |
+---+------+
|4 |a    |
|3 |b    |
|2 |c    |
|1 |d    |
+---+------+
      查看一下这条语句的执行计划：
                    mysql&gt;EXPLAINSELECT * FROM t1 \G
***************************1.row***************************
          id:1
 select_type: SIMPLE
       table:t1
        type:index
possible_keys:NULL
         key:b
     key_len:63
         ref:NULL
        rows:4
       Extra:Using index 
      可以看到，优化器选择了辅助索引。这里面的原因，
第一， 辅助索引存放了主键的值，通过辅助索引能找完整的行信息。
第二， 因为聚集索引叶子节点存放的完整行信息肯定大于辅助索引叶子节点存放的数据，所以辅助索引页包含的行数多于聚集索引，查找时需要的页数也就相对少于聚集索引。
基于以上两点，InnoDB总是会先在辅助索引上判断是否能取得需要的信息。 
插入缓存（InsertBuffering）
应用程序的插入行操作通常是按主键递增的顺序插入的，和聚集索引的结构一致，因此插入聚集索引通常都是顺序的，不需要随机读取。但是对于辅助索引，插入和更新操作影响的页并不是顺序的，因此会导致大量的随机I/O操作。
 
InnoDB的插入缓存机制解决了以上的问题，它会检查辅助索引页是否在缓冲池中，如果在，InnoDB就直接对索引页应用变更。否则，InnoDB在insertbuffer中记录变更，并周期性地合并同一索引页上的操作，大大提高了对辅助索引执行插入操作的性能。 
自适应哈希索引（adaptivehashindex）：
InnoDB存在一个监控索引查找的机制，当发现建立哈希索引可以提升查询效率时，便会自动创建，因此称之为自适应（adaptive）哈希索引。哈希索引的建立基于表上已存在的B+树，并且可只在那些经常访问的索引页上建立。 
mysql&gt;SHOWENGINE INNODBSTATUS \G可以查询到当前insertbuffer和adaptivehashindex的信息：
-------------------------------------
INSERTBUFFERAND ADAPTIVEHASH INDEX
-------------------------------------
Ibuf:size1, free list len789, seg size 791,
460662inserts, 460662merged recs,28908 merges 
Hashtablesize 34679, nodeheap has 51 buffer(s)
0.00 hashsearches/s, 0.00non-hash searches/s 
（7）     InnoDB磁盘I/O和文件空间管理
两次写（DoublewriteBuffer）
在将页写入数据文件之前，InnoDB先将页写入共享表空间中的doublewritebuffer（为连续的页，顺序写入），只有在这部操作完成后InnoDB才会将页写入数据文件。两次写机制增加了数据库的可靠性，如果在写入数据文件中出现了宕机，可能出现页损坏的情况，这时只要找到共享表空间doublewrite中该页的副本拷贝到数据文件，再应用重做日志便可完成恢复过程。 
页（Page），区（Extent），段（Segment），表空间（Tablespace）
             表空间：由段组成，最大的逻辑单位，共享表空间、独享表空间
             段：由区组成，数据段、索引段、回滚段
             区：64个连续的页组成一个区，大小为1MB
             页：16KB，InnoDB管理的最小单位
（8）     InnoDB重做日志
      InnoDB的重做日志文件在设计上类似Oracle的联机重做日志文件。至少要有1个重做日志组，每个日志组至少要有2个成员，默认情况下为ib_logfile0和ib_logfile1。通过设置镜像日志组带来更高的可靠性。这里和Oracle不同的是，Oracle是组间切换，组内镜像。InnoDB则是组内切换，组间镜像。  
      参数innodb_log_file_size指定重做日志文件的大小。innodb_log_file_in_group决定组内日志文件数量。innodb_mirrored_log_groups指令镜像日志组的数量。
      InnoDB重做日志和MySQL二进制日志的区别:




InnoDB  Redo Log


Bin  Log




只记录InnoDB的事务


记录MySQL所有存储引擎，包括InnoDB




记录每个Page的物理更改


记录事务的具体操作，基于Statement或Row




事务过程中，不断生成redo，写入由innodb_flush_log_at_trx_commit决定


在发出commit之后，事务提交之前，写入二进制缓冲，刷入文件由sync_binlog决定




可设置多个镜像日志组，组内循环使用


写满了生成新的bin log文件




（9）     InnoDB事务
      事务的实现——redo和undo




redo


undo




用于恢复


用于回滚




保存在重做日志文件中


保存在共享表空间，回滚段中




redo  log在日志组内循环利用


undo页会由master thread回收




事务的控制语句：
       ·STARTTRANSACTION| BEGIN显式地开始一个事务，BEGIN不能用在存储过程中（会保留字冲突）。打开自动提交时，需要显式地开始事务，不然每句语句结束后自动提交。
       ·COMMIT和COMMITWORK基本上一致。COMMITWORK的效果受到参数completion_type控制，为0时与COMMIT一样。为1时，COMMITAND
 CHAIN即提交后马上开始下一个事务。为2时，COMMITANDRELEASE即提交后释放连接。
      针对COMMITWORK，进行如下的实验：    





--  事务开始
BEGIN;
--  插入数据
INSERT  INTO t1 VALUES(1,'a'),(2,'b');
--  提交，根据completion_type不同产生不同的效果
COMMIT  WORK;
--  插入第三条数据
INSERT  INTO t1 VALUES(3,'c');
--  回滚操作结束事务
ROLLBACK;
--  查看t1表的内容
SELECT  * FROM t1;





      completion_type=0
                           mysql&gt;SELECT *FROM t1;
                           +------+------+
|id   | name |
+------+------+
|   1 |a   |
|   2 |b   |
|   3 |c   |
+------+------+
3 rows inset (0.00 sec)
由于是自动提交，所以插入第三条数据后已经提交完成了事务，回滚没有达到预期的效果。
      completion_type=1
                           mysql&gt;SELECT *FROM t1;
+------+------+
|id   | name |
+------+------+
|   1 |a   |
|   2 |b   |
+------+------+
2 rows inset (0.00 sec)
此时，COMMIT WORK提交后立即开始下一个事务，因此插入数据后不会自动提交，回滚操作将插入的第三条语句回滚掉，只剩下两条数据。
completion_type=2
mysql&gt;INSERT INTOt1VALUES(3,'c');
ERROR2006 (HY000):MySQLserver has gone away
      COMMITWORK提交后释放连接，执行INSERT时提示连接失效。
    ·ROLLBACK和ROLLBACKWORK也是类似的区别。
·SAVEPOINT与ROLLBACKTO[SAVEPOINT]，返回保存点不会结束事务





session  a


session  b




mysql&gt;  BEGIN;


 




mysql&gt;  INSERT INTO t1
VALUES(1,'a');


 




mysql&gt;  SAVEPOINT p1;


 




 


mysql&gt;  SELECT * FROM t1;
Empty  set (0.00 sec)




mysql&gt;  INSERT INTO t1
VALUES(2,'B');


 




mysql&gt;  ROLLBACK TO p1;


 




 


mysql&gt; SELECT * FROM  t1;
Empty set (0.00 sec)




mysql&gt;  COMMIT;


 




 


mysql&gt;  SELECT * FROM t1;
+------+------+
|  id   | name |
+------+------+
|    1 | a     |
+------+------+
1  row in set (0.00 sec)





 
·SETTRANSACTION设置事务的隔离级别，即：
                    1、READUNCOMMITTED
                    2、READCOMMITTED
                    3、REPEATABLEREAD
                    4、SERIALIZABLE
             




ISOLATION LEVEL


Dirty Reads


Non-Repeatable Reads


Phantom Reads




READ UNCOMMITTED


√


√


√




READ COMMITTED


×


√


√




REPEATABLE READ


×


×


√




SERIALIZABLE


×


×


×




 InnoDB默认支持REPEATABLEREAD，但是由于InnoDB使用Next-keyLock（即除了对索引记录加锁，还对索引记录之前的区间加锁），可以避免幻读，所以InnoDB的REPEATABLEREAD隔离级别已经达到了SQL标准的SERIALIZABLE。


       (1) 查看当前库所有mysql用户：
       SELECT HOST,USER FROM mysql.user;
       (2)创建一个新用户，密码自己定：
       CREATE USER 'sjdb'@'localhost' IDENTIFIED BY 'password';
       (3)给新用户增加增删改查权限：
       GRANT SELECT,INSERT,UPDATE,DELETE ON *.* TO sjdb@"localhost" IDENTIFIED BY "password"
       (4)如果需要更多权限，作如下设置：
       GRANT ALL PRIVILEGES ON *.* TO 'sjdb'@'%' IDENTIFIED BY 'password' WITH GRANT OPTION;
       (5)提交设置使及时生效：
       FLUSH   PRIVILEGES;
       (6)查看新用户状态：
       SELECT USER FROM mysql.user WHERE USER='sjdb';
       (7)创建新数据库：
       CREATE DATABASE bd_jrdb;
       (8)给新建用户对赋予对新创建数据库的操作权限：
       GRANT ALL PRIVILEGES ON bd_jrdb.* TO sjdb@localhost IDENTIFIED BY 'password';
       (9)在新库中创建新的库表SMTCN_EXCH_SECU：
       CREATE TABLE `SMTCN_EXCH_SECU` (
  `ID` INT NOT NULL,
  `INFO_SOUR` VARCHAR(200) CHARACTER SET utf8 ,
  `PUB_DT` DATETIME,
  `SECU_ID` INT NOT NULL,
  `BGN_DT` DATETIME NOT NULL,
  `END_DT` DATETIME NOT NULL,
  `IS_VALID` VARCHAR(1) CHARACTER SET utf8  NOT NULL,
  `TYP_CODEII` INT NOT NULL,
  `ENT_TIME` DATETIME NOT NULL,
  `UPD_TIME` DATETIME NOT NULL,
  `GRD_TIME` DATETIME NOT NULL,
  `RS_ID` VARCHAR(20) CHARACTER SET utf8  NOT NULL,
  `REC_ID` VARCHAR(50) CHARACTER SET utf8 ,
  PRIMARY KEY (`ID`),
  INDEX `IDX_SMTCN_EXCH_SECU` (`SECU_ID`, `BGN_DT`),
  INDEX `IDX_SMTCN_EXCH_SECU_RID` (`REC_ID`),
  INDEX `IDX_SMTCN_EXCH_SECU_UPDTIME` (`UPD_TIME`)
) ENGINE=InnoDB;
       (10)给新表SMTCN_EXCH_SECU插入数据：
     INSERT INTO `SMTCN_EXCH_SECU` VALUES(637223500, NULL, NULL, 14725, '2010-03-31', '4000-12-31', '1', 1001, '2012-11-18 08:30:03', '2015-10-16 12:50:03', '2015-03-26 11:14:53', 'JY', '325373671907');
       (11)删除表SMTCN_EXCH_SECU中secu_id在12175到52178区间的数据：
     DELETE FROM  WHERE  secu_id  BETWEEN 12175 AND 52178;
       (12)在表SMTCN_EXCH_SECU选出secu_id, end_dt并按secu_id降序排序：
    SELECT secu_id, end_dt FROM SMTCN_EXCH_SECU ORDER BY secu_id DESC;
       (13)在表SMTCN_EXCH_SECU选出secu_id, end_dt并按secu_id升序排序：
     SELECT secu_id, end_dt FROM SMTCN_EXCH_SECU ORDER BY secu_id ASC;
       (14)统计bd_jrdb数据库中所有表的数量：
     SELECT count(*) TABLES, table_schema FROM information_schema.TABLES where table_schema = 'bd_jrdb' GROUP BY table_schema;
      (15)列出bd_jrdb数据库中所有表的名称：
      SELECT table_name FROM information_schema.tables WHERE table_schema='bd_jrdb';
      (16)取消jrdb用户对数据库的操作权限
       REVOKE ALL PRIVILEGES ON *.* FROM sjdb@localhost;
      (17)删除sjdb用户
        DELETE FROM mysql.user WHERE USER='sjdb' AND HOST='localhost';
      (18)查看bd_jrdb数据库字符集：
        SHOW VARIABLES LIKE 'character_set_%';
      (19) 查找表SMTCN_EXCH_SECU中开始时间和结束时间大于2015年，更新时间大于2016年的数据
       SELECT CONCAT(id, " ", secu_id) FROM  SMTCN_EXCH_SECU WHERE BEG_DT/END_DT &gt; 2015 AND  UPD_TIME&gt;2016;
      (20)锁定数据表，避免在备份过程中，表被更新
       LOCK TABLES READ SMTCN_EXCH_SECU;
      (21)导出备份数据：
      SELECT * INTO OUTFILE SMTCN_EXCH_SECU.bak’ FROM SMTCN_EXCH_SECU;
      (22)解锁表：
      UNLOCK TABLES;
      (23)统计一张表有多少条数据：
      SELECT COUNT(*) FROM  SMTCN_EXCH_SECU;
       
       

        在我们数据库的迁移和传输中，尤其是java开发的数据传输软件，经常会遇到传输失败的问题，一般错误如下提示信息：
       discription: 插入失败,error msg[插入失败,异常信息如下[id=460025409051,error msg[PreparedStatementCallback; SQL [INSERT INTO LC_InterimBulletin(InvolvedStock,InfoTitle,Category,Detail,Media,BulletinDate,JSID,RecordDate,Market,BulletinType,ID,XGRQ) VALUES(?,?,?,?,?,?,?,?,?,?,?,?)];
 Packet for query is too large (1365565 &gt; 1048576). You can change this value on the server by setting the max_allowed_packet’ variable.; nested exception is com.mysql.jdbc.PacketTooBigException: Packet for query is too large (1365565 &gt; 10485 ；
      Got a packet bigger than 'max_allowed_packet' bytes”或者“MySQL server has gone away”等错误，一般是由于当前导入的数据大于系统的限制的最大包大小。
        此时，我们就要想到通过修改'max_allowed_packet' 参数的大小就可以解决问题：
        max_allowed_packet这个值理论上最大可以设置1G，但是实际上mysql客户端最大只支持16M。
1、修改配置文件

可以编辑my.cnf来修改（windows下my.ini）,在[mysqld]段或者mysql的server配置段进行修改。
(1)查看当前系统默认值：
      SHOW VARIABLES LIKE '%max_allowed_packet%';
       
(2)按所需定值设置


max_allowed_packet = 10M
设置完以后重启mysql：
   
查看设置后的参数是否生效：
   SHOW VARIABLES LIKE '%max_allowed_packet%';
    
max_allowed_packet=*M  （自己视情况设置）即可。


如果找不到my.cnf可以通过


mysql --help | grep my.cnf
或者find / -name  my.cnf


去寻找my.cnf文件。
linux下该文件在/etc/下。
2、在mysql命令行中修改
在mysql 命令行中运行


set global max_allowed_packet = 2*1024*1024*10


然后退出命令行，重启mysql服务，再进入。


show VARIABLES like '%max_allowed_packet%';


查看下max_allowed_packet是否生效！


一、 MySQL字符集设置
（1）       系统变量：
– character_set_server：默认的内部操作字符集
– character_set_client：客户端来源数据使用的字符集
– character_set_connection：连接层字符集
– character_set_results：查询结果字符集
– character_set_database：当前选中数据库的默认字符集
– character_set_system：系统元数据(字段名等)字符集
– 还有以collation_开头的同上面对应的变量，用来描述字符序。
（2）       用introducer指定文本字符串的字符集：
– 格式为：[_charset]’string’ [COLLATE collation]
– 例如：
       SELECT _latin1 ’string’;
       SELECT _utf8 ‘你好’ COLLATE utf8_general_ci;
– 由introducer修饰的文本字符串在请求过程中不经过多余的转码，直接转换为内部字符集处理。
二、 MySQL中的字符集转换过程
（1）       MySQL Server收到请求时将请求数据从character_set_client转换为character_set_connection；
（2）       进行内部操作前将请求数据从character_set_connection转换为内部操作字符集，其确定方法如下：
       - 使用每个数据字段的CHARACTER SET设定值；
       - 若上述值不存在，则使用对应数据表的DEFAULT CHARACTER SET设定值(MySQL扩展，非SQL标准)；
       - 若上述值不存在，则使用对应数据库的DEFAULT CHARACTER SET设定值；
       - 若上述值不存在，则使用character_set_server设定值。
  
三、MySQL默认字符集
MySQL对于字符集的指定可以细化到一个数据库，一张表，一列.传统的程序在创建数据库和数据表时并没有使用那么复杂的配置，它们用的是默认的配置.
    (1)      编译MySQL时，指定了一个默认的字符集，这个字符集是 latin1；
    (2)      安装MySQL时，可以在配置文件 (my.ini)中指定一个默认的的字符集，如果没指定，这个值继承自编译时指定的；
    (3)      启动mysqld时，可以在命令行参数中指定一个默认的的字符集，如果没指定，这个值继承自配置文件中的配置,此时
 character_set_server 被设定为这个默认的字符集；
    (4)      安装MySQL选择多语言支持，安装程序会自动在配置文件中把default_character_set设置为
 UTF-8，保证缺省情况下所有的数据库所有表的所有列的都用 UTF-8存储。


四、修改默认字符集
(1)        最简单的修改方法，就是修改mysql的my.cnf（这里注意：windows 是my.ini）文件中的字符集键值，


     修改完后，重启mysql的服务
(2)        还有一种修改字符集的方法，就是使用mysql的命令

    设置了表的默认字符集为utf8并且通过UTF-8编码发送查询，存入数据库的仍然是乱码。那connection连接层上可能出了问题。解决方法是在发送查询前执行一下下面这句：
 SET NAMES 'utf8';它相当于下面的三句指令：
SET character_set_client = utf8;
SET character_set_results = utf8;
SET character_set_connection = utf8;
 
五、使用MySQL字符集时的建议
    
（1）          建立数据库/表和进行数据库操作时尽量显式指出使用的字符集，而不是依赖于MySQL的默认设置，否则MySQL升级时可能带来很大困扰；
   数据库和连接字符集都使用latin1时，虽然大部分情况下都可以解决乱码问题，但缺点是无法以字符为单位来进行SQL操作，一般情况下将数据库和连接字符集都置为utf8是较好的选择；
（2）          使用mysql CAPI（mysql提供C语言操作的API）时，初始化数据库句柄后马上用mysql_options设定MYSQL_SET_CHARSET_NAME属性为utf8，这样就不用显式地用SET
 NAMES语句指定连接字符集，且用mysql_ping重连断开的长连接时也会把连接字符集重置为utf8；
（3）          对于mysql PHP API，一般页面级的PHP程序总运行时间较短，在连接到数据库以后显式用SET
 NAMES语句设置一次连接字符集即可；但当使用长连接时，请注意保持连接通畅并在断开重连后用SET NAMES语句显式重置连接字符集。
         
六、其他注意事项
                 my.cnf中的default_character_set设置只影响mysql命令连接服务器时的连接字符集，不会对使用libmysqlclient库的应用程序产生任何作用！
    对字段进行的SQL函数操作通常都是以内部操作字符集进行的，不受连接字符集设置的影响。

mysql在服务器重启后，无法正常启动解决办法：
1.第一个是立即关机 使用命令 shutdown -h now 关机，关机后在硬启动，进程就停止了。
2.mv /var/lib/mysql/mysql.sock /var/lib/mysql/mysql.sock.bak改名备份后在执行：# service mysqld restart
mv /var/lib/mysql/mysql.sock /var/lib/mysql/mysql.sock.bak

测试数据(一)
/* 表结构 */
DROP TABLE IF EXISTS `bas_info`;
CREATE TABLE IF NOT EXISTS `bas_info`(
  `id` INT(1) NOT NULL AUTO_INCREMENT,
  `chi_chi_name` VARCHAR(20) NOT NULL,
  `trd_code` VARCHAR(20) NOT NULL,
  PRIMARY KEY(`id`)
)Engine=InnoDB;
/* 插入测试数据 */
INSERT INTO `bas_info`(`id`,`chi_chi_name`,`trd_code`) VALUES
('1001','MSCI中华除B股+HSBC(红利总指)',"MSG033"),
('1002','MSCI中华除B股外(红利总指)',"MSG032"),
('1001','MSCI中华+HSBC(红利总指)',"MSG030"),
('1004','MSCI中华(红利总指)',"MSG031"),
('1005','MSCI中国民企股(红利总指)',"MSG031"),;


SELECT * FROM `bas_info`;
/* 查找id最小的重复数据(只查找id字段) */
SELECT DISTINCT MIN(`id`) AS `id`
FROM `bas_info`
GROUP BY `id`,`chi_chi_name`,`trd_code`
HAVING COUNT(1) &gt; 1;
/* 查找所有重复数据 */
SELECT `bas_info`.*
FROM `bas_info`,(
  SELECT `id`,`chi_chi_name`,`trd_code`
  FROM `bas_info`
  GROUP BY `id`,`chi_chi_name`,`trd_code`
  HAVING COUNT(1) &gt; 1
) AS `bas_info_test`
WHERE `bas_info`.`chi_name` = `bas_info_test`.`chi_name`
  AND `bas_info`.`trd_code` = `bas_info_test`.`trd_code`;
/* 查找除id最小的数据外的重复数据 */
SELECT `bas_info`.*
FROM `bas_info`,(
  SELECT DISTINCT MIN(`id`) AS `id`,`chi_name`,`trd_code`
  FROM `bas_info`
  GROUP BY `chi_name`,`trd_code`
  HAVING COUNT(1) &gt; 1001
) AS `bas_info_test`
WHERE `bas_info`.`chi_name` = `bas_info_test`.`chi_name`
  AND `bas_info`.`trd_code` = `bas_info_test`.`trd_code`
  AND `bas_info`.`id` &lt;&gt; `bas_info_test`.`id`;  
  例2，表中没有主键(可唯一标识的字段)，或者主键并非数字类型(也可以删除重复数据，但效率上肯定比较慢)
测试数据（二）
/* 表结构 */
DROP TABLE IF EXISTS `base_code`;
CREATE TABLE IF NOT EXISTS `base_code`(
  `id` VARCHAR(20) NOT NULL COMMENT '字符串主键',
  `chi_name` VARCHAR(20) NOT NULL,
  `trd_code` VARCHAR(20) NOT NULL,
  PRIMARY KEY(`id`)
)Engine=InnoDB;


/* 测试数据，与上例一样的测试数据，只是主键变为字符串形式 */
INSERT INTO `base_code`(`id`,`chi_name`,`trd_code`) VALUES
('61001','中证香港',"L01141"),
('61002','中证香港',"L01141"),
('61003','中证香港',"L01141"), 
/* 为表添加自增长的id字段 */
ALTER TABLE `base_code` trd_code `id` INT(1) NOT NULL AUTO_INCREMENT, trd_code INDEX `id`(`id`);
Query OK, 23 rows affected (0.16 sec)
Records: 23  Duplicates: 0  Warnings: 0
MySQL中必须是有索引的字段才可以使用AUTO_INCREMENT


删除重复数据与上例一样，记得删除完数据把id字段也删除了
/* 删除重复数据，只保留一条数据 */
DELETE FROM `base_code`
USING `base_code`,(
  SELECT DISTINCT MIN(`id`) AS `id`,`chi_name`,`trd_code`
  FROM `base_code`
  GROUP BY `chi_name`,`trd_code`
  HAVING COUNT(1) &gt;61001
) AS `bas_info_test`
WHERE `base_code`.`chi_name` = `bas_info_test`.`chi_name`
  AND `base_code`.`trd_code` = `bas_info_test`.`trd_code`
  AND `base_code`.`id` &lt;&gt; `bas_info_test`.`id`;
Query OK, 2 rows affected (0.05 sec)


/* 删除id字段 */
ALTER TABLE `base_code` DROP `id`;
Query OK, 3 rows affected (0.16 sec)
Records: 3  Duplicates: 0  Warnings: 0


   Mysql导入大量数据时，会报如下错误，主键重复，不能再继续执行。
   Query:
INSERT INTO `FNDCN_MNG` VALUES(787390578, 1, '杨明', 95585, '2015-06-08', '招募说明书', 80100, NULL, '1', NULL, '3', '2015-06-29', NULL, '1', '    杨明先生，中央财经大学硕士研究生，14年银行、基金从业经历。曾在上海银行从事信贷员、交易员及风险管理工作。2004年10月加入华安基金管理有限公司，任研究发展部研究员。2013年6月起担任华安策略优选股票型基金的基金经理。2014年6月起担任投资研究部高级总监。',
 '2015-11-09 12:33:58', '2016-01-06 21:34:41', '2015-12-31 12:14:18', 'JY', '488969752640', NULL, NULL, NULL, NULL)



Error occured at:2016-02-16 15:23:41
Line no.:220
Error Code: 1062 - Duplicate entry '95585-80100-2015-06-29 00:00:00-1--1' for key 'IDX_FNDCN_MNG'
解决方案一：
   将主键设置为自动增长。可以在数据库手动将相关表设置ID为自增长。
   如图：
   

   再继续导入正常。
解决方案二：
  问题描述：单独导入出错sql,会报如下错误：

1 queries executed, 0 success, 1 errors, 0 warnings

查询：INSERT INTO `FNDCN_MNG` VALUES(787192513, 1, '丁进', 150150, '2015-12-28', '招募说明书', 80100, NULL, '1', NULL, '3', '20...

错误代码： 1062
Duplicate entry '787192513' for key 'PRIMARY'
   检查了下，主键生成策略是：@GeneratedValue(strategy=GenerationType.IDENTITY)，也没问题。

   删除导入出错的表；
   新建需要导入数据的表；

我的问题解决是在数据库中：在数据库中，没有将主键设为自动增长。将已经存在的表的主键设为自动增长SQL语句为：
alter table course change course_id   course_id   int(10)   not null   auto_increment ; 

   然后用sql脚本的方式导入所需数据，这样也可以很容易定位错误。
   







一. 什么是Spark？


Spark是UC Berkeley AMP lab所开源的类Hadoop MapReduce的通用的并行计算框架，Spark基于map reduce算法实现的分布式计算，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是Job中间输出和结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的map reduce的算法。其架构如下图所示：



二.Spark与Hadoop的对比

Spark的中间数据放到内存中，对于迭代运算效率更高。

Spark更适合于迭代运算比较多的ML和DM运算。因为在Spark里面，有RDD的抽象概念。
Spark比Hadoop更通用。

Spark提供的数据集操作类型有很多种，不像Hadoop只提供了Map和Reduce两种操作。比如map, filter, flatMap, sample, groupByKey, reduceByKey, union, join, cogroup, mapValues, sort,partionBy等多种操作类型，Spark把这些操作称为Transformations。同时还提供Count,
 collect, reduce, lookup, save等多种actions操作。这些多种多样的数据集操作类型，给给开发上层应用的用户提供了方便。各个处理节点之间的通信模型不再像Hadoop那样就是唯一的Data Shuffle一种模式。用户可以命名，物化，控制中间结果的存储、分区等。可以说编程模型比Hadoop更灵活。不过由于RDD的特性，Spark不适用那种异步细粒度更新状态的应用，例如web服务的存储或者是增量的web爬虫和索引。就是对于那种增量修改的应用模型不适合。
容错性。

在分布式数据集计算时通过checkpoint来实现容错，而checkpoint有两种方式，一个是checkpoint data，一个是logging the updates。用户可以控制采用哪种方式来实现容错。
可用性。

Spark通过提供丰富的Scala, Java，Python API及交互式Shell来提高可用性。


三.Spark与Hadoop的结合

Spark可以直接对HDFS进行数据的读写，同样支持Spark on YARN。Spark可以与MapReduce运行于同集群中，共享存储资源与计算，数据仓库Shark实现上借用Hive，几乎与Hive完全兼容。

四.Spark的适用场景

Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小由于RDD的特性，Spark不适用那种异步细粒度更新状态的应用，例如web服务的存储或者是增量的web爬虫和索引。就是对于那种增量修改的应用模型不适合。总的来说Spark的适用面比较广泛且比较通用。

五.运行模式

本地模式Standalone模式Mesoes模式yarn模式

六.Spark生态系统

Shark ( Hive on Spark): Shark基本上就是在Spark的框架基础上提供和Hive一样的H iveQL命令接口，为了最大程度的保持和Hive的兼容性，Shark使用了Hive的API来实现query Parsing和 Logic Plan generation，最后的PhysicalPlan execution阶段用Spark代替Hadoop MapReduce。通过配置Shark参数，Shark可以自动在内存中缓存特定的RDD，实现数据重用，进而加快特定数据集的检索。同时，Shark通过UDF用户自定义函数实现特定的数据分析学习算法，使得SQL数据查询和运算分析能结合在一起，最大化RDD的重复使用。Spark streaming: 构建在Spark上处理Stream数据的框架，基本的原理是将Stream数据分成小的时间片断（几秒），以类似batch批量处理的方式来处理这小部分数据。Spark Streaming构建在Spark上，一方面是因为Spark的低延迟执行引擎（100ms+）可以用于实时计算，另一方面相比基于Record的其它处理框架（如Storm），RDD数据集更容易做高效的容错处理。此外小批量处理的方式使得它可以同时兼容批量和实时数据处理的逻辑和算法。方便了一些需要历史数据和实时数据联合分析的特定应用场合。Bagel: Pregel on Spark，可以用Spark进行图计算，这是个非常有用的小项目。Bagel自带了一个例子，实现了Google的PageRank算法。

七.在业界的使用

Spark项目在2009年启动，2010年开源, 现在使用的有：Berkeley, Princeton, Klout, Foursquare, Conviva, Quantifind, Yahoo! Research &amp; others, 淘宝等，豆瓣也在使用Spark的python克隆版Dpark。

八.Spark核心概念

Resilient Distributed Dataset (RDD)弹性分布数据集

RDD是Spark的最基本抽象,是对分布式内存的抽象使用，实现了以操作本地集合的方式来操作分布式数据集的抽象实现。RDD是Spark最核心的东西，它表示已被分区，不可变的并能够被并行操作的数据集合，不同的数据集格式对应不同的RDD实现。RDD必须是可序列化的。RDD可以cache到内存中，每次对RDD数据集的操作之后的结果，都可以存放到内存中，下一个操作可以直接从内存中输入，省去了MapReduce大量的磁盘IO操作。这对于迭代运算比较常见的机器学习算法,
 交互式数据挖掘来说，效率提升比较大。RDD的特点：

它是在集群节点上的不可变的、已分区的集合对象。通过并行转换的方式来创建如（map, filter, join, etc）。失败自动重建。可以控制存储级别（内存、磁盘等）来进行重用。必须是可序列化的。是静态类型的。
RDD的好处

RDD只能从持久存储或通过Transformations操作产生，相比于分布式共享内存（DSM）可以更高效实现容错，对于丢失部分数据分区只需根据它的lineage就可重新计算出来，而不需要做特定的Checkpoint。RDD的不变性，可以实现类Hadoop MapReduce的推测式执行。RDD的数据分区特性，可以通过数据的本地性来提高性能，这与Hadoop MapReduce是一样的。RDD都是可序列化的，在内存不足时可自动降级为磁盘存储，把RDD存储于磁盘上，这时性能会有大的下降但不会差于现在的MapReduce。
RDD的存储与分区

用户可以选择不同的存储级别存储RDD以便重用。当前RDD默认是存储于内存，但当内存不足时，RDD会spill到disk。RDD在需要进行分区把数据分布于集群中时会根据每条记录Key进行分区（如Hash 分区），以此保证两个数据集在Join时能高效。
RDD的内部表示
在RDD的内部实现中每个RDD都可以使用5个方面的特性来表示：



分区列表（数据块列表）计算每个分片的函数（根据父RDD计算出此RDD）对父RDD的依赖列表对key-value RDD的Partitioner【可选】每个数据分片的预定义地址列表(如HDFS上的数据块的地址)【可选】
RDD的存储级别
RDD根据useDisk、useMemory、deserialized、replication四个参数的组合提供了11种存储级别：







1

2

3

4

5

6

7

8

9

10

11




val
 NONE = new StorageLevel(false, false, false) 

    val
 DISK_ONLY = new StorageLevel(true, false, false) 

    val
 DISK_ONLY_2 = new StorageLevel(true, false, false, 2) 

    val
 MEMORY_ONLY = new StorageLevel(false, true, true) 

    val
 MEMORY_ONLY_2 = new StorageLevel(false, true, true, 2) 

    val
 MEMORY_ONLY_SER = new StorageLevel(false, true, false) 

    val
 MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, 2) 

    val
 MEMORY_AND_DISK = new StorageLevel(true, true, true) 

    val
 MEMORY_AND_DISK_2 = new StorageLevel(true, true, true, 2) 

    val
 MEMORY_AND_DISK_SER = new StorageLevel(true, true, false) 

    val
 MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, 2)








RDD定义了各种操作，不同类型的数据由不同的RDD类抽象表示，不同的操作也由RDD进行抽实现。

九.RDD的生成


RDD有两种创建方式：
1、从Hadoop文件系统（或与Hadoop兼容的其它存储系统）输入（例如HDFS）创建。
2、从父RDD转换得到新RDD。下面来看一从Hadoop文件系统生成RDD的方式，如：val file = spark.textFile("hdfs://...")，file变量就是RDD（实际是HadoopRDD实例），生成的它的核心代码如下：








1

2

3

4

5

6

7

8




//
 SparkContext根据文件/目录及可选的分片数创建RDD, 这里我们可以看到Spark与Hadoop MapReduce很像 

   //
 需要InputFormat, Key、Value的类型，其实Spark使用的Hadoop的InputFormat, Writable类型。 

   def
 textFile(path: String, minSplits: Int = defaultMinSplits): RDD[String] = { 


       hadoopFile(path,
 classOf[TextInputFormat], classOf[LongWritable], 

       classOf[Text],
 minSplits) .map(pair =&gt; pair._2.toString) }

 

   //
 根据Hadoop配置，及InputFormat等创建HadoopRDD  

   new
 HadoopRDD(this, conf, inputFormatClass, keyClass, valueClass, minSplits)








对RDD进行计算时，RDD从HDFS读取数据时与Hadoop MapReduce几乎一样的：







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16




//
 根据hadoop配置和分片从InputFormat中获取RecordReader进行数据的读取。 

    reader
 = fmt.getRecordReader(split.inputSplit.value, conf, Reporter.NULL)

 

    val
 key: K = reader.createKey()

    val
 value: V = reader.createValue()

 

    //使用Hadoop
 MapReduce的RecordReader读取数据，每个Key、Value对以元组返回。

    override
 def getNext() = {

    try
 {

      finished
 = !reader.next(key, value)

    }
 catch {

      case
 eof: EOFException =&gt;

        finished
 = true

    }

      (key,
 value)

    }








十.RDD的转换与操作

对于RDD可以有两种计算方式：转换（返回值还是一个RDD）与操作（返回值不是一个RDD）。转换(Transformations) (如：map, filter, groupBy, join等)，Transformations操作是Lazy的，也就是说从一个RDD转换生成另一个RDD的操作不是马上执行，Spark在遇到Transformations操作时只会记录需要这样的操作，并不会去执行，需要等到有Actions操作的时候才会真正启动计算过程进行计算。操作(Actions) (如：count, collect, save等)，Actions操作会返回结果或把RDD数据写到存储系统中。Actions是触发Spark启动计算的动因。下面使用一个例子来示例说明Transformations与Actions在Spark的使用。







1

2

3

4

5

6

7

8

9

10

11

12

13




val
 sc = new SparkContext(master, "Example", System.getenv("SPARK_HOME"), 

        Seq(System.getenv("SPARK_TEST_JAR")))

 

    val
 rdd_A = sc.textFile(hdfs://.....)

    val
 rdd_B = rdd_A.flatMap((line =&gt; line.split("\\s+"))).map(word =&gt; (word, 1))

 

    val
 rdd_C = sc.textFile(hdfs://.....)

    val
 rdd_D = rdd_C.map(line =&gt; (line.substring(10), 1))

    val
 rdd_E = rdd_D.reduceByKey((a, b) =&gt; a + b)

 

    val
 rdd_F = rdd_B.jion(rdd_E)

 

    rdd_F.saveAsSequenceFile(hdfs://....)











十一.Lineage（血统）

利用内存加快数据加载,在众多的其它的In-Memory类数据库或Cache类系统中也有实现，Spark的主要区别在于它处理分布式运算环境下的数据容错性（节点实效/数据丢失）问题时采用的方案。为了保证RDD中数据的鲁棒性，RDD数据集通过所谓的血统关系(Lineage)记住了它是如何从其它RDD中演变过来的。相比其它系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据转换（Transformation）操作（filter,
 map, join etc.)行为。当这个RDD的部分分区数据丢失时，它可以通过Lineage获取足够的信息来重新运算和恢复丢失的数据分区。这种粗颗粒的数据模型，限制了Spark的运用场合，但同时相比细颗粒度的数据模型，也带来了性能的提升。RDD在Lineage依赖方面分为两种Narrow Dependencies与Wide Dependencies用来解决数据容错的高效性。Narrow Dependencies是指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区或多个父RDD的分区对应于一个子RDD的分区，也就是说一个父RDD的一个分区不可能对应一个子RDD的多个分区。Wide
 Dependencies是指子RDD的分区依赖于父RDD的多个分区或所有分区，也就是说存在一个父RDD的一个分区对应一个子RDD的多个分区。对与Wide Dependencies，这种计算的输入和输出在不同的节点上，lineage方法对与输入节点完好，而输出节点宕机时，通过重新计算，这种情况下，这种方法容错是有效的，否则无效，因为无法重试，需要向上其祖先追溯看是否可以重试（这就是lineage，血统的意思），Narrow Dependencies对于数据的重算开销要远小于Wide Dependencies的数据重算开销。

十二.容错

在RDD计算，通过checkpint进行容错，做checkpoint有两种方式，一个是checkpoint data，一个是logging the updates。用户可以控制采用哪种方式来实现容错，默认是logging the updates方式，通过记录跟踪所有生成RDD的转换（transformations）也就是记录每个RDD的lineage（血统）来重新计算生成丢失的分区数据。

十三.资源管理与作业调度

Spark对于资源管理与作业调度可以使用Standalone(独立模式)，Apache Mesos及Hadoop YARN来实现。 Spark on Yarn在Spark0.6时引用，但真正可用是在现在的branch-0.8版本。Spark on Yarn遵循YARN的官方规范实现，得益于Spark天生支持多种Scheduler和Executor的良好设计，对YARN的支持也就非常容易，Spark
 on Yarn的大致框架图。 让Spark运行于YARN上与Hadoop共用集群资源可以提高资源利用率。

十四.编程接口

Spark通过与编程语言集成的方式暴露RDD的操作，类似于DryadLINQ和FlumeJava，每个数据集都表示为RDD对象，对数据集的操作就表示成对RDD对象的操作。Spark主要的编程语言是Scala，选择Scala是因为它的简洁性（Scala可以很方便在交互式下使用）和性能（JVM上的静态强类型语言）。Spark和Hadoop MapReduce类似，由Master(类似于MapReduce的Jobtracker)和Workers(Spark的Slave工作节点)组成。用户编写的Spark程序被称为Driver程序，Dirver程序会连接master并定义了对各RDD的转换与操作，而对RDD的转换与操作通过Scala闭包(字面量函数)来表示，Scala使用Java对象来表示闭包且都是可序列化的，以此把对RDD的闭包操作发送到各Workers节点。
 Workers存储着数据分块和享有集群内存，是运行在工作节点上的守护进程，当它收到对RDD的操作时，根据数据分片信息进行本地化数据操作，生成新的数据分片、返回结果或把RDD写入存储系统。



十五.Scala


Spark使用Scala开发，默认使用Scala作为编程语言。编写Spark程序比编写Hadoop MapReduce程序要简单的多，SparK提供了Spark-Shell，可以在Spark-Shell测试程序。写SparK程序的一般步骤就是创建或使用(SparkContext)实例，使用SparkContext创建RDD，然后就是对RDD进行操作。如：











1

2

3




valsc
=new

SparkContext(master, appName, [sparkHome], [jars]) 

valtextFile
=sc.textFile("hdfs://.....")

textFile.map(....).filter(.....).....








十六.Java


Spark支持Java编程，但对于使用Java就没有了Spark-Shell这样方便的工具，其它与Scala编程是一样的，因为都是JVM上的语言，Scala与Java可以互操作，Java编程接口其实就是对Scala的封装。如：











1

2

3

4

5

6

7

8

9




JavaSparkContext
 sc = newJavaSparkContext(...); 


JavaRDD
 lines = ctx.textFile("hdfs://...");

JavaRDD
 words = lines.flatMap( 

  newFlatMapFunction&lt;String,
 String&gt;() { 

     publicIterable
 call(String s) { 

        returnArrays.asList(s.split("
 "));

     }

   }

);








十七.Python
      现在Spark也提供了Python编程接口，Spark使用py4j来实现python与java的互操作，从而实现使用python编写Spark程序。Spark也同样提供了pyspark，一个Spark的python
 shell，可以以交互式的方式使用Python编写Spark程序。 如：














1

2

3

4




frompyspark
importSparkContext


sc=SparkContext("local","Job
 Name",
 pyFiles=['MyFile.py','lib.zip','app.egg'])

words=sc.textFile("/usr/share/dict/words")

words.filter(lambdaw:
 w.startswith("spar")).take(5)








使用示例

十八.Standalone模式

为方便Spark的推广使用，Spark提供了Standalone模式，Spark一开始就设计运行于Apache Mesos资源管理框架上，这是非常好的设计，但是却带了部署测试的复杂性。为了让Spark能更方便的部署和尝试，Spark因此提供了Standalone运行模式，它由一个Spark Master和多个Spark worker组成，与Hadoop MapReduce1很相似，就连集群启动方式都几乎是一样。以Standalone模式运行Spark集群


下载Scala2.9.3，并配置SCALA_HOME下载Spark代码（可以使用源码编译也可以下载编译好的版本）这里下载 编译好的版本（http://spark-project.org/download/spark-0.7.3-prebuilt-cdh4.tgz）解压spark-0.7.3-prebuilt-cdh4.tgz安装包修改配置（conf/*） slaves: 配置工作节点的主机名 spark-env.sh：配置环境变量。








1

2

3

4

5

6

7

8

9




SCALA_HOME=/home/spark/scala-2.9.3

JAVA_HOME=/home/spark/jdk1.6.0_45

SPARK_MASTER_IP=spark1            

SPARK_MASTER_PORT=30111

SPARK_MASTER_WEBUI_PORT=30118

SPARK_WORKER_CORES=2
 SPARK_WORKER_MEMORY=4g 

SPARK_WORKER_PORT=30333

SPARK_WORKER_WEBUI_PORT=30119

SPARK_WORKER_INSTANCES=1







把Hadoop配置copy到conf目录下在master主机上对其它机器做ssh无密码登录把配置好的Spark程序使用scp copy到其它机器在master启动集群







1




$SPARK_HOME/start-all.sh








十九.yarn模式

Spark-shell现在还不支持Yarn模式，使用Yarn模式运行，需要把Spark程序全部打包成一个jar包提交到Yarn上运行。目录只有branch-0.8版本才真正支持Yarn。以Yarn模式运行Spark

下载Spark代码.







1




git
 clone git://github.com/mesos/spark







切换到branch-0.8







1

2




cd
 spark 

git
 checkout -b yarn --track origin/yarn







使用sbt编译Spark并







1

2

3




$SPARK_HOME/sbt/sbt

&gt;
 package 

&gt;
 assembly







把Hadoop yarn配置copy到conf目录下运行测试







1

2

3




  SPARK_JAR=./core/target/scala-2.9.3/spark-core-assembly-0.8.0-SNAPSHOT.jar
 \ 

./run
 spark.deploy.yarn.Client --jar examples/target/scala-2.9.3/ \ 

--class
 spark.examples.SparkPi --args yarn-standalone










二十.使用Spark-shell

Spark-shell使用很简单，当Spark以Standalon模式运行后，使用$SPARK_HOME/spark-shell进入shell即可，在Spark-shell中SparkContext已经创建好了，实例名为sc可以直接使用，还有一个需要注意的是，在Standalone模式下，Spark默认使用的调度器的FIFO调度器而不是公平调度，而Spark-shell作为一个Spark程序一直运行在Spark上，其它的Spark程序就只能排队等待，也就是说同一时间只能有一个Spark-shell在运行。在Spark-shell上写程序非常简单，就像在Scala Shell上写程序一样。







1

2

3

4

5

6

7

8




scala&gt;
 val textFile = sc.textFile("hdfs://hadoop1:2323/user/data") 

textFile:
 spark.RDD[String] = spark.MappedRDD@2ee9b6e3

 

scala&gt;
 textFile.count() // Number of items in this RDD

res0:
 Long = 21374

 

scala&gt;
 textFile.first() // First item in this RDD

res1:
 String = # Spark









二十一.编写Driver程序

在Spark中Spark程序称为Driver程序，编写Driver程序很简单几乎与在Spark-shell上写程序是一样的，不同的地方就是SparkContext需要自己创建。如WorkCount程序如下：







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25




import
 spark.SparkContext

import
 SparkContext._

 

object
 WordCount {

  def
 main(args: Array[String]) {

    if
 (args.length ==0 ){

      println("usage
 is org.test.WordCount ")

    }

    println("the
 args: ")

    args.foreach(println)

 

    val
 hdfsPath = "hdfs://hadoop1:8020"

 

    //
 create the SparkContext， args(0)由yarn传入appMaster地址

    val
 sc = new SparkContext(args(0), "WrodCount",

    System.getenv("SPARK_HOME"),
 Seq(System.getenv("SPARK_TEST_JAR")))

 

    val
 textFile = sc.textFile(hdfsPath + args(1))

 

    val
 result = textFile.flatMap(line =&gt; line.split("\\s+"))

        .map(word
 =&gt; (word, 1)).reduceByKey(_ + _)

 

    result.saveAsTextFile(hdfsPath
 + args(2))

  }

}










       Hadoop的高吞吐，海量数据处理的能力使得人们可以方便地处理海量数据。但是，Hadoop的缺点也和它的优点同样鲜明——延迟大，响应缓慢，运维复杂。

      有需求也就有创造，在Hadoop基本奠定了大数据霸主地位的时候，很多的开源项目都是以弥补Hadoop的实时性为目标而被创造出来。而在这个节骨眼上Storm横空出世了。

      Storm带着流式计算的标签华丽丽滴出场了，看看它的一些卖点：

分布式系统：可横向拓展,现在的项目不带个分布式特性都不好意思开源。运维简单：Storm的部署的确简单。虽然没有Mongodb的解压即用那么简单，但是它也就是多安装两个依赖库而已。高度容错：模块都是无状态的，随时宕机重启。无数据丢失：Storm创新性提出的ack消息追踪框架和复杂的事务性处理,能够满足很多级别的数据处理需求。不过，越高的数据处理需求，性能下降越严重。多语言：实际上，Storm的多语言更像是临时添加上去似的。因为，你的提交部分还是要使用Java实现。

一.Storm简介

    Storm是一个免费开源、分布式、高容错的实时计算系统。Storm令持续不断的流计算变得容易，弥补了Hadoop批处理所不能满足的实时要求。Storm经常用于在实时分析、在线机器学习、持续计算、分布式远程调用和ETL等领域。Storm的部署管理非常简单，而且，在同类的流式计算工具，Storm的性能也是非常出众的。

    Storm主要分为两种组件Nimbus和Supervisor。这两种组件都是快速失败的，没有状态。任务状态和心跳信息等都保存在Zookeeper上的，提交的代码资源都在本地机器的硬盘上。

Nimbus负责在集群里面发送代码，分配工作给机器，并且监控状态。全局只有一个。Supervisor会监听分配给它那台机器的工作，根据需要启动/关闭工作进程Worker。每一个要运行Storm的机器上都要部署一个，并且，按照机器的配置设定上面分配的槽位数。Zookeeper是Storm重点依赖的外部资源。Nimbus和Supervisor甚至实际运行的Worker都是把心跳保存在Zookeeper上的。Nimbus也是根据Zookeerper上的心跳和任务运行状况，进行调度和任务分配的。Storm提交运行的程序称为Topology。Topology处理的最小的消息单位是一个Tuple，也就是一个任意对象的数组。Topology由Spout和Bolt构成。Spout是发出Tuple的结点。Bolt可以随意订阅某个Spout或者Bolt发出的Tuple。Spout和Bolt都统称为component。

下图是一个Topology设计的逻辑图的例子。




     下图是Topology的提交流程图。




      下图是Storm的数据交互图。可以看出两个模块Nimbus和Supervisor之间没有直接交互。状态都是保存在Zookeeper上。Worker之间通过ZeroMQ传送数据。




       虽然，有些地方做得还是不太好，例如，底层使用的ZeroMQ不能控制内存使用(下个release版本，引入了新的消息机制使用netty代替ZeroMQ），多语言支持更多是噱头，Nimbus还不支持HA。但是，就像当年的Hadoop那样，很多公司选择它是因为它是唯一的选择。而这些先期使用者，反过来促进了Storm的发展。

二.Storm发展


Storm已经发展到0.10.0版本了，看一下两年多来，它取得的成就：

有50个大大小小的公司在使用Storm，相信更多的不留名的公司也在使用。这些公司中不乏淘宝，百度，Twitter，Groupon，雅虎等重量级公司。从开源时候的0.5.0版本，到现在的0.10.0，和即将到来的0.10.0+。先后添加了以下重大的新特性：

使用kryo作为Tuple序列化的框架（0.6.0）添加了Transactional topologies（事务性拓扑）的支持（0.7.0）添加了Trident的支持（0.8.0）引入netty作为底层消息机制（0.9.0）



Transactional topologies和Trident都是针对实际应用中遇到的重复计数问题和应用性问题的解决方案。可以看出，实际的商用给予了Storm很多良好的反馈。


在GitHub上超过6000个项目负责人。Storm集成了许多库，支持包括Kestrel、Kafka、JMS、Cassandra、Memcached以及更多系统。随着支持的库越来越多，Storm更容易与现有的系统协作。Storm的拥有一个活跃的社区和一群热心的贡献者。过去两年，Storm的发展是成功的。
三.Storm发展



      Storm被广泛应用于实时分析，在线机器学习，持续计算、分布式远程调用等领域。来看一些实际的应用:

一淘-实时分析系统pora：实时分析用户的属性，并反馈给搜索引擎。最初，用户属性分析是通过每天在云梯上定时运行的MR
 job来完成的。为了满足实时性的要求，希望能够实时分析用户的行为日志，将最新的用户属性反馈给搜索引擎，能够为用户展现最贴近其当前需求的结果。携程-网站性能监控：实时分析系统监控携程网的网站性能。利用HTML5提供的performance标准获得可用的指标，并记录日志。Storm集群实时分析日志和入库。使用DRPC聚合成报表，通过历史数据对比等判断规则，触发预警事件。


如果，业务场景中需要低延迟的响应，希望在秒级或者毫秒级完成分析、并得到响应，而且希望能够随着数据量的增大而拓展。那就可以考虑下，使用Storm了。

试想下，如果，一个游戏新版本上线，有一个实时分析系统，收集游戏中的数据，运营或者开发者可以在上线后几秒钟得到持续不断更新的游戏监控报告和分析结果，然后马上针对游戏的参数和平衡性进行调整。这样就能够大大缩短游戏迭代周期，加强游戏的生命力（实际上，zynga就是这么干的！虽然使用的不是Storm……Zynga研发之道探秘：用数据说话）。除了低延迟，Storm的Topology灵活的编程方式和分布式协调也会给我们带来方便。用户属性分析的项目，需要处理大量的数据。使用传统的MapReduce处理是个不错的选择。但是，处理过程中有个步骤需要根据分析结果，采集网页上的数据进行下一步的处理。这对于MapReduce来说就不太适用了。但是，Storm的Topology就能完美解决这个问题。基于这个问题，我们可以画出这样一个Storm的Topology的处理图。








        我们只需要实现每个分析的过程，而Storm帮我们把消息的传送和接受都完成了。更加激动人心的是，你只需要增加某个Bolt的并行度就能够解决掉某个结点上的性能瓶颈。

四.Storm的未来

      在流式处理领域里，Storm的直接对手是S4。不过，S4冷淡的社区、半成品的代码，在实际商用方面输给Storm不止一条街。

如果把范围扩大到实时处理，Storm就一点都不寂寞了。

Puma：Facebook使用puma和Hbase相结合来处理实时数据,使批处理 计算平台具备一定实时能力。 不过这不算是一个开源的产品。只是内部使用。HStreaming：尝试为Hadoop环境添加一个实时的组件HStreaming能让一个Hadoop平台在几天内转为一个实时系统。分商业版和免费版。也许HStreaming可以借Hadoop的东风，撼动Storm。Spark Streaming：作为UC Berkeley云计算software stack的一部分，Spark Streaming是建立在Spark上的应用框架，利用Spark的底层框架作为其执行基础，并在其上构建了DStream的行为抽象。利用DStream所提供的api，用户可以在数据流上实时进行count，join，aggregate等操作。

      当然，Storm也有Yarn-Storm项目，能让Storm运行在Hadoop2.0的Yarn框架上，可以让Hadoop的MapReduce和Storm共享资源。

五.小结


       知乎上有一个挺好的问答： 问：实时处理系统（类似s4, storm）对比直接用MQ来做好处在哪里？  答：好处是它帮你做了： 1) 集群控制。2) 任务分配。3) 任务分发 4) 监控 等等。

需要知道Storm不是一个完整的解决方案。使用Storm你需要加入消息队列做数据入口，考虑如何在流中保存状态，考虑怎样将大问题用分布式去解决。解决这些问题的成本可能比增加一个服务器的成本还高。但是，一旦下定决定使用了Storm并解决了那些恼人的细节，你就能享受到Storm给你带来的简单，可拓展等优势了。


Mapreduce初析

       Mapreduce是一个计算框架，既然是做计算的框架，那么表现形式就是有个输入（input），mapreduce操作这个输入（input），通过本身定义好的计算模型，得到一个输出（output），这个输出就是我们所需要的结果。

       重点就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。

Mapreduce的基础实例

       讲解mapreduce运行原理前，首先我们看看mapreduce里的hello world实例WordCount,这个实例在任何一个版本的hadoop安装程序里都会有，大家很容易找到，这里我还是贴出代码，便于我后面的讲解，代码如下：







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

79

80

81

82

83

84

85

86




/**

 *
 Licensed to the Apache Software Foundation (ASF) under one

 *
 or more contributor license agreements.  See the NOTICE file

 *
 distributed with this work for additional information

 *
 regarding copyright ownership.  The ASF licenses this file

 *
 to you under the Apache License, Version 2.0 (the

 *
 "License"); you may not use this file except in compliance

 *
 with the License.  You may obtain a copy of the License at

 *

 *    
 http://www.apache.org/licenses/LICENSE-2.0

 *

 *
 Unless required by applicable law or agreed to in writing, software

 *
 distributed under the License is distributed on an "AS IS" BASIS,

 *
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

 *
 See the License for the specific language governing permissions and

 *
 limitations under the License.

 */

packageorg.apache.hadoop.examples;

 

importjava.io.IOException;

importjava.util.StringTokenizer;

 

importorg.apache.hadoop.conf.Configuration;

importorg.apache.hadoop.fs.Path;

importorg.apache.hadoop.io.IntWritable;

importorg.apache.hadoop.io.Text;

importorg.apache.hadoop.mapreduce.Job;

importorg.apache.hadoop.mapreduce.Mapper;

importorg.apache.hadoop.mapreduce.Reducer;

importorg.apache.hadoop.mapreduce.lib.input.FileInputFormat;

importorg.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

importorg.apache.hadoop.util.GenericOptionsParser;

 

publicclass

WordCount {

 

  publicstatic

class 
TokenizerMapper 

       extendsMapper&lt;Object,
 Text, Text, IntWritable&gt;{

 

    privatefinal

static 
IntWritable one = newIntWritable(1);

    privateText
 word = newText();

 

    publicvoid

map(Object key, Text value, Context context

                    )throwsIOException,
 InterruptedException {

      StringTokenizer
 itr = newStringTokenizer(value.toString());

      while(itr.hasMoreTokens())
 {

        word.set(itr.nextToken());

        context.write(word,
 one);

      }

    }

  }

 

  publicstatic

class 
IntSumReducer 

       extendsReducer&lt;Text,IntWritable,Text,IntWritable&gt;
 {

    privateIntWritable
 result = newIntWritable();

 

    publicvoid

reduce(Text key, Iterable&lt;IntWritable&gt; values, 

                       Context
 context

                       )throwsIOException,
 InterruptedException {

      intsum
 = 0;

      for(IntWritable
 val : values) {

        sum
 += val.get();

      }

      result.set(sum);

      context.write(key,
 result);

    }

  }

 

  publicstatic

void 
main(String[] args) throwsException
 {

    Configuration
 conf = newConfiguration();

    String[]
 otherArgs = newGenericOptionsParser(conf,
 args).getRemainingArgs();

    if(otherArgs.length
 != 2)
 {

      System.err.println("Usage:
 wordcount &lt;in&gt; &lt;out&gt;");

      System.exit(2);

    }

    Job
 job = newJob(conf,
"word
 count");

    job.setJarByClass(WordCount.class);

    job.setMapperClass(TokenizerMapper.class);

    job.setCombinerClass(IntSumReducer.class);

    job.setReducerClass(IntSumReducer.class);

    job.setOutputKeyClass(Text.class);

    job.setOutputValueClass(IntWritable.class);

    FileInputFormat.addInputPath(job,newPath(otherArgs[0]));

    FileOutputFormat.setOutputPath(job,newPath(otherArgs[1]));

    System.exit(job.waitForCompletion(true)
 ? 0:
1);

  }

}








       如何运行它，这里不做累述了，大伙可以百度下，网上这方面的资料很多。这里的实例代码是使用新的api，大家可能在很多书籍里看到讲解mapreduce的WordCount实例都是老版本的api，这里我不给出老版本的api，因为老版本的api不太建议使用了，大家做开发最好使用新版本的api，新版本api和旧版本api有区别在哪里：

新的api放在：org.apache.hadoop.mapreduce,旧版api放在：org.apache.hadoop.mapred新版api使用虚类，而旧版的使用的是接口，虚类更加利于扩展，这个是一个经验，大家可以好好学习下hadoop的这个经验。

      其他还有很多区别，都是说明新版本api的优势，因为我提倡使用新版api，这里就不讲这些，因为没必要再用旧版本，因此这种比较也没啥意义了。

     下面我对代码做简单的讲解，大家看到要写一个mapreduce程序，我们的实现一个map函数和reduce函数。我们看看map的方法：







1




publicvoid

map(Object key, Text value, Context context) throwsIOException,
 InterruptedException {…}








      这里有三个参数，前面两个Object key, Text value就是输入的key和value，第三个参数Context context这是可以记录输入的key和value，例如：context.write(word, one);此外context还会记录map运算的状态。

对于reduce函数的方法：







1




publicvoid

reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throwsIOException,
 InterruptedException {…}








     reduce函数的输入也是一个key/value的形式，不过它的value是一个迭代器的形式Iterable&lt;IntWritable&gt; values，也就是说reduce的输入是一个key对应一组的值的value，reduce也有context和map的context作用一致。

    至于计算的逻辑就是需要自己去实现了。

下面就是main函数的调用了，这个我要详细讲述下，首先是：







1




Configuration
 conf = newConfiguration();








     运行mapreduce程序前都要初始化Configuration，该类主要是读取mapreduce系统配置信息，这些信息包括hdfs还有mapreduce，也就是安装hadoop时候的配置文件例如：core-site.xml、hdfs-site.xml和mapred-site.xml等等文件里的信息，有些童鞋不理解为啥要这么做，这个是没有深入思考mapreduce计算框架造成，我们程序员开发mapreduce时候只是在填空，在map函数和reduce函数里编写实际进行的业务逻辑，其它的工作都是交给mapreduce框架自己操作的，但是至少我们要告诉它怎么操作啊，比如hdfs在哪里啊，mapreduce的jobstracker在哪里啊，而这些信息就在conf包下的配置文件里。

接下来的代码是：







1

2

3

4

5




String[]
 otherArgs = newGenericOptionsParser(conf,
 args).getRemainingArgs();

  if(otherArgs.length
 != 2)
 {

    System.err.println("Usage:
 wordcount &lt;in&gt; &lt;out&gt;");

    System.exit(2);

  }








      If的语句好理解，就是运行WordCount程序时候一定是两个参数，如果不是就会报错退出。至于第一句里的GenericOptionsParser类，它是用来解释常用hadoop命令，并根据需要为Configuration对象设置相应的值，其实平时开发里我们不太常用它，而是让类实现Tool接口，然后再main函数里使用ToolRunner运行程序，而ToolRunner内部会调用GenericOptionsParser。

接下来的代码是：







1

2

3

4

5




Job
 job = newJob(conf,
"word
 count");

job.setJarByClass(WordCount.class);

job.setMapperClass(TokenizerMapper.class);

job.setCombinerClass(IntSumReducer.class);

job.setReducerClass(IntSumReducer.class);








       第一行就是在构建一个job，在mapreduce框架里一个mapreduce任务也叫mapreduce作业也叫做一个mapreduce的job，而具体的map和reduce运算就是task了，这里我们构建一个job，构建时候有两个参数，一个是conf这个就不累述了，一个是这个job的名称。

      第二行就是装载程序员编写好的计算程序，例如我们的程序类名就是WordCount了。这里我要做下纠正，虽然我们编写mapreduce程序只需要实现map函数和reduce函数，但是实际开发我们要实现三个类，第三个类是为了配置mapreduce如何运行map和reduce函数，准确的说就是构建一个mapreduce能执行的job了，例如WordCount类。

      第三行和第五行就是装载map函数和reduce函数实现类了，这里多了个第四行，这个是装载Combiner类，这个我后面讲mapreduce运行机制时候会讲述，其实本例去掉第四行也没有关系，但是使用了第四行理论上运行效率会更好。

      接下来的代码：







1

2




job.setOutputKeyClass(Text.class);

job.setOutputValueClass(IntWritable.class);








      这个是定义输出的key/value的类型，也就是最终存储在hdfs上结果文件的key/value的类型。

     最后的代码是：







1

2

3




FileInputFormat.addInputPath(job,newPath(otherArgs[0]));

FileOutputFormat.setOutputPath(job,newPath(otherArgs[1]));

System.exit(job.waitForCompletion(true)
 ? 0:
1);








    第一行就是构建输入的数据文件，第二行是构建输出的数据文件，最后一行如果job运行成功了，我们的程序就会正常退出。FileInputFormat和FileOutputFormat是很有学问的，我会在下面的mapreduce运行机制里讲解到它们。

    好了，mapreduce里的hello word程序讲解完毕，我这个讲解是从新办api进行，这套讲解在网络上还是比较少的，应该很具有代表性的。

Mapreduce运行机制

    下面我要讲讲mapreduce的运行机制了，前不久我为公司出了一套hadoop面试题，里面就问道了mapreduce运行机制，出题时候我发现这个问题我自己似乎也将不太清楚，因此最近几天恶补了下，希望在本文里能说清楚这个问题。

    下面我贴出几张图，这些图都是我在百度图片里找到的比较好的图片：

图片一：



　　图片二：



　　图片三：



　　图片四：



　　图片五：



　　图片六：



     我现在学习技术很喜欢看图，每次有了新理解就会去看看图，每次都会有新的发现。

     mapreduce运行机制，可以从很多不同的角度来理解，比如说从mapreduce运行流程来讲解，也可以从计算模型的逻辑流程来进行讲解，也许有些深入理解了mapreduce运行机制还会从更好的角度来描述，但是将mapreduce运行机制有些东西是避免不了的，就是一个个参入的实例对象，一个就是计算模型的逻辑定义阶段，我这里讲解不从什么流程出发，就从这些一个个牵涉的对象，不管是物理实体还是逻辑实体。

      首先讲讲物理实体，参入mapreduce作业执行涉及4个独立的实体：

客户端（client）：编写mapreduce程序，配置作业，提交作业，这就是程序员完成的工作；JobTracker：初始化作业，分配作业，与TaskTracker通信，协调整个作业的执行；TaskTracker：保持与JobTracker的通信，在分配的数据片段上执行Map或Reduce任务，TaskTracker和JobTracker的不同有个很重要的方面，就是在执行任务时候TaskTracker可以有n多个，JobTracker则只会有一个（JobTracker只能有一个就和hdfs里namenode一样存在单点故障，我会在后面的mapreduce的相关问题里讲到这个问题的）Hdfs：保存作业的数据、配置信息等等，最后的结果也是保存在hdfs上面

那么mapreduce到底是如何运行的呢？

      首先是客户端要编写好mapreduce程序，配置好mapreduce的作业也就是job，接下来就是提交job了，提交job是提交到JobTracker上的，这个时候JobTracker就会构建这个job，具体就是分配一个新的job任务的ID值，接下来它会做检查操作，这个检查就是确定输出目录是否存在，如果存在那么job就不能正常运行下去，JobTracker会抛出错误给客户端，接下来还要检查输入目录是否存在，如果不存在同样抛出错误，如果存在JobTracker会根据输入计算输入分片（Input Split），如果分片计算不出来也会抛出错误，至于输入分片我后面会做讲解的，这些都做好了JobTracker就会配置Job需要的资源了。分配好资源后，JobTracker就会初始化作业，初始化主要做的是将Job放入一个内部的队列，让配置好的作业调度器能调度到这个作业，作业调度器会初始化这个job，初始化就是创建一个正在运行的job对象（封装任务和记录信息），以便JobTracker跟踪job的状态和进程。

      初始化完毕后，作业调度器会获取输入分片信息（input split），每个分片创建一个map任务。接下来就是任务分配了，这个时候tasktracker会运行一个简单的循环机制定期发送心跳给jobtracker，心跳间隔是5秒，程序员可以配置这个时间，心跳就是jobtracker和tasktracker沟通的桥梁，通过心跳，jobtracker可以监控tasktracker是否存活，也可以获取tasktracker处理的状态和问题，同时tasktracker也可以通过心跳里的返回值获取jobtracker给它的操作指令。任务分配好后就是执行任务了。在任务执行时候jobtracker可以通过心跳机制监控tasktracker的状态和进度，同时也能计算出整个job的状态和进度，而tasktracker也可以本地监控自己的状态和进度。当jobtracker获得了最后一个完成指定任务的tasktracker操作成功的通知时候，jobtracker会把整个job状态置为成功，然后当客户端查询job运行状态时候（注意：这个是异步操作），客户端会查到job完成的通知的。如果job中途失败，mapreduce也会有相应机制处理，一般而言如果不是程序员程序本身有bug，mapreduce错误处理机制都能保证提交的job能正常完成。

       下面我从逻辑实体的角度讲解mapreduce运行机制，这些按照时间顺序包括：输入分片（input split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。

1. 输入分片（input split）：在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，输入分片（input
 split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组，输入分片（input split）往往和hdfs的block（块）关系很密切，假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（input split）而127mb也是两个输入分片（input split），换句话说我们如果在map计算前做输入分片调整，例如合并小文件，那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。

2. map阶段：就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行；

3. combiner阶段：combiner阶段是程序员可以选择的，combiner其实也是一种reduce操作，因此我们看见WordCount类里是用reduce进行加载的。Combiner是一个本地化的reduce操作，它是map运算的后续操作，主要是在map计算出中间文件前做一个简单的合并重复key值的操作，例如我们对文件里的单词频率做统计，map计算时候如果碰到一个hadoop的单词就会记录为1，但是这篇文章里hadoop可能会出现n多次，那么map输出文件冗余就会很多，因此在reduce计算前对相同的key做一个合并操作，那么文件会变小，这样就提高了宽带的传输效率，毕竟hadoop计算力宽带资源往往是计算的瓶颈也是最为宝贵的资源，但是combiner操作是有风险的，使用它的原则是combiner的输入不会影响到reduce计算的最终输入，例如：如果计算只是求总数，最大值，最小值可以使用combiner，但是做平均值计算使用combiner的话，最终的reduce计算结果就会出错。


4. shuffle阶段：将map的输出作为reduce的输入的过程就是shuffle了，这个是mapreduce优化的重点地方。这里我不讲怎么优化shuffle阶段，讲讲shuffle阶段的原理，因为大部分的书籍里都没讲清楚shuffle阶段。Shuffle一开始就是map阶段做输出操作，一般mapreduce计算的都是海量数据，map输出时候不可能把所有文件都放到内存操作，因此map写入磁盘的过程十分的复杂，更何况map输出时候要对结果进行排序，内存开销是很大的，map在做输出时候会在内存里开启一个环形内存缓冲区，这个缓冲区专门用来输出的，默认大小是100mb，并且在配置文件里为这个缓冲区设定了一个阀值，默认是0.80（这个大小和阀值都是可以在配置文件里进行配置的），同时map还会为输出操作启动一个守护线程，如果缓冲区的内存达到了阀值的80%时候，这个守护线程就会把内容写到磁盘上，这个过程叫spill，另外的20%内存可以继续写入要写进磁盘的数据，写入磁盘和写入内存操作是互不干扰的，如果缓存区被撑满了，那么map就会阻塞写入内存的操作，让写入磁盘操作完成后再继续执行写入内存操作，前面我讲到写入磁盘前会有个排序操作，这个是在写入磁盘操作时候进行，不是在写入内存时候进行的，如果我们定义了combiner函数，那么排序前还会执行combiner操作。

       每次spill操作也就是写入磁盘操作时候就会写一个溢出文件，也就是说在做map输出有几次spill就会产生多少个溢出文件，等map输出全部做完后，map会合并这些输出文件。这个过程里还会有一个Partitioner操作，对于这个操作很多人都很迷糊，其实Partitioner操作和map阶段的输入分片（Input split）很像，一个Partitioner对应一个reduce作业，如果我们mapreduce操作只有一个reduce操作，那么Partitioner就只有一个，如果我们有多个reduce操作，那么Partitioner对应的就会有多个，Partitioner因此就是reduce的输入分片，这个程序员可以编程控制，主要是根据实际key和value的值，根据实际业务类型或者为了更好的reduce负载均衡要求进行，这是提高reduce效率的一个关键所在。到了reduce阶段就是合并map输出文件了，Partitioner会找到对应的map输出文件，然后进行复制操作，复制操作时reduce会开启几个复制线程，这些线程默认个数是5个，程序员也可以在配置文件更改复制线程的个数，这个复制过程和map写入磁盘过程类似，也有阀值和内存大小，阀值一样可以在配置文件里配置，而内存大小是直接使用reduce的tasktracker的内存大小，复制时候reduce还会进行排序操作和合并文件操作，这些操作完了就会进行reduce计算了。

5. reduce阶段：和map函数一样也是程序员编写的，最终结果是存储在hdfs上的。

Mapreduce的相关问题

        这里我要谈谈我学习mapreduce思考的一些问题，都是我自己想出解释的问题，但是某些问题到底对不对，就要广大童鞋帮我确认了。

① jobtracker的单点故障：jobtracker和hdfs的namenode一样也存在单点故障，单点故障一直是hadoop被人诟病的大问题，为什么hadoop的做的文件系统和mapreduce计算框架都是高容错的，但是最重要的管理节点的故障机制却如此不好，我认为主要是namenode和jobtracker在实际运行中都是在内存操作，而做到内存的容错就比较复杂了，只有当内存数据被持久化后容错才好做，namenode和jobtracker都可以备份自己持久化的文件，但是这个持久化都会有延迟，因此真的出故障，任然不能整体恢复，另外hadoop框架里包含zookeeper框架，zookeeper可以结合jobtracker，用几台机器同时部署jobtracker，保证一台出故障，有一台马上能补充上，不过这种方式也没法恢复正在跑的mapreduce任务。

② 做mapreduce计算时候，输出一般是一个文件夹，而且该文件夹是不能存在，我在出面试题时候提到了这个问题，而且这个检查做的很早，当我们提交job时候就会进行，mapreduce之所以这么设计是保证数据可靠性，如果输出目录存在reduce就搞不清楚你到底是要追加还是覆盖，不管是追加和覆盖操作都会有可能导致最终结果出问题，mapreduce是做海量数据计算，一个生产计算的成本很高，例如一个job完全执行完可能要几个小时，因此一切影响错误的情况mapreduce是零容忍的。

③ Mapreduce还有一个InputFormat和OutputFormat，我们在编写map函数时候发现map方法的参数是之间操作行数据，没有牵涉到InputFormat，这些事情在我们new Path时候mapreduce计算框架帮我们做好了，而OutputFormat也是reduce帮我们做好了，我们使用什么样的输入文件，就要调用什么样的InputFormat，InputFormat是和我们输入的文件类型相关的，mapreduce里常用的InputFormat有FileInputFormat普通文本文件，SequenceFileInputFormat是指hadoop的序列化文件，另外还有KeyValueTextInputFormat。OutputFormat就是我们想最终存储到hdfs系统上的文件格式了，这个根据你需要定义了，hadoop有支持很多文件格式，这里不一一列举。


数据处理与联机分析处理 ( OLAP )

        联机分析处理是那些为了支持商业智能，报表和数据挖掘与探索等业务而开展的工作。这类工作的例子有零售商按地区和季度两个维度计算门店销售额，银行按语言和月份两个维度计算手机银行装机量，设备制造商定位有哪些零部件的故障率比期望值高，以及医院研究有哪些事件会引起高危婴儿紧张等。

       如果原始数据来源于 OLTP 系统，典型的做法是将这些数据拷贝到 OLAP 数据库中，再进行这类“离线”分析任务的处理，这么做有很多原因，但考虑最多的还是性能因素。

       假设一下，如果一个实体店使用他们的事务处理系统来承担数据分析工作，这种情况下分析师提交的粗暴查询就可能会实实在在地影响并拉低门店对于那些已经记录在册等待结算的订单结算率。另外用于事务中的查询类型从根本上就不同于数据分析类查询。

       事务系统典型的查询是基于某个独立的实体，比如某一个客户或某一个用户。例如当一个在线零售网站创建一个交易订单状态页时，数据的查询是针对某一个客户已经提交的特定订单。然而在数据分析的用例中，分析师最感兴趣的却是那些根据时间维度划分查询本身已跨越了订单或用户数据的汇总信息。就如前面提到的那样，根据区域和季度两个维度统计的门店销售额会查询给定时间段内所有订单数据。

      行动之前还有最后一个注意要点，本篇中的数据库并不提供传统关系型数据库用户所期望的那类增删改操作。与事务系统不同的是，分析类型的查询主要是那些涉及到数百万甚至数亿行数据的 SELECT 查询。分析型数据库的优化也主要围绕着这类负载进行，而这些优化措施却会导致针对小批量数据的增删改操作执行起来代价昂贵。

      即便这类数据库在接口和语义方面都与关系型数据库不同，但他们也确实提供了增加行 （ INSERT ），更新行（ UPDATE ）和删除行（ DELETE ）操作的功能支持。有些读者或许正在问 Hive 系统里最近新增加的 “ ACID ” 相关的问题，容后详禀。

     在 Hive 系统的 “ACID” 功能之外，处理更新操作有两种方式可选，一种是使用数据所在的 HBase 系统本身提供的更新功能。尽管 HBase 经常主要用于 OLTP 业务，但有些 OLAP 系统会使用 HBase 来存储一些小表，典型的称为维度表，这类表需要周期性地更新。第二种处理更新操作的方式是执行一次合并操作。

    从一个 ETL 开发者角度出发，一个合并过程会引入额外工作量。因此有个问题一定会被问到，那就是既然 HBase 系统已经提供了更新功能，那这类合并工作就不是必须的，那为什么不直接都用 HBase 呢？原因是扫描查询的处理性能，如果要在基于尾部追加模式的 HDFS 文件系统提供随机更新的功能，HBase 就得在它读取每一行时都做少量的合并操作，这个架构决定了能提供较高的写和随机读性能，但与 HDFS 相比，只能提供较差的扫描查询和顺序读操作性能。这样一来 HBase 就只能用于存储那些需要频繁更新的小表场合；

这个领域包含几个子目录：

Apache HiveDremel clonesSpark SQL

Apache Hive

       本项目最初由脸谱公司创建，Hive 是第一个基于 Hadoop 之上的 SQL 引擎，且至今仍是最成熟的。Hive 原先是构建在MapReduce之上的，也曾经被改造过以便运行在 Apache Tez 上，现在正在进行的是为适应 Apache Spark 而进行的改造，基于 Spark 的Hive 改造被称为是最后的工作，但不能与 Spark 项目上的其他 SQL 支持项目相互混淆，关于 Spark 上的其他 SQL 支持项目我会再找个合适时机进行讨论。

      到目前为止，Hive 拥有最完整的 SQL 功能支持，并且也是拥有最多贡献者的项目，几乎所有的 Hadoop 用户都会部署Hive，同时几乎 Hadoop 上其他 SQL 引擎使用者也都会部署 Hive ，事实上大多数 SQL 引擎都以这种或那种方式依赖于Hive 。

     大多数 Hadoop 赞助商，包括 Cloudera 和 Hortonworks，都一致认同 Hive 是唯一有能力处理大批量任务和集成多种非标准数据格式的组件。Hortonworks 与 Cloudera 意见相左的地方在于对 Hive 的性能评价，Clourdera 觉得 Hive 的性能简直不能与 Dremel clones 相比，而 Hortonworks 则觉得 Hive 可以和 Dremel Clones 一较高下。

Dremel Clones

      就像开源界一样，谷歌内部也创立了多个 SQL 引擎，他们有一个类似于 Hive 的 SQL 引擎叫 Tenzing，还有另外一个系统叫 Dremel。Hive 的创立者 Facebook 公司也创建了一个 Dremel 的克隆版本叫 Presto。

      Cloudera Impala 和 Apache Drill 是最杰出的两个 Dremel 克隆版本，Cloudera 将 Impala 市场定位为最成熟的开源 Dremel 分支，Impala 在2013年年中发布 GA 版本，MapR 是 Drill 背后的主要赞助商，他把 Drill 的市场角色定位为最灵活的 Dremel 分支， Impala 能满足在 Hive 系统中存储元数据表的需求，而 Drill 可以直接查询 JSON 和自定义格式文件，比如 Apache Parquet 和 Avro
 文件格式等。

Spark SQL

      尽管有 Hadoop 上有其他多个 SQL 引擎，但 Spark SQL 却有着对其感兴趣的最广泛受众。Spark SQL 是 Spark 引擎上的榜眼，而状元是 Shark， Shark 因为顾及 Spark SQL 和 Hive on Spark 项目，Shark 目前已经终止开发，与 Shark 项目曾近是加州伯克利大学的一个研究项目不同，Spark SQL 和 Hive on Spark 已经在 Spark 赞助商们的支持下建立了各自的开源项目；

      基于 Spark 的 Hive 可以简单地说成是前端是 Hive 后端是 Spark ，基于 MR 或 Tez 的 Hive 既有用户可以在原系统与 Hive on Spark 系统之间轻松切换，切换工作仅仅只需要简单地修改下配置参数。

      Spark SQL 是一个完整的新引擎，今天的 Spark SQL 对那些希望把 SQL 嵌入到他们的 Scala，Java 或者 Python 程序的Spark 开发者而言是最有用的，但 Spark SQL 的主要赞助商 Databricks 对 Spark SQL 还有着更大的雄心，并指望将 Spark SQL 的使用范围扩展到非 Spark 开发者中去；


Hadoop 引擎上的 SQL 有许多广泛的应用领域：

数据处理与在线分析处理（OLAP）改进优化在线事务处理（OLTP）

存储引擎:

今天 Hadoop 主要有三个存储引擎：分别是 Apache HBase、Apache Hadoop HDFS 和 Hadoop Accumulo。Apache Accumlo与 Hbase 非常相似，但它本是由 NSA 组织创建的项目，历史上特别看重系统的安全性，尤其在授权认证方面；在我们看来，HBase 现在已经将安全特性方面的工作加入到项目中了，这样的话后面就不再进一步讨论 Accumulo 了。

HBase 是一个分布式键值存储系统，数据是通过排好序的 map 形式存储，也就是说数据都是经过对 key 列排序的，就像我们下面要描述的那样，HBase 典型的用例是 OLTP 应用，HDFS 是一个文件系统并能够以分布式的方式存储极大容量的数据集合；

HBase 在 HDFS 里存储的数据是以 HFile 格式存入的，这种格式不可配置。当不使用 HBase 而直接使用 HDFS 时，用户是必须要选择一种文件格式；

当进行文件格式选择时是有许多要点需要考虑的，比如，

主要的读取模式是怎样的？是读取所有行呢，还是只读取所有行数据的一个子集；数据是不是还可能含有非 ASCII 码的文本内容？哪些工具会读写这些数据呢（Apache Hive,  Spark ?）；

广义上说有两种文件格式与 HDFS 一起使用，它们是 Columnar 和 row-wise。Columnar 格式例如 RCFILE、ORC 和 Apache Parquet等，这些类型能提供极致的压缩性能（通过类似行程编码的多种编码方式进行压缩），同时在只读取行内少量列的场景下，也能提供较高的读性能；比如你一行数据有五十到一百个列却只需读取七八个列的场合；

row-wise 格式，比如有受限定的文本格式、Sequence 文件格式以及 Apache Avro 格式，这些格式虽不提供有效的压缩特性，但比较适合那些需要读取表中大多数列的业务场景，也适合那种数据是以流的方式，每次小批量地导进表中的业务场景；

我们建议排除文本格式，RCfile 和 Sequence 文件这几种格式，因为他们都是历史遗留的文件格式，另外不推荐也是因为集成历史系统数据时它们有潜在的异常问题。我们不建议使用这些格式是因为他们容易发生文本冲突（如非 ASCII 码文本），性能差，还有除了文本方式之外很少有工具可以读取它们；

一旦我们回答了选 columnar 还是 row-wise 的问题，并排除了历史遗留的那些文件格式，最重要的问题就变成了哪一个工具和引擎能够读取和写入这些数据，大量的 Hadoop 生态链工具和引擎已经集成了 Avro 和 Parquet 项目，其中 ORC 是性能最好的 Apache Hive 文件格式。

在线事务处理

Apache HBase 项目提供 OLTP 类型的操作并极具扩展性，HBase 是唯一一个通常用于在线用户数据存储的Hadoop子模块，但是 HBase 项目的目标并不是做一个关系型数据库管理系统，而且它也不是为了替换 MySQL、Oracle 或者 DB2 这类关系型数据库的，实际上 HBase 自己并不提供任何 SQL 接口，而且用户还必须用 Java, Python, 或者 Ruby 编程来存储和检索数据；

Apache Phoenix 项目目标是基于 Apache HBase 提供 OLTP 类型的 SQL，Phoenix 允许用户基于 HBase 数据模型执行插入更新和删除操作，但是就像前面提到的一样，HBase 数据模型从根本上就不同于传统关系型数据库，这样的话 HBase 加 Phoenix 也仍然不是关系型数据库的替代者；

HBase （以及Phoenix） 项目对于那些基于 RDBMS 之上，在应用扩展过程中遇到麻烦的业务场景非常有用；传统关系型数据库领域里的一个传统解决方案是进行水平分区，但这种解决方案跑起来却常遭受以下缺陷的困扰：

跨分片事务没有得到支持增加机器进行水平扩容时需要复杂且昂贵的再分片过程，

就像经过分片的数据库一样，HBase 并不支持事务，但增加机器进行水平扩展和在HBase内部做负载再均衡，HBase 系统就要容易得多；

新的节点可以被加入到 HBase 集群中，HBase 能够自动分配数据分片到不同节点，如果假定分片数据库和 HBase 都缺少事务支持的话，HBase 就会因提供易于增加机器水平扩展的能力而胜出，有一些公司已经在做底层使用 HBase 架构基础而上层增加事务 SQL 支持的产品，比如 Splice Machine公司等。


P2P、P2C 、O2O 、B2C、B2B、 C2C的概念解析

P2P到底是什么呢?

　　P2P借贷是一种将非常小额度的资金聚集起来借贷给有资金需求人群的一种民间小额借贷模式。P2P是“Peer-to-Peer”的简写，个人对个人的意思，P2P借贷指个人通过第三方平台(P2P公司)在收取一定服务费用的前提下向其他个人提供小额借贷的金融模式。

　　P2P模式

　　第一种是纯线上模式，是纯粹的P2P，在这种平台模式上纯粹进行信息匹配，帮助资金借贷双方更好的进行资金匹配，但缺点明显，这种线上模式并不参与担保;

　　第二种是债权转让模式，平台本身先行放贷，再将债权放到平台进行转让，很明显能让企业提高融资端的工作效率，但容易出现资金池，不能让资金充分发挥效益。

O2O到底是什么呢?

　　O2O是目前微信二维码营销的超火概念，即Online
 To Offline，也即将线下商务的机会与互联网结合在了一起，让互联网成为线下交易的前台。

　　这样线下服务就可以用线上来揽客，消费者可以用线上来筛选服务，还有成交可以在线上结算，很快达到规模。

　　该模式最重要的特点是：推广效果可查，每笔交易可跟踪。

　　O2O的优势

　　O2O的优势在于把网上和网下的优势完美结合。通过网购导购机，把互联网与地面店完美对接，实现互联网落地。让消费者在享受线上优惠价格的同时，又可享受线下贴身的服务。同时，O2O模式还可实现不同商家的联盟。

　　O2O营销模式的核心

　　O2O营销模式的核心是在线预付，在线支付不仅是支付本身的完成，是某次消费得以最终形成的唯一标志，更是消费数据唯一可靠的考核标准。其是对提供online服务的互联网专业公司而言，只有用户在线上完成支付，自身才可能从中获得效益，

B2C的概念

　　B2C是Business-to-Customer的缩写，而其中文简称为“商对客”。“商对客”是电子商务的一种模式，也就是通常说的商业零售，直接面向消费者销售产品和服务。这种形式的电子商务一般以网络零售业为主，主要借助于互联网开展在线销售活动。B2C即企业通过互联网为消费者提供一个新型的购物环境——网上商店，消费者通过网络在网上购物、在网上支付。

　　网站组成

　　B2C电子商务网站由三个基本部分组成：

　　1、为顾客提供在线购物场所的商场网站;

　　2、负责为客户所购商品进行配送的配送系统;

　　3、负责顾客身份的确认及货款结算的银行及认证系统。

　　代表网站：

　　天猫——为人服务做平台

　　京东——自主经营卖产品

　　凡客——自产自销做品牌

B2B概念

　　B2B(企业对企业的电子商务模式)

　　B2B(也有写成BTB)是指企业对企业之间的营销关系，它将企业内部网，通过B2B网站与客户紧密结合起来，通过网络的快速反应，为客户提供更好的服务，从而促进企业的业务发展(Business Development)。近年来B2B发展势头迅猛，趋于成熟。

　　B2B是指进行电子商务交易的供需双方都是商家(或企业、公司)，她(他)们使用了互联网的技术或各种商务网络平台，完成商务交易的过程。电子商务是现代B2Bmarketing的一种具体主要的表现形式。

　　含有三要素

　　1.买卖：B2B网站平台为消费者提供质优价廉的商品，吸引消费者购买的同时促使更多商家的入驻。

　　2.合作：与物流公司建立合作关系，为消费者的购买行为提供最终保障，这是B2B平台硬性条件之一。

　　3.服务：物流主要是为消费者提供购买服务，从而实现再一次的交易。

　　代表网站：

　　阿里巴巴

　　阿里巴巴是国内也是全球最大的B2B电子商务网站。是中小企业首选的B2B平台，主要提供“诚信通”服务，但由于所有用户基本上都是“诚信通”客户。所以没有专业的电子商务运营能力和做阿里巴巴的其它推广业务，很难取得显著效果。

　　中国制造网

　　中国制造网是B2B电子商务行业网站后起之秀，干净的网站风格，实用的网站类容，深受用户喜爱。虽然位居中国B2B网站行业第三位，但由于“百销通”付费用户还不算多，听说效果还算不错，推荐做电子商务的企业尝试一下。

　　中国供应商

　　中国供应商是由中国互联网新闻中心主办的B2B贸易平台。提供多种样式的广告服务，但网站以免费普通会员居多，好好运营商铺，效果还可以。反正不要钱，建议多注册一个B2B平台尝试一下。

C2C的概念

　　C2C实际是电子商务的专业用语，是个人与个人之间的电子商务。C指的是消费者，因为消费者的英文单词是Customer(Consumer)，所以简写为C，而C2C即
 Customer(Consumer) to Customer(Consumer)。C2C的意思就是个人与个人之间的电子商务。比如一个消费者有一台电脑，通过网络进行交易，把它出售给另外一个消费者，此种交易类型就称为C2C电子商务。

　　代表网站： 淘宝网 易趣网 拍拍网

　　毫无疑问，淘宝在C2C领域的领先地位暂时还没有人能够撼动。然而，淘宝却也不得不承受这份领先带来的沉甸甸压力。在领先与压力之间，淘宝在奋力往前走

P2C概念

　　P2C即Production
 to Consumer简称为商品和顾客，产品从生产企业直接送到消费者手中，中间没有任何的交易环节。是继B2B、B2C、C2C之后的又一个电子商务新概念。在国内叫做：生活服务平台。

　　P2C具体表现为：如果哪天家乐福、沃尔玛、大中电器等这些零售业巨头也进军电子商务，通过互联网开展商务活动，这种商务活动的可能性一直是存在的，并且随着互联网技术的平台发展，还会向中小企业逐步渗透。

　　P2C把老百姓日常生活当中的一切密切相关的服务信息，如房产、餐饮、交友、家政服务、票务、健康、医疗、保健等聚合在平台上，实现服务业的电子商务化。

　　personal(个人) to Company(公司)

　　platform(平台) to CreditAssignment(债权转让)

　　是继P2P之后的又一个互联网金融新概念

　　该理论是国内首个P2C互联网金融服务，对债权转让企业进行资质审核、实地考察，筛选出具有投资价值的优质债权项目在平台上向投资者公开;并提供在线投资的交易平台，实时为投资者生成具有法律效力的债权转让及服务协议;监督企业的项目经营，管理评估风险，确保投资者资金安全。

O2O、C2C、B2B、B2C的区别在哪里？

　　O2O是Online
 to offline分为四种运营模式：


Online to offline是线上交易到线下消费体验
offline to online是线下营销到线上交易
offline to Online to offline是线下营销到线上交易再到线下消费体验
Online to offline to online是线上交易或营销到线下消费体验再到线上消费体验

　　

　　比如:保险直购O2O,苏宁易购O2O,大众点评O2O等

　　C2C是Consumer to
 Consumer就是个人对个人的，比如淘宝的小店铺。

　　B2C是Business
 to consumer是商家对个人，这个就很多了卓越、当当、京东等等都是。

　　

　　B2C、C2C很重要的一点是都运用了物流。

　　B2B是Business to
 Business是企业间的，比如阿里巴巴。

　　　　 举例通俗说明一下就是：

　　C2C就是我卖东西你来买

　　B2C就是我成立个公司卖东西，你来买

　　O2O就是我成立个公司卖东西你来买但是要你自己来拿

　　B2B就是你也成立了公司买我公司的东西

电子商务知识口诀


B2B有三宝：企业、中介、沟通好
B2C有三宝：品牌、渠道、销售好
C2C有三宝：你开、我买、支付宝
O2O有三宝：线上、线下、一起搞
LBS有三宝：签到、优惠、位置找
NFC有三宝：近场、支付、安全好
SEO有三宝：内容、外链、权重屌
EDM有三宝：内容、受众、分析好
CPA有三宝：行动、转化、站长恼
CPS有三宝：佣金、销量、效果好
CPC有三宝：点击、引导、作弊少
CPM有三宝：展示、千人、不可靠
PHP有三宝：开放、高效、成本少

电子商务模式汇总

　　从事电子商务，以上都要知晓的电子商务模式:


B2B(经济组织对经济组织)
B2C(经济组织对消费者)
B2B2C(企业对企业对消费者)
C2B(T)(消费者集合竞价-团购)
C2C(消费者对消费者)
B2F(企业对家庭)
O2O(网上与网下相结合)
SaaS(软件服务)
PaaS(平台服务)
IaaS(基础服务)
M-B(移动电子商务)
B2G(政府采购)
G2B(政府抛售)
B2M(面向市场营销的电子商务企业)
M2C(生产厂商对消费者)
SoLoMo(社交+本地化+移动)
ABC(代理商-商家-消费者)
BAB(企业-联盟-企业)
P2C(生活服务平台)
P2P(点对点、渠道对渠道)
SNS-EC(社会化网络电子商务)
B2S(分享式商务，或体验式商务)


        在初学计算机编程时，我想大多数人的经历会和作者一样，学校为我们挑选一门语言，大多为 C 或 Java，先是基本的数据类型，然后是程序控制语句，条件判断，循环等，书上会教我们如何定义一个函数，会说程序就是一条一条的指令，告诉计算机该如何操作。同时，我们还会看到如何定义一个递归函数，用来计算阶乘或斐波那契数列。工作以后，其他的这些基础还在日复一日的使用，但递归却很少再被用到，以致我们很难再用递归的方式去解决问题了，为此，我们还有一个借口：递归性能差，使用循环效率高。事实真是这样的吗？我们为自己某种能力的丧失编织了一个美丽的谎言，直到越来越多的编程语言变得流行起来，使我们有机会看到各种语言、各种风格写出的程序，才发现自己应该重新审视递归这一概念了。

为什么递归会受到忽视

         为了回答这一问题，必须先说到编程范式。在所有的编程范式中，面向对象编程（Object-Oriented Programming）无疑是最大的赢家。看看网上的招聘启事，无一例外，会要求应聘者熟练掌握面向对象编程。但其实面向对象编程并不是一种严格意义上的编程范式，严格意义上的编程范式分为：命令式编程（Imperative Programming）、函数式编程（Functional Programming）和逻辑式编程（Logic Programming）。面向对象编程只是上述几种范式的一个交叉产物，更多的还是继承了命令式编程的基因。遗憾的是，在长期的教学过程中，只有命令式编程得到了强调，那就是程序员要告诉计算机应该怎么做，而不是告诉计算机做什么。而递归则通过灵巧的函数定义，告诉计算机做什么。因此在使用命令式编程思维的程序中，不得不说，这是现在多数程序采用的编程方式，递归出镜的几率很少，而在函数式编程中，大家可以随处见到递归的方式。下面，我们就通过实例，为大家展示递归如何作为一种普遍方式，来解决编程问题的。

一组简单的例子

         如何为一组整数数列求和？按照通常命令式编程的思维，我们会采用循环，依次遍历列表中的每个元素进行累加，最终给出求和结果。这样的程序不难写，稍微具备一点编程经验的人在一分钟之内就能写出来。这次我们换个思维，如何用递归的方式求和？为此，我们不妨把问题简化一点，假设数列包含 N 个数，如果我们已经知道了后续 N – 1 个数的和，那么整个数列的和即为第一个数加上后续 N – 1 个数的和，依此类推，我们可以以同样的方式为 N – 1 个数继续求和，直到数列为空，显然，空数列的和为零。听起来复杂，事实上我们可以用一句话来总结：一个数列的和即为数列中的第一个数加上由后续数字组成的数列的和。现在，让我们用
 Scala 语言把这个想法表达出来。

清单 1. 数列求和







1

2

3

4




//xs.head
 返回列表里的头元素，即第一个元素

//xs.tail
 返回除头元素外的剩余元素组成的列表

def

sum(xs:

List[Int]):

Int =

 if

(xs.isEmpty) 0

else 
xs.head + sum(xs.tail)








       大家可以看到，我们只使用一行程序，就将上面求和的方法表达出来了，而且这一行程序看上去简单易懂。尽量少写代码，这也是 Scala 语言的设计哲学之一，较少的代码量意味着写起来更加容易，读起来更加易懂，同时代码出错的概率也会降低。同样的程序，使用 Scala 语言写出的代码量通常会比 Java 少一半甚至更多。

     上述这个数列求和的例子并不是特别的，它代表了递归对于列表的一种普遍的处理方式，即对一个列表的操作，可转化为对第一个元素，及剩余列表的相同操作。比如我们可以用同样的方式求一个数列中的最大值。我们假设已经知道了除第一个元素外剩余数列的最大值，那么整个数列的最大值即为第一个元素和剩余数列最大值中的大者。这里需要注意的是对于一个空数列求最大值是没有意义的，所以我们需要向外抛出一个异常。当数列只包含一个元素时，最大值就为这个元素本身，这种情况是我们这个递归的边界条件。一个递归算法，必须要有这样一个边界条件，否则会一直递归下去，形成死循环。

清单 2. 求最大值







1

2

3

4

5

6

7

8




def

max(xs:

List[Int]):

Int =

{ 

   if

(xs.isEmpty) 

     throw

new 
java.util.NoSuchElementException 

   if

(xs.size ==

1)


     xs.head


   else

     if

(xs.head &gt; max(xs.tail)) xs.head else

max(xs.tail) 

}








同样的方式，我们也可以求一个数列中的最小值，作为一个练习，读者可下去自行实现。

         让我们再看一个例子：如何反转一个字符串？比如给定一个字符串"abcd"，经过反转之后变为 "dcba"。同样的，我们可以做一个大胆的假设，假设后续字符串已经反转过来，那么接上第一个字符，整个字符串就反转过来了。对于一个只有一个字符的字符串，不需要反转，这是我们这个递归算法的边界条件。程序实现如下：

清单 3. 反转字符串







1

2




def

reverse(xs:

String):

String =

if

(xs.length ==

1)
 xs else

reverse(xs.tail) + xs.head








        最后一个例子是经典的快速排序，读者可能会觉得这个例子算不上简单，但是我们会看到，使用递归的方式，再加上 Scala 简洁的语言特性，我们只需要短短几行程序，就可以实现快速排序算法。快速排序算法的核心思想是：在一个无序列表中选择一个值，根据该值将列表分为两部分，比该值小的那一部分排在前面，比该值大的部分排在后面。对于这两部分各自使用同样的方式进行排序，直到他们为空，显然，我们认为一个空的列表即为一个排好序的列表，这就是这个算法中的边界条件。为了方便起见，我们选择第一个元素作为将列表分为两部分的值。程序实现如下：

清单 4. 快速排序







1

2

3

4

5




def

quickSort(xs:

List[Int]):

List[Int] =

{ 

   if

(xs.isEmpty) xs 

   else

     quickSort(xs.filter(x=&gt;x&lt;xs.head)):::xs.head::quickSort(xs.filter(x=&gt;x&gt;xs.head))


}








        当然，为了使程序更加简洁，作者在这里使用了列表中的一些方法：给列表增加一个元素，连接两个列表以及过滤一个列表，并在其中使用了 lambda 表达式。但这一切都使程序变得更符合算法的核心思想，更加易读。

尾递归

       从上面的例子中我们可以看到，使用递归方式写出的程序通常通俗易懂，这其实代表这两种编程范式的不同，命令式编程范式倾向于使用循环，告诉计算机怎么做，而函数式编程范式则使用递归，告诉计算机做什么。习惯于命令式编程范式的程序员还有一个担忧：相比循环，递归不是存在效率问题吗？每一次递归调用，都会分配一个新的函数栈，如果递归嵌套很深，容易出现栈溢出的问题。比如下面计算阶乘的递归程序：

清单 5. 递归求阶乘







1

2




def

factorial(n:

Int):

Int =

if

(n ==

0)
1

else 
n * factorial(n - 1)








       当递归调用 n – 1的阶乘时，由于需要保存前面的 n，必须分配一个新的函数栈，这样当 n很大时，函数栈将很快被耗尽。然而尾递归能帮我们解决这个问题，所谓尾递归是指在函数调用的最后一步，只调用该递归函数本身，此时，由于无需记住其他变量，当前的函数栈可以被重复使用。上面的程序只需稍微改造一下，既可以变成尾递归式的程序，在效率上，和循环是等价的。

清单 6. 尾递归求阶乘







1

2

3

4

5

6

7




def

factorial(n:

Int):

Int =

{ 

   @tailrec


   def

loop(acc:

Int, n:

Int):

Int =

     if

(n ==

0)
 acc else

loop(n * acc, n - 1)


 

   loop(1,
 n) 

}








       在上面的程序中，我们在阶乘函数内部定义了一个新的递归函数，该函数最后一步要么返回结果，要么调用该递归函数本身，所以这是一个尾递归函数。该函数多出一个变量 acc，每次递归调用都会更新该变量，直到递归边界条件满足时返回该值，即为最后的计算结果。这是一种通用的将非尾递归函数转化为尾递归函数的方法，大家可多加练习，掌握这一方法。对于尾递归，Scala
 语言特别增加了一个注释 @tailrec，该注释可以确保程序员写出的程序是正确的尾递归程序，如果由于疏忽大意，写出的不是一个尾递归程序，则编译器会报告一个编译错误，提醒程序员修改自己的代码。

一道面试题

       也许有的读者看了上面的例子后，还是感到不能信服：虽然使用递归会让程序变得简洁易懂，但我用循环也一样可以实现，大不了多几行代码而已，而且我还不用知道什么尾递归，写出的程序就是效率最高的。那我们一起来看看下面这个问题：有趣的零钱兑换问题。题目大致如下：假设某国的货币有若干面值，现给一张大面值的货币要兑换成零钱，问有多少种兑换方式。这个问题经常被各大公司作为一道面试题，不知难倒了多少同学，下面我给出该问题的递归解法，读者们可以试试该问题的非递归解法，看看从程序的易读性，及代码数量上，两者会有多大差别。该问题的递归解法思路很简单：首先确定边界条件，如果要兑换的钱数为
 0，那么返回 1，即只有一种兑换方法：没法兑换。这里要注意的是该问题计算所有的兑换方法，无法兑换也算一种方法。如果零钱种类为 0 或钱数小于 0，没有任何方式进行兑换，返回 0。我们可以把找零的方法分为两类：使用不包含第一枚硬币（零钱）所有的零钱进行找零，使用包含第一枚硬币（零钱）的所有零钱进行找零，两者之和即为所有的找零方式。第一种找零方式总共有 countChange(money,
 coins.tail)种，第二种找零方式等价为对于 money – conins.head进行同样的兑换，则这种兑换方式有 countChange(money
 - coins.head, coins)种，两者之和即为所有的零钱兑换方式。
清单 7. 零钱兑换问题的递归解法







1

2

3

4

5

6

7

8




def

countChange(money:

Int, coins:

List[Int]):

Int =

{ 

  if

(money ==

0)


    1

  else

if 
(coins.size ==

0 
|| money &lt; 0)


    0

  else

    countChange(money,
 coins.tail) + countChange(money - coins.head, coins) 

}









定义

关于闭包有太多种解释，但基本上都很难用一两句解释清楚，下面这句简短的定义是我见过的最精炼且准确的解释了：
A closure is a function that carries an implicit binding to all the variables referenced within it. In other words, the function (or method) encloses a context around the things it references.  


首先，闭包是一个函数，然后，也是最本质的地方：这个函数内部会引用（依赖）到一些变量，这些变量既不是全局的也不是局部的，而是在定义在上下文中的（这种变量被称为“自由变量”，我们会在稍后的例子中看到这种变量），闭包的“神奇”之处是它可以“cache”或者说是持续的“trace”它所引用的这些变量。（从语言实现层面上解释就是：这些变量以及它们引用的对象不会被GC释放）。同样是这件事情，换另一种说法就是：闭包是一个函数，但同时这个函数“背后”还自带了一个“隐式”的上下文保存了函数内部引用到的一些（自由）变量。

第一个例子



第二个例子



对两个例子的补充

上述两个例子的代码都在解释闭包的概念，但是解释的角度不太一样，相对而言第二个例子揭示地更为深刻一些，它揭示闭包会隐式地持续trace（也就是不会被垃圾回收）它所使用的那些自由变量！

每当我们去调用一个闭包时，脑子里一定要意识到：闭包不单单是定义它的那段代码，同时还有一个绑定在它“后面”（隐式的）的持续保持它所引用的所有自用变量的一个“上下文”（“环境”）！

更透彻地理解：闭包产生的根源

某种角度上，我们可以说闭包是函数字面量的一个“衍生品”。函数字面量的存在使得函数的定义与普通变量无异，也就是val 变量名=函数字面量，既然普通变量在赋值时可以引用另一个变量的值，那么定义函数时，在函数字面量里引用其他变量也变成非常自然的事情（而在传统的函数体内是没有办法直接引用函数体外部的变量的），比如,像下面这样定义普通变量是非常常见的：
&lt;code class="hljs cs has-numbering" style="display: block; padding: 0px; color: inherit; box-sizing: border-box; font-family: 'Source Code Pro', monospace;font-size:undefined; white-space: pre; border-radius: 0px; word-wrap: normal; background: transparent;"&gt;&lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;var&lt;/span&gt; a=&lt;span class="hljs-number" style="color: rgb(0, 102, 102); box-sizing: border-box;"&gt;1&lt;/span&gt;;
&lt;span class="hljs-keyword" style="color: rgb(0, 0, 136); box-sizing: border-box;"&gt;var&lt;/span&gt; b=a+&lt;span class="hljs-number" style="color: rgb(0, 102, 102); box-sizing: border-box;"&gt;1&lt;/span&gt;;&lt;/code&gt;&lt;ul class="pre-numbering" style="box-sizing: border-box; position: absolute; width: 50px; top: 0px; left: 0px; margin: 0px; padding: 6px 0px 40px; border-right-width: 1px; border-right-style: solid; border-right-color: rgb(221, 221, 221); list-style: none; text-align: right; background-color: rgb(238, 238, 238);"&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;1&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;2&lt;/li&gt;&lt;/ul&gt;

显然变量b的赋值过程中引用了变量a. 同样的，在函数编程语言里，像下面这样定义函数：
&lt;code class="hljs coffeescript has-numbering" style="display: block; padding: 0px; color: inherit; box-sizing: border-box; font-family: 'Source Code Pro', monospace;font-size:undefined; white-space: pre; border-radius: 0px; word-wrap: normal; background: transparent;"&gt;&lt;span class="hljs-reserved" style="box-sizing: border-box;"&gt;var&lt;/span&gt; a=&lt;span class="hljs-number" style="color: rgb(0, 102, 102); box-sizing: border-box;"&gt;1&lt;/span&gt;;
val &lt;span class="hljs-function" style="box-sizing: border-box;"&gt;&lt;span class="hljs-title" style="box-sizing: border-box;"&gt;b&lt;/span&gt;=&lt;span class="hljs-params" style="color: rgb(102, 0, 102); box-sizing: border-box;"&gt;()&lt;/span&gt;=&gt;&lt;/span&gt;a+&lt;span class="hljs-number" style="color: rgb(0, 102, 102); box-sizing: border-box;"&gt;1&lt;/span&gt;&lt;/code&gt;&lt;ul class="pre-numbering" style="box-sizing: border-box; position: absolute; width: 50px; top: 0px; left: 0px; margin: 0px; padding: 6px 0px 40px; border-right-width: 1px; border-right-style: solid; border-right-color: rgb(221, 221, 221); list-style: none; text-align: right; background-color: rgb(238, 238, 238);"&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;1&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;2&lt;/li&gt;&lt;/ul&gt;

又有何不可呢？这时b成了就成了典型的闭包，它所引用的变量的a是定义在上下文的。

为什么需要闭包

闭包被创造出来显然是因为有场景需要的。一个最为普遍和典型的使用场合是：推迟执行。我们可以把一段代码封装到闭包里，你可以等到“时机”成熟时去执行它。比如：在Spark里，针对RDD的计算任务都要分布到每个节点（准确的说是executor）上并行处理，Spark就需要封装一个闭包，把相关的操作（方法）和需要的变量引入到闭包中分发给节点执行。


主构造函数

       首先，我们必须要非常清晰明确的认识到：主构造函数不是你看到的class后面跟的参数列表，那怎么可能是主构造函数呢？那只是主构造函数的函数列表！那主构造函数的函数体在那里呢？答案是：class body里所有除去字段和方法声明的语句，剩下的一切都是主构造函数的，它们在class实例化时一定会被执行。

所以说，Scala的主构造函数包含这些部分：

         1.The constructor parameters

         2.Methods that are called in the body of the class

        3.Statements and expressions that are executed in the body of the class

请看实例代码中的class Person1以及它的输出。从这个例子上我们可以看出：主构造函数看上去和类的定义已经完全融合在了一起！它的参数列表放到了类名的后面（我们也可以直接叫它类参数列表），它的方法体就是整个类体，实例化一个类时，类体（主构造函数）中所有可行的部分都会被执行，不管是函数调用还是表达式等等，只是对于类的字段和方法声明而言是没有什么可执行的，它们只是声明而已。

主构造函数的参数（类参数）

首先，我们还是要非常清晰明确的认识到：

在主构造函数的参数列表中声明的参数和在类体中声明的变量本质上没有任何不同！基于前面我们对主构造参数的理解，这就像是：在函数的参数列表中声明的参数和方法体中声明的变量本质上没有任何不同一样！

那么接下来的问题就是要讨论不管是主构造函数列表中的参数还是类体中的字段，它们的可见性到底怎样定义？

首先，要说明的是：var/val限定的是变量是否可读写，与可见性无关，即对外可见；public和private声明的才是可见性，只是说：对于类字段和类参数来说，如果没有特别指定，它们总是public的。

规则如下：

     (1).对于var修饰的参数：外部可读/可改写 （实际上是：编译器为该类参数（字段）自动生成了getter和setter）

     (2.)对于val修饰的参数：外部可读/不可改写（实际上是：编译器为该类参数（字段）只生成了getter没有生成setter）

     (3.)对于private var修饰的参数：内部可读/可改写 （编译器不会为私有类参数（字段）自动生成getter和setter）

     (4.)对于private val修饰的参数：内部可读/不可改写 （编译器不会为该类参数（字段）自动生成getter和setter）

     对于主构造函数的参数列表中声明的参数和在类体中声明的变量是否地位一致以及它们的可见性，请看实例代码中的class Person2和Person3以及它们的输出。

类字段的getter/setter

虽然通过val/var,我们已经可以轻松地指定字段的外部可访问性了，但是我们还是要清楚的知道under the hood! 也就是说在scala里getter和setter是怎么写的，虽然绝大多数情况下我们不会再像java那样去手动地编写getter与setter了。

示例代码中的Person4给我们一个很好的示例。虽然参数_age被声明为了私有的，但是通过手动的添加getter和setter，我们还是一样可以在外部改写他们。

示例代码
&lt;code class="language-scala hljs  has-numbering" style="display: block; padding: 0px; box-sizing: border-box; font-family: 'Source Code Pro', monospace;font-size:undefined; white-space: pre; border-radius: 0px; word-wrap: normal; background: transparent;"&gt;&lt;span class="hljs-class" style="box-sizing: border-box;"&gt;&lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;object&lt;/span&gt; &lt;span class="hljs-title" style="box-sizing: border-box;"&gt;PrimaryConstructorDemo&lt;/span&gt; {&lt;/span&gt;
    &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;//Person1 is to show: primary constructor consists of not only class args list but also all runnable part in class body.&lt;/span&gt;
    &lt;span class="hljs-class" style="box-sizing: border-box;"&gt;&lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;class&lt;/span&gt; &lt;span class="hljs-title" style="box-sizing: border-box;"&gt;Person1&lt;/span&gt;&lt;span class="hljs-params" style="box-sizing: border-box;"&gt;(var firstName: String, var lastName: String)&lt;/span&gt; {&lt;/span&gt;
        println(&lt;span class="hljs-string" style="box-sizing: border-box;"&gt;"the constructor begins"&lt;/span&gt;)
        &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;// some class fields&lt;/span&gt;
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;private&lt;/span&gt; &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;val&lt;/span&gt; HOME = &lt;span class="hljs-string" style="box-sizing: border-box;"&gt;"/root"&lt;/span&gt;
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;var&lt;/span&gt; age = &lt;span class="hljs-number" style="box-sizing: border-box;"&gt;30&lt;/span&gt;
        &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;// some methods&lt;/span&gt;
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;override&lt;/span&gt; &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;def&lt;/span&gt; toString = s&lt;span class="hljs-string" style="box-sizing: border-box;"&gt;"$firstName $lastName is $age years old"&lt;/span&gt;
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;def&lt;/span&gt; printHome { println(s&lt;span class="hljs-string" style="box-sizing: border-box;"&gt;"HOME = $HOME"&lt;/span&gt;) }
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;def&lt;/span&gt; printFullName { println(&lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;this&lt;/span&gt;) } &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;// uses toString&lt;/span&gt;
        printHome
        printFullName
        println(&lt;span class="hljs-string" style="box-sizing: border-box;"&gt;"still in the constructor"&lt;/span&gt;)
    }

    &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;// Person2 is to show: the visibility of class fields&lt;/span&gt;
    &lt;span class="hljs-class" style="box-sizing: border-box;"&gt;&lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;class&lt;/span&gt; &lt;span class="hljs-title" style="box-sizing: border-box;"&gt;Person2&lt;/span&gt; {&lt;/span&gt;
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;var&lt;/span&gt; age = &lt;span class="hljs-number" style="box-sizing: border-box;"&gt;30&lt;/span&gt;
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;val&lt;/span&gt; gender = &lt;span class="hljs-string" style="box-sizing: border-box;"&gt;"male"&lt;/span&gt;
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;private&lt;/span&gt; &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;val&lt;/span&gt; healthy = &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;false&lt;/span&gt;
    }

    &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;// Person3 is to show: the visibility of primary constructor args&lt;/span&gt;
    &lt;span class="hljs-class" style="box-sizing: border-box;"&gt;&lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;class&lt;/span&gt; &lt;span class="hljs-title" style="box-sizing: border-box;"&gt;Person3&lt;/span&gt;&lt;span class="hljs-params" style="box-sizing: border-box;"&gt;(var age:Int,val gender:String,private val healthy:Boolean)&lt;/span&gt;&lt;/span&gt;

    &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;// Person4 is to show: change visibility for primary constructor args&lt;/span&gt;
    &lt;span class="hljs-class" style="box-sizing: border-box;"&gt;&lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;class&lt;/span&gt; &lt;span class="hljs-title" style="box-sizing: border-box;"&gt;Person4&lt;/span&gt;&lt;span class="hljs-params" style="box-sizing: border-box;"&gt;(private var _age:Int)&lt;/span&gt; {&lt;/span&gt;
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;def&lt;/span&gt; age = _age &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;// this is getter&lt;/span&gt;
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;def&lt;/span&gt; age_=(newAge: Int) = _age = newAge &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;//this is setter&lt;/span&gt;
    }

    &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;def&lt;/span&gt; main(args: Array[String]) {
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;val&lt;/span&gt; p1 = &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;new&lt;/span&gt; Person1(&lt;span class="hljs-string" style="box-sizing: border-box;"&gt;"Tome"&lt;/span&gt;,&lt;span class="hljs-string" style="box-sizing: border-box;"&gt;"White"&lt;/span&gt;)
        println(&lt;span class="hljs-string" style="box-sizing: border-box;"&gt;"---------------------------------"&lt;/span&gt;)
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;val&lt;/span&gt; p2 = &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;new&lt;/span&gt; Person2
        println(p2.age)
        p2.age = &lt;span class="hljs-number" style="box-sizing: border-box;"&gt;40&lt;/span&gt;;
        println(p2.age)
        println(p2.gender)
        &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;//error, no setter for gender.&lt;/span&gt;
        &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;//p2.gender = false&lt;/span&gt;
        &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;//error, invisible out of class.&lt;/span&gt;
        &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;//println(p2.healthy)&lt;/span&gt;
        println(&lt;span class="hljs-string" style="box-sizing: border-box;"&gt;"---------------------------------"&lt;/span&gt;)
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;val&lt;/span&gt; p3 = &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;new&lt;/span&gt; Person3(&lt;span class="hljs-number" style="box-sizing: border-box;"&gt;30&lt;/span&gt;,&lt;span class="hljs-string" style="box-sizing: border-box;"&gt;"male"&lt;/span&gt;,&lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;false&lt;/span&gt;)
        println(p3.age)
        p3.age = &lt;span class="hljs-number" style="box-sizing: border-box;"&gt;40&lt;/span&gt;;
        println(p3.age)
        println(p3.gender)
        &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;//error, no setter for gender.&lt;/span&gt;
        &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;//p3.gender = false&lt;/span&gt;
        &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;//error, invisible out of class.&lt;/span&gt;
        &lt;span class="hljs-comment" style="box-sizing: border-box;"&gt;//println(p3.healthy)&lt;/span&gt;
        println(&lt;span class="hljs-string" style="box-sizing: border-box;"&gt;"---------------------------------"&lt;/span&gt;)
        &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;val&lt;/span&gt; p4 = &lt;span class="hljs-keyword" style="box-sizing: border-box;"&gt;new&lt;/span&gt; Person4(&lt;span class="hljs-number" style="box-sizing: border-box;"&gt;30&lt;/span&gt;)
        println(p4.age)
        p4.age = &lt;span class="hljs-number" style="box-sizing: border-box;"&gt;40&lt;/span&gt;
        println(p4.age)
    }
}&lt;/code&gt;&lt;ul class="pre-numbering" style="box-sizing: border-box; position: absolute; width: 50px; top: 0px; left: 0px; margin: 0px; padding: 6px 0px 40px; border-right-width: 1px; border-right-style: solid; border-right-color: rgb(221, 221, 221); list-style: none; text-align: right; background-color: rgb(238, 238, 238);"&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;1&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;2&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;3&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;4&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;5&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;6&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;7&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;8&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;9&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;10&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;11&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;12&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;13&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;14&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;15&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;16&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;17&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;18&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;19&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;20&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;21&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;22&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;23&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;24&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;25&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;26&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;27&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;28&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;29&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;30&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;31&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;32&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;33&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;34&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;35&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;36&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;37&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;38&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;39&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;40&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;41&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;42&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;43&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;44&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;45&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;46&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;47&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;48&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;49&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;50&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;51&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;52&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;53&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;54&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;55&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;56&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;57&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;58&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;59&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;60&lt;/li&gt;&lt;li style="box-sizing: border-box; padding: 0px 5px;"&gt;61&lt;/li&gt;&lt;/ul&gt;

程序输出：
&lt;code class="hljs asciidoc has-numbering" style="display: block; padding: 0px; box-sizing: border-box; font-family: 'Source Code Pro', monospace;font-size:undefined; white-space: pre; border-radius: 0px; word-wrap: normal; background-image: initial; background-attachment: initial; background-color: transparent; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;"&gt;the constructor begins
HOME = /root
Tome White is 30 years old
&lt;span class="hljs-header" style="box-sizing: border-box;"&gt;still in the constructor
---------------------------------&lt;/span&gt;
30
40
&lt;span class="hljs-header" style="box-sizing: border-box;"&gt;male
---------------------------------&lt;/span&gt;
30
40
&lt;span class="hljs-header" style="box-sizing: border-box;"&gt;male
---------------------------------&lt;/span&gt;
30&lt;/code&gt;


Java 并发性支持

在 Java 平台诞生之初，并发性支持就是它的一个特性，线程和同步的实现为它提供了超越其他竞争语言的优势。Scala 基于 Java 并在 JVM 上运行，能够直接访问所有 Java 运行时（包括所有并发性支持）。所以在分析 Scala 特性之前，我首先会快速回顾一下 Java 语言已经提供的功能。

Java 线程基础

在 Java 编程过程中创建和使用线程非常容易。它们由 java.lang.Thread 类表示，线程要执行的代码为 java.lang.Runnable 实例的形式。如果需要的话，可以在应用程序中创建大量线程，您甚至可以创建数千个线程。在有多个核心时，JVM
 使用它们来并发执行多个线程；超出核心数量的线程会共享这些核心。


Java 5：并发性的转折点

Java 从一开始就包含对线程和同步的支持。但在线程间共享数据的最初规范不够完善，这带来了 Java 5 的 Java 语言更新中的重大变化 (JSR-133)。Java Language Specification for Java 5 更正并规范化了 synchronized 和 volatile 操作。该规范还规定不变的对象如何使用多线程。（基本上讲，只要在执行构造函数时不允许引用
 “转义”，不变的对象始终是线程安全的。）以前，线程间的交互通常需要使用阻塞的 synchronized 操作。这些更改支持使用 volatile 在线程间执行非阻塞协调。因此，在
 Java 5 中添加了新的并发集合类来支持非阻塞操作 — 这与早期仅支持阻塞的线程安全方法相比是一项重大改进。


线程操作的协调难以让人理解。只要从程序的角度让所有内容保持一致，Java 编译器和 JVM 就不会对您代码中的操作重新排序，这使得问题变得更加复杂。例如：如果两个相加操作使用了不同的变量，编译器或 JVM 可以安装与指定的顺序相反的顺序执行这些操作，只要程序不在两个操作都完成之前使用两个变量的总数。这种重新排序操作的灵活性有助于提高 Java 性能，但一致性只被允许应用在单个线程中。硬件也有可能带来线程问题。现代系统使用了多种缓存内存级别，一般来讲，不是系统中的所有核心都能同样看到这些缓存。当某个核心修改内存中的一个值时，其他核心可能不会立即看到此更改。

由于这些问题，在一个线程使用另一个线程修改的数据时，您必须显式地控制线程交互方式。Java 使用了特殊的操作来提供这种控制，在不同线程看到的数据视图中建立顺序。基本操作是，线程使用 synchronized 关键字来访问一个对象。当某个线程在一个对象上保持同步时，该线程将会获得此对象所独有的一个锁的独占访问。如果另一个线程已持有该锁，等待获取该锁的线程必须等待，或者被阻塞，直到该锁被释放。当该线程在一个 synchronized 代码块内恢复执行时，Java
 会保证该线程可以 “看到了” 以前持有同一个锁的其他线程写入的所有数据，但只是这些线程通过离开自己的 synchronized 锁来释放该锁之前写入的数据。这种保证既适用于编译器或 JVM 所执行的操作的重新排序，也适用于硬件内存缓存。一个 synchronized 块的内部是您代码中的一个稳定性孤岛，其中的线程可依次安全地执行、交互和共享信息。

在变量上对 volatile 关键字的使用，为线程间的安全交互提供了一种稍微较弱的形式。synchronized 关键字可确保在您获取该锁时可以看到其他线程的存储，而且在您之后，获取该锁的其他线程也会看到您的存储。volatile 关键字将这一保证分解为两个不同的部分。如果一个线程向volatile 变量写入数据，那么首先将会擦除它在这之前写入的数据。如果某个线程读取该变量，那么该线程不仅会看到写入该变量的值，还会看到写入的线程所写入的其他所有值。所以读取一个 volatile 变量会提供与输入 一个 synchronized 块相同的内存保证，而且写入一个volatile 变量会提供与离开 一个 synchronized 块相同的内存保证。但二者之间有很大的差别：volatile 变量的读取或写入绝不会受阻塞。

抽象 Java 并发性

同步很有用，而且许多多线程应用程序都是在 Java 中仅使用基本的 synchronized 块开发出来的。但协调线程可能很麻烦，尤其是在处理许多线程和许多块的时候。确保线程仅在安全的方式下交互并 避免潜在的死锁（两个或更多线程等待对方释放锁之后才能继续执行），这很困难。支持并发性而不直接处理线程和锁的抽象，这为开发人员提供了处理常见用例的更好方法。

java.util.concurrent 分层结构包含一些集合变形，它们支持并发访问、针对原子操作的包装器类，以及同步原语。这些类中的许多都是为支持非阻塞访问而设计的，这避免了死锁的问题，而且实现了更高效的线程。这些类使得定义和控制线程之间的交互变得更容易，但他们仍然面临着基本线程模型的一些复杂性。

java.util.concurrent 包中的一对抽象，支持采用一种更加分离的方法来处理并发性：Future&lt;T&gt; 接口、Executor 和ExecutorService 接口。这些相关的接口进而成为了对
 Java 并发性支持的许多 Scala 和 Akka 扩展的基础，所以更详细地了解这些接口和它们的实现是值得的。

Future&lt;T&gt; 是一个 T 类型的值的持有者，但奇怪的是该值一般在创建 Future 之后才能使用。正确执行一个同步操作后，才会获得该值。收到Future 的线程可调用方法来：

查看该值是否可用等待该值变为可用在该值可用时获取它如果不再需要该值，则取消该操作

Future 的具体实现结构支持处理异步操作的不同方式。

Executor 是一种围绕某个执行任务的东西的抽象。这个 “东西” 最终将是一个线程，但该接口隐藏了该线程处理执行的细节。Executor 本身的适用性有限，ExecutorService 子接口提供了管理终止的扩展方法，并为任务的结果生成了 Future。Executor 的所有标准实现还会实现ExecutorService，所以实际上，您可以忽略根接口。

线程是相对重量级的资源，而且与分配并丢弃它们相比，重用它们更有意义。ExecutorService 简化了线程间的工作共享，还支持自动重用线程，实现了更轻松的编程和更高的性能。ExecutorService 的 ThreadPoolExecutor 实现管理着一个执行任务的线程池。

应用 Java 并发性

并发性的实际应用常常涉及到需要与您的主要处理逻辑独立的外部交互的任务（与用户、存储或其他系统的交互）。这类应用很难浓缩为一个简单的示例，所以在演示并发性的时候，人们通常会使用简单的计算密集型任务，比如数学计算或排序。我将使用一个类似的示例。

任务是找到离一个未知的输入最近的已知单词，其中的最近 是按照Levenshtein 距离 来定义的：将输入转换为已知的单词所需的最少的字符增加、删除或更改次数。我使用的代码基于 Wikipedia 上的 Levenshtein
 距离 文章中的一个示例，该示例计算了每个已知单词的 Levenshtein 距离，并返回最佳匹配值（或者如果多个已知的单词拥有相同的距离，那么返回结果是不确定的）。

清单 1 给出了计算 Levenshtein 距离的 Java 代码。该计算生成一个矩阵，将行和列与两个对比的文本的大小进行匹配，在每个维度上加 1。为了提高效率，此实现使用了一对大小与目标文本相同的数组来表示矩阵的连续行，将这些数组包装在每个循环中，因为我只需要上一行的值就可以计算下一行。

清单 1. Java 中的 Levenshtein 距离计算








1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36




/**

 *
 Calculate edit distance from targetText to known word.

 *

 *
 @param word known word

 *
 @param v0 int array of length targetText.length() + 1

 *
 @param v1 int array of length targetText.length() + 1

 *
 @return distance

 */

private

int 
editDistance(String word, int[]
 v0, int[]
 v1) {

 

    //
 initialize v0 (prior row of distances) as edit distance for empty 'word'

    for

(int

i = 0;
 i &lt; v0.length; i++) {

        v0[i]
 = i;

    }

 

    //
 calculate updated v0 (current row distances) from the previous row v0

    for

(int

i = 0;
 i &lt; word.length(); i++) {

 

        //
 first element of v1 = delete (i+1) chars from target to match empty 'word'

        v1[0]
 = i + 1;

 

        //
 use formula to fill in the rest of the row

        for

(int

j = 0;
 j &lt; targetText.length(); j++) {

            int

cost = (word.charAt(i) == targetText.charAt(j)) ? 0

: 1;

            v1[j
 + 1]
 = minimum(v1[j] + 1,
 v0[j + 1]
 + 1,
 v0[j] + cost);

        }

 

        //
 swap v1 (current row) and v0 (previous row) for next iteration

        int[]
 hold = v0;

        v0
 = v1;

        v1
 = hold;

    }

 

    //
 return final value representing best edit distance

    return

v0[targetText.length()];

}









如果有大量已知词汇要与未知的输入进行比较，而且您在一个多核系统上运行，那么您可以使用并发性来加速处理：将已知单词的集合分解为多个块，将每个块作为一个独立任务来处理。通过更改每个块中的单词数量，您可以轻松地更改任务分解的粒度，从而了解它们对总体性能的影响。清单 2 给出了分块计算的 Java 代码，摘自 示例代码 中的 ThreadPoolDistance 类。清单
 2 使用一个标准的 ExecutorService，将线程数量设置为可用的处理器数量。

清单 2. 在 Java 中通过多个线程来执行分块的距离计算








1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

79

80

81

82

83

84

85

86




private

final 
ExecutorService threadPool;

private

final 
String[] knownWords;

private

final 
int 
blockSize;

 

public

ThreadPoolDistance(String[] words, int

block) {

    threadPool
 = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors());

    knownWords
 = words;

    blockSize
 = block;

}

 

public

DistancePair bestMatch(String target) {

 

    //
 build a list of tasks for matching to ranges of known words

    List&lt;DistanceTask&gt;
 tasks = new

ArrayList&lt;DistanceTask&gt;();

 

    int

size = 0;

    for

(int

base = 0;
 base &lt; knownWords.length; base += size) {

        size
 = Math.min(blockSize, knownWords.length - base);

        tasks.add(new

DistanceTask(target, base, size));

    }

    DistancePair
 best;

    try

{

 

        //
 pass the list of tasks to the executor, getting back list of futures

        List&lt;Future&lt;DistancePair&gt;&gt;
 results = threadPool.invokeAll(tasks);

 

        //
 find the best result, waiting for each future to complete

        best
 = DistancePair.WORST_CASE;

        for

(Future&lt;DistancePair&gt; future: results) {

            DistancePair
 result = future.get();

            best
 = DistancePair.best(best, result);

        }

 

    }
catch

(InterruptedException e) {

        throw

new 
RuntimeException(e);

    }
catch

(ExecutionException e) {

        throw

new 
RuntimeException(e);

    }

    return

best;

}

 

/**

 *
 Shortest distance task implementation using Callable.

 */

public

class 
DistanceTask implements

Callable&lt;DistancePair&gt;

{

    private

final 
String targetText;

    private

final 
int 
startOffset;

    private

final 
int 
compareCount;

 

    public

DistanceTask(String target, int

offset, int

count) {

        targetText
 = target;

        startOffset
 = offset;

        compareCount
 = count;

    }

 

    private

int 
editDistance(String word, int[]
 v0, int[]
 v1) {

        ...

    }

 

    /*
 (non-Javadoc)

     *
 @see java.util.concurrent.Callable#call()

     */

    @Override

    public

DistancePair call() throws

Exception {

 

        //
 directly compare distances for comparison words in range

        int[]
 v0 = new

int[targetText.length()
 + 1];

        int[]
 v1 = new

int[targetText.length()
 + 1];

        int

bestIndex = -1;

        int

bestDistance = Integer.MAX_VALUE;

        boolean

single = false;

        for

(int

i = 0;
 i &lt; compareCount; i++) {

            int

distance = editDistance(knownWords[i + startOffset], v0, v1);

            if

(bestDistance &gt; distance) {

                bestDistance
 = distance;

                bestIndex
 = i + startOffset;

                single
 = true;

            }
else

if 
(bestDistance == distance) {

                single
 = false;

            }

        }

        return

single ? new

DistancePair(bestDistance, knownWords[bestIndex]) :

                new

DistancePair(bestDistance);

    }

}









清单 2 中的 bestMatch() 方法构造一个 DistanceTask 距离列表，然后将该列表传递给 ExecutorService。这种对 ExecutorService 的调用形式将会接受一个 Collection&lt;?
 extends Callable&lt;T&gt;&gt; 类型的参数，该参数表示要执行的任务。该调用返回一个 Future&lt;T&gt; 列表，用它来表示执行的结果。ExecutorService 使用在每个任务上调用 call() 方法所返回的值，异步填写这些结果。在本例中，T 类型为DistancePair—
 一个表示距离和匹配的单词的简单的值对象，或者在没有找到惟一匹配值时近表示距离。

bestMatch() 方法中执行的原始线程依次等待每个 Future 完成，累积最佳的结果并在完成时返回它。通过多个线程来处理 DistanceTask 的执行，原始线程只需等待一小部分结果。剩余结果可与原始线程等待的结果并发地完成。

并发性性能

要充分利用系统上可用的处理器数量，必须为 ExecutorService 配置至少与处理器一样多的线程。您还必须将至少与处理器一样多的任务传递给ExecutorService 来执行。实际上，您或许希望拥有比处理器多得多的任务，以实现最佳的性能。这样，处理器就会繁忙地处理一个接一个的任务，近在最后才空闲下来。但是因为涉及到开销（在创建任务和
 future 的过程中，在任务之间切换线程的过程中，以及最终返回任务的结果时），您必须保持任务足够大，以便开销是按比例减小的。

图 1 展示了我在使用 Oracle 的 Java 7 for 64-bit Linux? 的四核 AMD 系统上运行测试代码时测量的不同任务数量的性能。每个输入单词依次与 12,564 个已知单词相比较，每个任务在一定范围的已知单词中找到最佳的匹配值。全部 933 个拼写错误的输入单词会重复运行，每轮运行之间会暂停片刻供 JVM 处理，该图中使用了 10 轮运行后的最佳时间。从图 1 中可以看出，每秒的输入单词性能在合理的块大小范围内（基本来讲，从 256 到大于 1,024）看起来是合理的，只有在任务变得非常小或非常大时，性能才会极速下降。对于块大小
 16,384，最后的值近创建了一个任务，所以显示了单线程性能。

图 1. ThreadPoolDistance 性能



Fork-Join

Java 7 引入了 ExecutorService 的另一种实现：ForkJoinPool 类。ForkJoinPool 是为高效处理可反复分解为子任务的任务而设计的，它使用 RecursiveAction 类（在任务未生成结果时）或 RecursiveTask&lt;T&gt; 类（在任务具有一个 T 类型的结果时）来处理任务。RecursiveTask&lt;T&gt; 提供了一种合并子任务结果的便捷方式，如清单
 3 所示。

清单 3. RecursiveTask&lt;DistancePair&gt; 示例








1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73




private

ForkJoinPool threadPool = new

ForkJoinPool();

 

private

final 
String[] knownWords;

 

private

final 
int 
blockSize;

 

public

ForkJoinDistance(String[] words, int

block) {

    knownWords
 = words;

    blockSize
 = block;

}

 

public

DistancePair bestMatch(String target) {

    return

threadPool.invoke(new

DistanceTask(target, 0,
 knownWords.length, knownWords));

}

 

/**

 *
 Shortest distance task implementation using RecursiveTask.

 */

public

class 
DistanceTask extends

RecursiveTask&lt;DistancePair&gt;

{

    private

final 
String compareText;

    private

final 
int 
startOffset;

    private

final 
int 
compareCount;

    private

final 
String[] matchWords;

 

    public

DistanceTask(String from, int

offset, int

count, String[] words) {

        compareText
 = from;

        startOffset
 = offset;

        compareCount
 = count;

        matchWords
 = words;

    }

 

    private

int 
editDistance(int

index, int[]
 v0, int[]
 v1) {

        ...

    }

 

    /*
 (non-Javadoc)

     *
 @see java.util.concurrent.RecursiveTask#compute()

     */

    @Override

    protected

DistancePair compute() {

        if

(compareCount &gt; blockSize) {

 

            //
 split range in half and find best result from bests in each half of range

            int

half = compareCount / 2;

            DistanceTask
 t1 = new

DistanceTask(compareText, startOffset, half, matchWords);

            t1.fork();

            DistanceTask
 t2 = new

DistanceTask(compareText, startOffset + half,

                compareCount
 - half, matchWords);

            DistancePair
 p2 = t2.compute();

            return

DistancePair.best(p2, t1.join());

        }

 

        //
 directly compare distances for comparison words in range

        int[]
 v0 = new

int[compareText.length()
 + 1];

        int[]
 v1 = new

int[compareText.length()
 + 1];

        int

bestIndex = -1;

        int

bestDistance = Integer.MAX_VALUE;

        boolean

single = false;

        for

(int

i = 0;
 i &lt; compareCount; i++) {

            int

distance = editDistance(i + startOffset, v0, v1);

            if

(bestDistance &gt; distance) {

                bestDistance
 = distance;

                bestIndex
 = i + startOffset;

                single
 = true;

            }
else

if 
(bestDistance == distance) {

                single
 = false;

            }

        }

        return

single ? new

DistancePair(bestDistance, knownWords[bestIndex]) :

            new

DistancePair(bestDistance);

    }

}









图 2 显示了清单 3 中的 ForkJoin 代码与 清单
 2 中的 ThreadPool 代码的性能对比。ForkJoin 代码在所有块大小中稳定得多，仅在您只有单个块（意味着执行是单线程的）时性能会显著下降。标准的 ThreadPool 代码仅在块大小为
 256 和 1,024 时会表现出更好的性能。

图 2. ThreadPoolDistance 与 ForkJoinDistance 的性能对比



这些结果表明，如果可调节应用程序中的任务大小来实现最佳的性能，那么使用标准 ThreadPool 比 ForkJoin 更好。但请注意，ThreadPool的
 “最佳性能点” 取决于具体任务、可用处理器数量以及您系统的其他因素。一般而言，ForkJoin 以最小的调优需求带来了优秀的性能，所以最好尽可能地使用它。



Scala 并发性基础

Scala 通过许多方式扩展了 Java 编程语言和运行时，其中包括添加更多、更轻松的处理并发性的方式。对于初学者而言，Future&lt;T&gt; 的 Scala 版本比 Java 版本灵活得多。您可以直接从代码块中创建 future，可向 future 附加回调来处理这些
 future 的完成。清单 4 显示了 Scala future 的一些使用示例。该代码首先定义了 futureInt() 方法，以便按需提供 Future&lt;Int&gt;，然后通过三种不同的方式来使用
 future。

清单 4. Scala Future&lt;T&gt; 示例代码








1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31




import

ExecutionContext.Implicits.global

 

val
 lastInteger = new

AtomicInteger

def
 futureInt() = future {

  Thread
 sleep 2000

  lastInteger
 incrementAndGet

}

 

//
 use callbacks for completion of futures

val
 a1 = futureInt

val
 a2 = futureInt

a1.onSuccess
 {

    case

i1 =&gt; {

      a2.onSuccess
 {

        case

i2 =&gt; println("Sum
 of values is " 
+ (i1 + i2))

      }

    }

}

Thread
 sleep 3000

 

//
 use for construct to extract values when futures complete

val
 b1 = futureInt

val
 b2 = futureInt

for

(i1 &lt;- b1; i2 &lt;- b2) yield println("Sum
 of values is " 
+ (i1 + i2))

Thread
 sleep 3000

 

//
 wait directly for completion of futures

val
 c1 = futureInt

val
 c2 = futureInt

println("Sum
 of values is " 
+ (Await.result(c1, Duration.Inf) +

  Await.result(c2,
 Duration.Inf)))









清单 4 中的第一个示例将回调闭包附加到一对 future 上，以便在两个 future 都完成时，将两个结果值的和打印到控制台上。回调是按照创建它们的顺序直接嵌套在 future 上，但是，即使更改顺序，它们也同样有效。如果在您附加回调时 future 已完成，该回调仍会运行，但无法保证它会立即运行。原始执行线程会在 Thread
 sleep 3000 行上暂停，以便在进入下一个示例之前完成 future。

第二个示例演示了使用 Scala for comprehension 从 future 中异步提取值，然后直接在表达式中使用它们。for comprehension
 是一种 Scala 结构，可用于简洁地表达复杂的操作组合（map、filter、flatMap 和 foreach）。它一般与各种形式的集合结合使用，但
 Scala future 实现了相同的单值方法来访问集合值。所以可以使用 future 作为一种特殊的集合，一种包含最多一个值（可能甚至在未来某个时刻之前之后才包含该值）的集合。在这种情况下，for 语句要求获取 future 的结果，并在表达式中使用这些结果值。在幕后，这种技术会生成与第一个示例完全相同的代码，但以线性代码的形式编写它会得到更容易理解的更简单的表达式。和第一个示例一样，原始执行线程会暂停，以便在进入下一个示例之前完成
 future。

第三个示例使用阻塞等待来获取 future 的结果。这与 Java future 的工作原理相同，但在 Scala 中，一个获取最大等待时间参数的特殊Await.result() 方法调用会让阻塞等待变得更为明显。

清单 4 中的代码没有显式地将 future 传递给 ExecutorService 或等效的对象，所以如果没有使用过
 Scala，那么您可能想知道 future 内部的代码是如何执行的。答案取决于 清单 4 中最上面一行：import
 ExecutionContext.Implicits.global。Scala API 常常为代码块中频繁重用的参数使用 implicit 值。future
 { } 结构要求 ExecutionContext 以隐式参数的形式提供。这个 ExecutionContext 是
 JavaExecutorService 的一个 Scala 包装器，以相同方式用于使用一个或多个托管线程来执行任务。

除了 future 的这些基本操作之外，Scala 还提供了一种方式将任何集合转换为使用并行编程的集合。将集合转换为并行格式后，您在集合上执行的任何标准的 Scala 集合操作（比如 map、filter 或 fold）都会自动地尽可能并行完成。（本文稍后会在 清单
 7 中提供一个相关示例，该示例使用 Scala 查找一个单词的最佳匹配值。）

错误处理

Java 和 Scala 中的 future 都必须解决错误处理的问题。在 Java 中，截至 Java 7，future 可抛出一个 ExecutionException 作为返回结果的替代方案。应用程序可针对具体的失败类型而定义自己的 ExecutionException 子类，或者可连锁异常来传递详细信息，但这限制了灵活性。

Scala future 提供了更灵活的错误处理。您可以通过两种方式完成 Scala future：成功时提供一个结果值（假设要求一个结果值），或者在失败时提供一个关联的 Throwable。您也可以采用多种方式处理 future 的完成。在 清单
 4 中，onSuccess 方法用于附加回调来处理 future 的成功完成。您还可以使用 onComplete 来处理任何形式的完成（它将结果或
 throwable 包装在一个 Try 中来适应两种情况），或者使用 onFailure 来专门处理错误结果。Scala
 future 的这种灵活性扩展到了您可以使用 future 执行的所有操作，所以您可以将错误处理直接集成到代码中。

这个 Scala Future&lt;T&gt; 还有一个紧密相关的 Promise&lt;T&gt; 类。future
 是一个结果的持有者，该结果在某个时刻可能可用（或不可用 — 无法内在地确保一个 future 将完成）。future 完成后，结果是固定的，不会发生改变。promise 是这个相同契约的另一端：结果的一个一次性、可分配的持有者，具有结果值或 throwable 的形式。可从 promise 获取 future，在 promise 上设置了结果后，就可以在该 future 上设置此结果。

应用 Scala 并发性

现在您已熟悉一些基本的 Scala 并发性概念，是时候来了解一下解决 Levenshtein 距离问题的代码了。清单 5 显示了 Levenshtein 距离计算的一个比较符合语言习惯的 Scala 实现，该代码基本上与 清单
 1 中的 Java 代码类似，但采用了函数风格。

清单 5. Scala 中的 Levenshtein 距离计算








1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37




val
 limit = targetText.length

/**
 Calculate edit distance from targetText to known word.

  *

  *
 @param word known word

  *
 @param v0 int array of length targetText.length + 1

  *
 @param v1 int array of length targetText.length + 1

  *
 @return distance

  */

def
 editDistance(word: String, v0: Array[Int], v1: Array[Int]) = {

 

  val
 length = word.length

 

  @tailrec

  def
 distanceByRow(rnum: Int, r0: Array[Int], r1: Array[Int]): Int = {

    if

(rnum &gt;= length) r0(limit)

    else

{

 

      //
 first element of r1 = delete (i+1) chars from target to match empty 'word'

      r1(0)
 = rnum + 1

 

      //
 use formula to fill in the rest of the row

      for

(j &lt;- 0

until limit) {

        val
 cost = if

(word(rnum) == targetText(j)) 0

else 
1

        r1(j
 + 1)
 = min(r1(j) + 1,
 r0(j + 1)
 + 1,
 r0(j) + cost);

      }

 

      //
 recurse with arrays swapped for next row

      distanceByRow(rnum
 + 1,
 r1, r0)

    }

  }

 

  //
 initialize v0 (prior row of distances) as edit distance for empty 'word'

  for

(i &lt;- 0

to limit) v0(i) = i

 

  //
 recursively process rows matching characters in word being compared to find best

  distanceByRow(0,
 v0, v1)

}









清单 5 中的代码对每个行值计算使用了尾部递归 distanceByRow() 方法。此方法首先检查计算了多少行，如果该数字与检查的单词中的字符数匹配，则返回结果距离。否则会计算新的行值，然后递归地调用自身来计算下一行（将两个行数组包装在该进程中，以便正确地传递新的最新的行值）。Scala
 将尾部递归方法转换为与 Java while 循环等效的代码，所以保留了与 Java 代码的相似性。

但是，此代码与 Java 代码之间有一个重大区别。清单 5 中的 for comprehension
 使用了闭包。闭包并不总是得到了当前 JVM 的高效处理（参阅Why is using for/foreach on a Range slow?，了解有关的详细信息），所以它们在该计算的最里层循环上增加了大量开销。如上所述，清单
 5 中的代码的运行速度没有 Java 版本那么快。清单 6 重写了代码，将 for comprehension 替换为添加的尾部递归方法。这个版本要详细得多，但执行效率与 Java 版本相当。

清单 6. 为提升性能而重新构造的计算代码








1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50




val
 limit = targetText.length

 

/**
 Calculate edit distance from targetText to known word.

  *

  *
 @param word known word

  *
 @param v0 int array of length targetText.length + 1

  *
 @param v1 int array of length targetText.length + 1

  *
 @return distance

  */

def
 editDistance(word: String, v0: Array[Int], v1: Array[Int]) = {

 

  val
 length = word.length

 

  @tailrec

  def
 distanceByRow(row: Int, r0: Array[Int], r1: Array[Int]): Int = {

    if

(row &gt;= length) r0(limit)

    else

{

 

      //
 first element of v1 = delete (i+1) chars from target to match empty 'word'

      r1(0)
 = row + 1

 

      //
 use formula recursively to fill in the rest of the row

      @tailrec

      def
 distanceByColumn(col: Int): Unit = {

        if

(col &lt; limit) {

          val
 cost = if

(word(row) == targetText(col)) 0

else 
1

          r1(col
 + 1)
 = min(r1(col) + 1,
 r0(col + 1)
 + 1,
 r0(col) + cost)

          distanceByColumn(col
 + 1)

        }

      }

      distanceByColumn(0)

 

      //
 recurse with arrays swapped for next row

      distanceByRow(row
 + 1,
 r1, r0)

    }

  }

 

  //
 initialize v0 (prior row of distances) as edit distance for empty 'word'

  @tailrec

  def
 initArray(index: Int): Unit = {

    if

(index &lt;= limit) {

      v0(index)
 = index

      initArray(index
 + 1)

    }

  }

  initArray(0)

 

  //
 recursively process rows matching characters in word being compared to find best

  distanceByRow(0,
 v0, v1)

}









清单 7 给出的 Scala 代码执行了与 清单 2 中的 Java 代码相同的阻塞的距离计算。bestMatch() 方法找到由 Matcher 类实例处理的特定单词块中与目标文本最匹配的单词，使用尾部递归 best() 方法来扫描单词。*Distance 类创建多个 Matcher 实例，每个对应一个单词块，然后协调匹配结果的执行和组合。

清单 7. Scala 中使用多个线程的一次阻塞距离计算








1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64




class

Matcher(words: Array[String]) {

 

  def
 bestMatch(targetText: String) = {

 

    val
 limit = targetText.length

    val
 v0 = new

Array[Int](limit + 1)

    val
 v1 = new

Array[Int](limit + 1)

 

    def
 editDistance(word: String, v0: Array[Int], v1: Array[Int]) = {

      ...

    }

 

    @tailrec

    /**
 Scan all known words in range to find best match.

      * 


      *
 @param index next word index

      *
 @param bestDist minimum distance found so far

      *
 @param bestMatch unique word at minimum distance, or None if not unique

      *
 @return best match

      */

    def
 best(index: Int, bestDist: Int, bestMatch: Option[String]): DistancePair =

      if

(index &lt; words.length) {

        val
 newDist = editDistance(words(index), v0, v1)

        val
 next = index + 1

        if

(newDist &lt; bestDist) best(next, newDist, Some(words(index)))

        else

if 
(newDist == bestDist) best(next, bestDist, None)

        else

best(next, bestDist, bestMatch)

      }
else

DistancePair(bestDist, bestMatch)

 

    best(0,
 Int.MaxValue, None)

  }

}

 

class

ParallelCollectionDistance(words: Array[String], size: Int) extends

TimingTestBase {

 

  val
 matchers = words.grouped(size).map(l =&gt; new

Matcher(l)).toList

 

  def
 shutdown = {}

 

  def
 blockSize = size

 

  /**
 Find best result across all matchers, using parallel collection. */

  def
 bestMatch(target: String) = {

    matchers.par.map(m
 =&gt; m.bestMatch(target)).

      foldLeft(DistancePair.worstMatch)((a,
 m) =&gt; DistancePair.best(a, m))

  }

}

 

class

DirectBlockingDistance(words: Array[String], size: Int) extends

TimingTestBase {

 

  val
 matchers = words.grouped(size).map(l =&gt; new

Matcher(l)).toList

 

  def
 shutdown = {}

 

  def
 blockSize = size

 

  /**
 Find best result across all matchers, using direct blocking waits. */

  def
 bestMatch(target: String) = {

    import

ExecutionContext.Implicits.global

    val
 futures = matchers.map(m =&gt; future { m.bestMatch(target) })

    futures.foldLeft(DistancePair.worstMatch)((a,
 v) =&gt;

      DistancePair.best(a,
 Await.result(v, Duration.Inf)))

  }

}









清单 7 中的两个 *Distance 类显示了协调 Matcher 结果的执行和组合的不同方式。ParallelCollectionDistance 使用前面提到的
 Scala 的并行集合 feature 来隐藏并行计算的细节，只需一个简单的 foldLeft 就可以组合结果。

DirectBlockingDistance 更加明确，它创建了一组 future，然后在该列表上为每个结果使用一个 foldLeft 和嵌套的阻塞等待。

性能再分析

清单 7 中的两个 *Distance 实现都是处理 Matcher 结果的合理方法。（它们不仅合理，而且非常高效。示例代码 包含我在试验中尝试的其他两种实现，但未包含在本文中。）在这种情况下，性能是一个主要问题，所以图
 3 显示了这些实现相对于 Java ForkJoin 代码的性能。

图 3. ForkJoinDistance 与 Scala 替代方案的性能对比



图 3 显示，Java ForkJoin 代码的性能比每种 Scala 实现都更好，但 DirectBlockingDistance 在
 1,024 的块大小下提供了更好的性能。两种 Scala 实现在大部分块大小下，都提供了比 清单 1 中的 ThreadPool 代码更好的性能。

这些性能结果仅是演示结果，不具权威性。如果您在自己的系统上运行计时测试，可能会看到不同的性能，尤其在使用不同数量的核心的时候。如果希望为距离任务获得最佳的性能，那么可以实现一些优化：可以按照长度对已知单词进行排序，首先与长度和输入相同的单词进行比较（因为编辑距离总是不低于与单词长度之差）。或者我可以在距离计算超出之前的最佳值时，提前退出计算。但作为一个相对简单的算法，此试验公平地展示了两种并发操作是如何提高性能的，以及不同的工作共享方法的影响。

在性能方面，清单 7 中的 Scale 控制代码与 清单
 2 和 清单 3 中的 Java 代码的对比结果很有趣。Scala 代码短得多，而且（假设您熟悉 Scala！）比 Java 代码更清晰。Scala
 和 Java 可很好的相互操作，您可以在本文的 完整示例代码 中看到：Scala 代码对 Scala 和 Java 代码都运行了计时测试，Java 代码进而直接处理 Scala 代码的各部分。得益于这种轻松的互操作性，您可以将
 Scala 引入现有的 Java 代码库中，无需进行通盘修改。最初使用 Scala 为 Java 代码实现高水平控制常常很有用，这样您就可以充分利用 Scala 强大的表达特性，同时没有闭包或转换的任何重大性能影响。

清单 7 中的 ParallelCollectionDistance Scala
 代码的简单性非常具有吸引力。使用此方法，您可以从代码中完全抽象出并发性，从而编写类似单线程应用程序的代码，同时仍然获得多个处理器的优势。幸运的是，对于喜欢此方法的简单性但又不愿意或无法执行 Scala 开发的人而言，Java 8 带来了一种执行直接的 Java 编程的类似特性。



      原文出处： IBM
 DeveloperWorks 


      在本系列的第一篇文章 《使用递归的方式去思考》中，作者并没有首先介绍 Scala 的语法，这样做有两个原因：一是因为过多的陷入语法的细节当中，会分散读者的注意力，反而忽略了对于基本概念，基本思想的理解；二是因为
 Scala 语法非常简洁，拥有其他语言编程经验的程序员很容易读懂 Scala 代码。现在我们将回过头来，从基本的语法开始学习 Scala 语言。大家会发现 Scala 语言异常精炼，实现同样功能的程序，在代码量上，使用 Scala 实现通常比 Java 实现少一半或者更多。短小精悍的代码常常意味着更易懂，更易维护。本文将为大家介绍 Scala 语言的基本语法，帮助大家写出自己的第一个 Scala 程序。

开发环境

     学习 Scala，最方便的方式是安装一个 Scala 的 IDE（集成开发环境），Typesafe 公司开发了一款基于 Eclipse 的 IDE。该款 IDE 为 Scala 初学者提供了一个方便的功能：Worksheet。像 Python 或者 Ruby 提供的 RELP（Read-Eval-Print Loop）一样，Worksheet 允许用户输入 Scala 表达式，保存后立即得到程序运行结果，非常方便用户体验 Scala 语言的各种特性。如何安装 Scala IDE 和使用 Worksheet，请大家参考 https://class.coursera.org/progfun-002/wiki/view?page=ToolsSetup。

Hello World

     让我们以经典的 Hello World 程序开始，只需在 Worksheet 里输入 println("Hello World!")保存即可，在该语句的右边就可以立刻看到程序执行结果。

    Worksheet 也可以被当作一个简单的计算器，试着输入一些算式，保存。

图 1. Worksheet



变量和函数

      当然，我们不能仅仅满足使用 Scala 来进行一些算术运算。写稍微复杂一点的程序，我们就需要定义变量和函数。Scala 为定义变量提供了两种语法。使用 val定义常量，一经定义后，该变量名不能被重新赋值。使用 var定义变量，可被重新赋值。在
 Scala 中，鼓励使用 val，除非你有明确的需求使用 var。对于
 Java 程序员来说，刚开始可能会觉得有违直觉，但习惯后你会发现，大多数场合下我们都不需要 var，一个可变的变量。

清单 1. 定义变量







1

2

3

4

5

6

7

8

9

10




val

x =

0

var

y =

1

 

y
=

2

//
 给常量赋值会出现编译错误

//
 x = 3 

 

//
 显式指定变量类型

val

x1:

Int =

0

var

y1:

Int =

0








仔细观察上述代码，我们会有两个发现：

      定义变量时没有指定变量类型。这是否意味着 Scala 是和 Python 或者 Ruby 一样的动态类型语言呢？恰恰相反，Scala 是严格意义上的静态类型语言，由于其采用了先进的类型推断（Type
 Inference）技术，程序员不需要在写程序时显式指定类型，编译器会根据上下文推断出类型信息。比如变量 x被赋值为 0，0 是一个整型，所以 x的类型被推断出为整型。当然，Scala
 语言也允许显示指定类型，如变量 x1，y1的定义。一般情况下，我们应尽量使用
 Scala 提供的类型推断系统使代码看上去更加简洁。

     另一个发现是程序语句结尾没有分号，这也是 Scala 中约定俗成的编程习惯。大多数情况下分号都是可省的，如果你需要将两条语句写在同一行，则需要用分号分开它们。

    函数的定义也非常简单，使用关键字 def，后跟函数名和参数列表，如果不是递归函数可以选择省略函数返回类型。Scala 还支持定义匿名函数，匿名函数由参数列表，箭头连接符和函数体组成。函数在 Scala 中属于一级对象，它可以作为参数传递给其他函数，可以作为另一个函数的返回值，或者赋给一个变量。在下面的示例代码中，定义的匿名函数被赋给变量 cube。匿名函数使用起来非常方便，比如 List对象中的一些方法需要传入一个简单的函数作为参数，我们当然可以定义一个函数，然后再传给 List对象中的方法，但使用匿名函数，程序看上去更加简洁。

清单 2. 定义函数







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15




//
 定义函数

def

square(x:

Int):

Int =

  x
 * x                      

//
 如果不是递归函数，函数返回类型可省略

def

sum_of_square(x:

Int, y:

Int) =

  square(x)
 + square(y)  

 

sum_of_square(2,
3)  


 

//
 定义匿名函数

val

cube =

(x:

Int) =&gt;
 x * x *x 

cube(3)


 

//
 使用匿名函数，返回列表中的正数

List(-2,
 -1,
0,
1,
2,
3).filter(x
=&gt;
 x &gt; 0)








      让我们再来和 Java 中对应的函数定义语法比较一下。首先，函数体没有像 Java 那样放在 {}里。Scala 中的一条语句其实是一个表达式，函数的执行过程就是对函数体内的表达式的求值过程，最后一条表达式的值就是函数的返回值。如果函数体只包含一条表达式，则可以省略 {}。其次，没有显示的 return语句，最后一条表达式的值会自动返回给函数的调用者。

      和 Java 不同，在 Scala 中，函数内部还可以定义其他函数。比如上面的程序中，如果用户只对 sum_of_square 函数感兴趣，则我们可以将 square 函数定义为内部函数，实现细节的隐藏。

清单 3. 定义内部函数







1

2

3

4

5




def

sum_of_square(x:

Int, y:

Int):

Int =

{ 

  def

square(x:

Int) =

    x
 * x 

  square(x)
 + square(y) 

}








流程控制语句

       复杂一点的程序离不开流程控制语句，Scala 提供了用于条件判断的 if else和表示循环的 while。和
 Java 中对应的条件判断语句不同，Scala 中的 if else是一个表达式，根据条件的不同返回相应分支上的值。比如下面例子中求绝对值的程序，由于 Scala 中的 if
 else是一个表达式，所以不用像 Java 那样显式使用 return返回相应的值。

清单 4. 使用 if else 表达式







1

2




def

abs(n:

Int):

Int =

if

(n &gt; 0)
 n else

-n








      和 Java 一样，Scala 提供了用于循环的 while 语句，在下面的例子中，我们将借助 while 循环为整数列表求和。

清单 5. 使用 while 为列表求和







1

2

3

4

5

6

7

8

9




def

sum(xs:

List[Int]) =

{ 

  var

total =

0

  var

index =

0

  while

(index &lt; xs.size) { 

    total
 +=

xs(index) 

    index
 +=

1

  }


  total


}








       上述程序是习惯了 Java 或 C++ 的程序员想到的第一方案，但仔细观察会发现有几个问题：首先，使用了 var定义变量，我们在前面说过，尽量避免使用 var。其次，这个程序太长了，第一次拿到这个程序的人需要对着程序仔细端详一会：程序首先定义了两个变量，并将其初始化为 0，然后在 index小于列表长度时执行循环，在循环体中，累加列表中的元素，并将 index加 1，最后返回最终的累加值。直到这时，这个人才意识到这个程序是对一个数列求和。

       让我们换个角度，尝试用递归的方式去思考这个问题，对一个数列的求和问题可以简化为该数列的第一个元素加上由后续元素组成的数列的和，依此类推，直到后续元素组成的数列为空返回 0。具体程序如下，使用递归，原来需要 9 行实现的程序现在只需要两行，而且程序逻辑看起来更清晰，更易懂。（关于如何使用递归的方式去思考问题，请参考作者的另外一篇文章《使用递归的方式去思考》）

清单 6. 使用递归对数列求和







1

2

3

4




  //xs.head
 返回列表里的头元素，即第一个元素

  //xs.tail
 返回除头元素外的剩余元素组成的列表

 def

sum1(xs:

List[Int]):

Int =

if

(xs.isEmpty) 0

else 
xs.head + sum1(xs.tail)








有没有更简便的方式呢？答案是肯定的，我们可以使用列表内置的一些方法达到同样的效果：







1




xs.foldLeft(0)((x0,
 x) =&gt; x0 + x)








       该方法传入一个初始值 0，一个匿名函数，该匿名函数累加列表中的每一个元素，最终返回整个列表的和。使用上面的方法，我们甚至不需要定义额外的方法，就可以完成同样的操作。事实上，List 已经为我们提供了 sum 方法，在实际应用中，我们应该使用该方法，而不是自己定义一个。作者只是希望通过上述例子，让大家意识到 Scala 虽然提供了用于循环的 while 语句，但大多数情况下，我们有其他更简便的方式能够达到同样的效果。

使用牛顿法求解平方根

       掌握了上面这些内容，我们已经可以利用 Scala 求解很多复杂的问题了。比如我们可以利用牛顿法定义一个函数来求解平方根。牛顿法求解平方根的基本思路如下：给定一个数 x，可假设其平方根为任意一个正数 ( 在这里，我们选定 1 为初始的假设 )，然后比较 x与该数的平方，如果两者足够近似（比如两者的差值小于
 0.0001），则该正数即为 x的平方根；否则重新调整假设，假设新的平方根为上次假设与 x/
 上次假设的和的平均数。通过下表可以看到，经过仅仅 4 次迭代，就能求解出相当精确的 2 的平方根。

表 1. 牛顿法求解 2 的平方根




假设

假设的平方与 2 进行比较

新的假设



1

|1 * 1 – 2| = 1

(1 + 2/1)/2 = 1.5



1.5

|1.5 * 1.5 – 2| = 0.25

(1.5 + 2/1.5)/2 = 1.4167



1.4167

|1.4167 * 1.4167 – 2| = 0.0070

(1.4167 + 2/1.4167)/2 = 1.4142



1.4142

|1.4142 * 1.4142 – 2| = 0.000038

……




        将上述算法转化为 Scala 程序，首先我们定义这个迭代过程，这也是该算法的核心部分，所幸这一算法非常简单，利用递归，一个 if else表达式就能搞定。后续为两个辅助方法，让我们的程序看起来更加清晰。最后我们选定初始假设为 1，定义出最终的 sqrt方法。

清单 7. 使用牛顿法求解平方根







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17




//
 迭代函数，若解不满足精度，通过递归调用接续迭代

def

sqrtIter(guess:

Double, x:

Double):

Double =

  if

(isGoodEnough(guess, x)) 

    guess


  else

    sqrtIter((guess
 + x / guess)/2,
 x)          

//
 判断解是否满足要求

def

isGoodEnough(guess:

Double, x:

Double) =

  abs(guess
 * guess - x)&lt; 0.0001               

//
 辅助函数，求绝对值

def

abs(x:

Double) =

  if

(x &lt; 0)
 -x else

x                          

//
 目标函数

def

sqrt(x:

Double):

Double =

  sqrtIter(1,
 x)                                

//
 测试代码  

sqrt(2)








      这段程序看起来相当优美：首先它没有使用 var定义其他辅助变量，在程序中避免使用 var总是一件好事情；其次它没有使用 while循环描述整个迭代过程，取而代之的是一段非常简洁的递归，使程序逻辑上看起来更加清晰；最后它没有将整个逻辑全部塞到一个函数里，而是分散到不同的函数里，每个函数各司其职。然而这段程序也有一个显而易见的缺陷，作为用户，他们只关心 sqrt函数，但这段程序却将其他一些辅助函数也暴露给了用户，我们在前面提到过，Scala
 里可以嵌套定义函数，我们可以将这些辅助函数定义为sqrt的内部函数，更进一步，由于内部函数可以访问其函数体外部定义的变量，我们可以去掉这些辅助函数中的 x参数。最终的程序如下：

清单 8. 使用牛顿法求解平方根 – 使用内部函数隐藏细节







1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17




//
 目标函数，通过将需要用到的辅助函数定义为内部函数，实现细节的隐藏

def

sqrt(x:

Double):

Double =

{ 

  //
 迭代函数，若解不满足精度，通过递归调用接续迭代

  def

sqrtIter(guess:

Double):

Double =

    if

(isGoodEnough(guess)) 

      guess


    else

      sqrtIter((guess
 + x / guess) / 2)


  //
 判断解是否满足要求

  def

isGoodEnough(guess:

Double) =

    abs(guess
 * guess - x) &lt; 0.0001

  //
 辅助函数，求绝对值

  def

abs(x:

Double) =

    if

(x &lt; 0)
 -x else

x 

 

  sqrtIter(1)


}








如何运行 Scala 程序

        我们已经利用 Scala 集成开发环境提供的 Worksheet 体验了 Scala 的基本语法，在实际开发中，我们更关心如何运行 Scala 程序。在运行方式上，Scala 又一次体现出了它的灵活性。它可以被当作一种脚本语言执行，也可以像 Java 一样，作为应用程序执行。

作为脚本执行

       我们可以将 Scala 表达式写在一个文件里，比如 Hello.scala。在命令行中直接输入 scala Hello.scala就可得到程序运行结果。

清单 9. Hello.scala







1




println(“Hello
 Script!”)








作为应用程序执行

       作为应用程序执行时，我们需要在一个单例对象中定义入口函数 main，经过编译后就可以执行该应用程序了。

清单 10. HelloWorld.scala







1

2

3

4

5




object

HelloWorld { 

 def

main(args:

Array[String]):

Unit =

{ 

   println("Hello
 World!")


 }


}








        Scala 还提供了一个更简便的方式，直接继承另一个对象 App，无需定义 main方法，编译即可运行。

清单 11. HelloScala.scala







1

2

3




object

HelloScala extends

App { 

 println("Hello
 Scala!")


}










结束语

        本文为大家介绍了 Scala 的基本语法，相比 Java，Scala 的语法更加简洁，比如 Scala 的类型推断可以省略程序中绝大多数的类型声明，短小精悍的匿名函数可以方便的在函数之间传递，还有各种在 Scala 社区约定俗成的习惯，比如省略的分号以及函数体只有一条表达式时的花括号，这一切都帮助程序员写出更简洁，更优雅的程序。限于篇幅，本文只介绍了 Scala 最基本的语法，如果读者想跟进一步学习 Scala，请参考 Scala 的 官方文档及文末所附的参考资源。

          掌握了这些基本的语法， 就可以用Scala 进行函数式编程，这是 Scala 最让人心动的特性之一，对于习惯了面向对象的程序员来说，学习 Scala 更多的是在学习如何使用 Scala 进行函数式编程。


R语言是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。

R是统计领域广泛使用的诞生于1980年左右的S语言的一个分支。可以认为R是S语言的一种实现。而S语言是由AT&amp;T贝尔实验室开发的一种用来进行数据探索、统计分析和作图的解释型语言。最初S语言的实现版本主要是S-PLUS。S-PLUS是一个商业软件，它基于S语言，并由MathSoft公司的统计科学部进一步完善。后来Auckland大学的Robert Gentleman和Ross Ihaka及其他志愿人员开发了一个R系统。由“R开发核心团队”负责开发。
 R的使用与S-PLUS有很多类似之处，这两种语言有一定的兼容性。S-PLUS的使用手册，只要稍加修改就可作为R的使用手册。所以有人说：R，是S-PLUS的一个“克隆”。

R是基于S语言的一个GNU项目，通常用S语言编写的代码都可以不作修改的在R环境下运行。 R的语法是来自Scheme。R语言是开源的，对所有人是完全免费，自由使用，R语言源代码托管在github上；可以运行在多操作系统中，如Windows、Linux和UNIX等。

R语言资源：

主页：http://www.r-project.orgCRAN: http://cran.r-project.org国内镜像：http://mirror.bjtu.edu.cn/cran/http://mirrors.ustc.edu.cn/CRAN/http://mirror.lzu.edu.cn/CRAN/http://mirrors.xmu.edu.cn/CRAN/

 

R优势及特点

R是一套完整的数据处理、计算和制图软件系统，提供了广泛的统计分析和绘图技术环境：包括线性和非线性模型、统计检验、时间序列、分类、聚类等方法。，包括若干统计程序和强大的各种数学计算、统计计算函数库，用户可以简单地指定数据库和若干参数进行进行一个统计分析，也可以灵活机动的进行数据分析，创造出符合需要的新的统计计算方法。

从 R 语言的发展历史上看，R 主要是统计学家为解决数据分析领域问题而开发的语言，因此 R 具有一些独特的优势：

有效数据存储和处理系统；拥有一整套数组和矩阵的操作运算符（其向量、矩阵运算方面功能尤其强大），统计学家和几乎覆盖整个统计领域的前沿算法（3700+ 扩展包）；完整连贯的统计分析工具；高质量、广泛的统计分析、数据挖掘平台重复性的分析工作（Sweave = R + LATEX），借助 R 语言的强大的分析能力 + LaTeX 完美的排版能力，可以自动生成分析报告；优秀的统计制图、绘图功能，制图具有印刷的素质，也可加入数学符号；一种相当完善、简洁和高效的程序设计语言：可操纵数据的输入和输出，可实现分支、循环，用户可自定义功能；R语言是彻底面向对象的统计编程语言；R语言和其它编程语言、数据库之间有很好的接口；开放的源代码（free, in both senses），可以部署在任何操作系统，比如 Windows, Linux, Mac OS X, BSD, Unix强大的社区支持方便的扩展性可通过相应接口连接数据库，如 Oracle、DB2、MySQL同 Python、Java、C、C++ 等语言进行互调提供 API 接口均可以调用，比如 Google、Twitter、Weibo其他统计软件大部分均可调用 R，比如 SAS、SPSS、Statistica等甚至一些比较直接的商业应用，比如 Oracle R Enterprise, IBM Netezza, R add-on for Teradata, SAP HANA, Sybase RAP

R的功能能够通过由用户撰写的套件增强。增加的功能有特殊的统计技术、绘图功能，以及编程界面和数据输出/输入功能。这些软件包是由R语言、LaTeX、Java及最常用C语言和Fortran撰写。其中有几款较为常用，例如用于经济计量、财经分析、人文科学研究以及人工智能。



 

与Matlab相比，R更具备开放性

R是自由软件，Matlab是商业软件；R可以方便的通过“包”进行扩展，R的核心只有25个包，但是有几千个外部包可以调用，当然你也可以开发自己的；R语言比Matlab的要强大；R和其他编程语言/数据库之间有很好的接口；其他语言也可以很方便的调用R的API和结果对象。R常用于金融和统计领域。大多数人使用R就是因为它的统计功能，R的内部实现了很多经典的or时髦的统计技术。

效果演示

通过一个简单的例子，让R看起来更直观。

在R的控制台输入如下命令：

&gt; install.packages(‘quantmod’) # 安装quantmod包

&gt; require(quantmod) #引用quantmod包

&gt; getSymbols(“GOOG”,src=”yahoo”,from=”2013-01-01″, to=’2013-04-24′) #从雅虎财经获取google的股票数据

&gt; chartSeries(GOOG,up.col=’red’,dn.col=’green’) #显示K线图 &gt; addMACD() #增加MACD图

就能够看到下图的效果了：




　   随着大数据热潮持续延烧，几乎每个产业都有如洪水般倾泻的信息，面对上万笔的顾客浏览纪录、购买行为数据，如果要用 Excel 来进行数据处理真是太不切实际了，Excel 相较于其他统计软件的功能已相去甚远;但如果只会操作统计软件而不会用逻辑分析数据背后的涵义与事实现况相应证的话，那也不过只能做数据处理，替代性很高的工作，而无法深入规划策略的核心。

　　当然，基本功是最不可忽略的环节，想要成为数据科学家，对于这几个程序你应该要有一定的认识：

　　R

　　若要列出所有程序语言，你能忘记其他的没关系，但最不能忘的就是 R。从 1997 年悄悄地出现，最大的优势就是它免费，为昂贵的统计软件像是 Matlab 或 SAS 的另一种选择。

　　但是在过去几年来，它的身价大翻转，变成了资料科学界眼中的宝。不只是木讷的统计学家熟知它，包括 Wall Street 交易员、生物学家，以及硅谷开发者，他们都相当熟悉 R。多元化的公司像是 Google、Facebook、美国银行以及 New York Times 通通都使用 R，它的商业效用持续提高。

　　R 的好处在于它简单易上手，透过 R，你可以从复杂的数据集中筛选你要的数据，从复杂的模型函数中操作数据，建立井然有序的图表来呈现数字，这些都只需要几行程序代码就可以了，打个比方，它就像是好动版本的 Excel。

　　R 最棒的资产就是活跃的动态系统，R 社群持续地增加新的软件包，还有以内建丰富的功能集为特点。目前估计已有超过 200 万人使用 R，最近的调查显示，R 在数据科学界里，到目前为止最受欢迎的语言，占了回复者的 61%(紧追在后的是 39% 的 Python)。

　　它也吸引了 Wall Street 的注目。传统而言，证券分析师在 Excel 档从白天看到晚上，但现在 R 在财务建模的使用率逐渐增加，特别是可视化工具，美国银行的副总裁 Niall O’Conno 说，「R 让我们俗气的表格变得突出」。

　　在数据建模上，它正在往逐渐成熟的专业语言迈进，虽然 R 仍受限于当公司需要制造大规模的产品时，而有的人说他被其他语言篡夺地位了。

　　“R 更有用的是在画图，而不是建模。”顶尖数据分析公司 Metamarkets 的 CEO，Michael Driscoll 表示，
“你不会在 Google 的网页排名核心或是 Facebook 的朋友们推荐算法时看到 R 的踪影，工程师会在 R 里建立一个原型，然后再到 Java 或 Python 里写模型语法”。

　　举一个使用 R 很有名的例子，在 2010 年时，Paul Butler 用 R 来建立 Facebook 的世界地图，证明了这个语言有多丰富多强大的可视化数据能力，虽然他现在比以前更少使用 R 了。

　　“R 已经逐渐过时了，在庞大的数据集底下它跑的慢又笨重”Butler 说。

　　所以接下来他用什么呢？



　　Python

　　如果说 R 是神经质又令人喜爱的 Geek，那 Python 就是随和又好相处的女生。

　　Python 结合了 R 的快速、处理复杂数据采矿的能力以及更务实的语言等各个特质，迅速地成为主流，Python 比起 R，学起来更加简单也更直观，而且它的生态系统近几年来不可思议地快速成长，在统计分析上比起 R 功能更强。

　　Butler 说，“过去两年间，从 R 到 Python 地显著改变，就像是一个巨人不断地推动向前进‘。

　　在数据处理范畴内，通常在规模与复杂之间要有个取舍，而 Python 以折衷的姿态出现。IPythonNotebook(记事本软件)和 NumPy 被用来暂时存取较低负担的工作量，然而 Python 对于中等规模的数据处理是相当好的工具;Python
 拥有丰富的资料族，提供大量的工具包和统计特征。

　　美国银行用 Python 来建立新产品和在银行的基础建设接口，同时也处理财务数据，“Python 是更广泛又相当有弹性，所以大家会对它趋之若鹜。”O’Donnell 如是说。

　　然而，虽然它的优点能够弥补 R 的缺点，它仍然不是最高效能的语言，偶尔才能处理庞大规模、核心的基础建设。Driscoll 是这么认为的。

　　Julia

　　今日大多数的数据科学都是透过 R、Python、Java、Matlab 及 SAS 为主，但仍然存在着鸿沟要去弥补，而这个时候，新进者 Julia 看到了这个痛点。

　　Julia 仍太过于神秘而尚未被业界广泛的采用，但是当谈到它的潜力足以抢夺 R 和 Python 的宝座时，数据黑客也难以解释。原因在于 Julia 是个高阶、不可思议的快速和善于表达的语言，比起 R 要快的许多，比起 Python 又有潜力处理更具规模的数据，也很容易上手。

　　“Julia 会变的日渐重要，最终，在 R 和 Python 可以做的事情在 Julia 也可以”。Butler 是这么认为的。

　　就现在而言，若要说 Julia 发展会倒退的原因，大概就是它太年轻了。Julia 的数据小区还在初始阶段，在它要能够和 R 或 Python 竞争前，它还需要更多的工具包和软件包。

　　Driscoll 说，它就是因为它年轻，才会有可能变成主流又有前景。



　　 Java

　　Driscoll 说，Java 和以 Java 为基础的架构，是由硅谷里最大的几家科技公司的核心所建立的，如果你从 Twitter、Linkedin 或是 Facebook 里观察，你会发现 Java 对于所有数据工程基础架构而言，是非常基础的语言。

　　Java 没有和 R 和 Python 一样好的可视化功能，它也不是统计建模的最佳工具，但是如果你需要建立一个庞大的系统、使用过去的原型，那 Java 通常会是你最基的选择。

　　 Hadoop and Hive

　　为了迎合大量数据处理的需求，以 Java 为基础的工具群兴起。Hadoop 为处理一批批数据处理，发展以 Java 为基础的架构关键;相较于其他处理工具，Hadoop 慢许多，但是无比的准确和可被后端数据库分析广泛使用。和 Hive 搭配的很好，Hive 是基于查询的架构下，运作的相当好。

　　Scala

　　又是另一个以 Java 为基础的语言，和 Java 很像，对任何想要进行大规模的机械学习或是建立高阶的算法，Scala 会是逐渐兴起的工具。它是善于呈现且拥有建立可靠系统的能力。

　　“Java 像是用钢铁建造的;Scala 则是让你能够把它拿进窑烤然后变成钢的黏土”Driscoll 说。

　　Kafka and Storm

　　说到当你需要快速的、实时的分析时，你会想到什么？Kafka 将会是你的最佳伙伴。其实它已经出现五年有了，只是因为最近串流处理兴起才变的越来越流行。

　　Kafka 是从 Linkedin 内诞生的，是一个特别快速的查询讯息系统。Kafka 的缺点呢？就是它太快了，因此在实时操作时它会犯错，有时候会漏掉东西。

　　鱼与熊掌不可兼得，「必须要在准确度跟速度之间做一个选择」，Driscoll 说。所以全部在硅谷的科技大公司都利用两个管道：用 Kafka 或 Storm 处理实时数据，接下来打开 Hadoop 处理一批批处理数据系统，这样听起来有点麻烦又会有些慢，但好处是，它非常非常精准。

　　Storm 是另一个从 Scala 写出来的架构，在硅谷逐渐大幅增加它在串流处理的受欢迎程度，被 Twitter 并购，这并不意外，因为 Twitter 对快速事件处理有极大的兴趣。



　　 Matlab

　　Matlab 可以说是历久不衰，即使它标价很高;在非常特定的利基市场它使用的相当广泛，包括密集的研究机器学习、信号处理、图像辨识等等。

　　 Octave

　　Octave 和 Matlab 很像，除了它是免费的之外。然而，在学术信号处理的圈子，几乎都会提到它。

　　 GO

　　GO 是另一个逐渐兴起的新进者，从 Google 开发出来的，放宽点说，它是从 C 语言来的，并且在建立强大的基础架构上，渐渐地成为 Java 和 Python 的竞争者。

　　这么多的软件可以使用，但我认为不见得每个都一定要会才行，知道你的目标和方向是什么，就选定一个最适合的工具使用吧!可以帮助你提升效率又达到精准的结果。


      许多分布式计算系统都可以实时或接近实时地处理大数据流。下面对三种Apache框架分别进行简单介绍，然后尝试快速、高度概述其异同。


Apache Storm

在Storm中，先要设计一个用于实时计算的图状结构，我们称之为拓扑（topology）。这个拓扑将会被提交给集群，由集群中的主控节点（master node）分发代码，将任务分配给工作节点（worker
 node）执行。一个拓扑中包括spout和bolt两种角色，其中spout发送消息，负责将数据流以tuple元组的形式发送出去；而bolt则负责转换这些数据流，在bolt中可以完成计算、过滤等操作，bolt自身也可以随机将数据发送给其他bolt。由spout发射出的tuple是不可变数组，对应着固定的键值对。






Apache Spark

Spark Streaming是核心Spark API的一个扩展，它并不会像Storm那样一次一个地处理数据流，而是在处理前按时间间隔预先将其切分为一段一段的批处理作业。Spark针对持续性数据流的抽象称为DStream（DiscretizedStream），一个DStream是一个微批处理（micro-batching）的RDD（弹性分布式数据集）；而RDD则是一种分布式数据集，能够以两种方式并行运作，分别是任意函数和滑动窗口数据的转换。






Apache Samza

Samza处理数据流时，会分别按次处理每条收到的消息。Samza的流单位既不是元组，也不是Dstream，而是一条条消息。在Samza中，数据流被切分开来，每个部分都由一组只读消息的有序数列构成，而这些消息每条都有一个特定的ID（offset）。该系统还支持批处理，即逐次处理同一个数据流分区的多条消息。Samza的执行与数据流模块都是可插拔式的，尽管Samza的特色是依赖Hadoop的Yarn（另一种资源调度器）和Apache
 Kafka。






共同之处

以上三种实时计算系统都是开源的分布式系统，具有低延迟、可扩展和容错性诸多优点，它们的共同特色在于：允许你在运行数据流代码时，将任务分配到一系列具有容错能力的计算机上并行运行。此外，它们都提供了简单的API来简化底层实现的复杂程度。

三种框架的术语名词不同，但是其代表的概念十分相似：






对比图

下面表格总结了一些不同之处：






数据传递形式分为三大类：



最多一次（At-most-once）：消息可能会丢失，这通常是最不理想的结果。最少一次（At-least-once）：消息可能会再次发送（没有丢失的情况，但是会产生冗余）。在许多用例中已经足够。恰好一次（Exactly-once）：每条消息都被发送过一次且仅仅一次（没有丢失，没有冗余）。这是最佳情况，尽管很难保证在所有用例中都实现。



另一个方面是状态管理：对状态的存储有不同的策略，Spark Streaming将数据写入分布式文件系统中（例如HDFS）；Samza使用嵌入式键值存储；而在Storm中，或者将状态管理滚动至应用层面，或者使用更高层面的抽象Trident。

用例

这三种框架在处理连续性的大量实时数据时的表现均出色而高效，那么使用哪一种呢？选择时并没有什么硬性规定，最多就是几个指导方针。

如果你想要的是一个允许增量计算的高速事件处理系统，Storm会是最佳选择。它可以应对你在客户端等待结果的同时，进一步进行分布式计算的需求，使用开箱即用的分布式RPC（DRPC）就可以了。最后但同样重要的原因：Storm使用Apache Thrift，你可以用任何编程语言来编写拓扑结构。如果你需要状态持续，同时/或者达到恰好一次的传递效果，应当看看更高层面的Trdent API，它同时也提供了微批处理的方式。






使用Storm的公司有：Twitter，雅虎，Spotify还有The Weather Channel等。

说到微批处理，如果你必须有状态的计算，恰好一次的递送，并且不介意高延迟的话，那么可以考虑Spark Streaming，特别如果你还计划图形操作、机器学习或者访问SQL的话，Apache Spark的stack允许你将一些library与数据流相结合（Spark SQL，Mllib，GraphX），它们会提供便捷的一体化编程模型。尤其是数据流算法（例如：K均值流媒体）允许Spark实时决策的促进。

使用Spark的公司有：亚马逊，雅虎，NASA JPL，eBay还有百度等。

如果你有大量的状态需要处理，比如每个分区都有许多十亿位元组，那么可以选择Samza。由于Samza将存储与处理放在同一台机器上，在保持处理高效的同时，还不会额外载入内存。这种框架提供了灵活的可插拔API：它的默认execution、消息发送还有存储引擎操作都可以根据你的选择随时进行替换。此外，如果你有大量的数据流处理阶段，且分别来自不同代码库的不同团队，那么Samza的细颗粒工作特性会尤其适用，因为它们可以在影响最小化的前提下完成增加或移除的工作。

使用Samza的公司有：LinkedIn，Intuit，Metamarkets，Quantiply，Fortscale等。

结论

本文中我们只对这三种Apache框架进行了简单的了解，并未覆盖到这些框架中大量的功能与更多细微的差异。同时，文中这三种框架对比也是受到限制的，因为这些框架都在一直不断的发展，这一点是我们应当牢记的。

     Python可以称为大数据全栈式开发语言。因为Python在云基础设施，DevOps，大数据处理等领域都是炙手可热的语言。
                         



就像只要会JavaScript就可以写出完整的Web应用，只要会Python，就可以实现一个完整的大数据处理平台。

云基础设施

这年头，不支持云平台，不支持海量数据，不支持动态伸缩，根本不敢说自己是做大数据的，顶多也就敢跟人说是做商业智能（BI）。

云平台分为私有云和公有云。私有云平台如日中天的 OpenStack ，就是Python写的。曾经的追赶者CloudStack，在刚推出时大肆强调自己是Java写的，比Python有优势。结果，搬石砸脚，2015年初，CloudStack的发起人Citrix宣布加入OpenStack基金会，CloudStack眼看着就要寿终正寝。

如果嫌麻烦不想自己搭建私有云，用公有云，不论是AWS，GCE，Azure，还是阿里云，青云，在都提供了Python SDK，其中GCE只提供Python和JavaScript的SDK，而青云只提供Python SDK。可见各家云平台对Python的重视。

提到基础设施搭建，不得不提Hadoop，在今天，Hadoop因为其MapReduce数据处理速度不够快，已经不再作为大数据处理的首选，但是HDFS和Yarn——Hadoop的两个组件——倒是越来越受欢迎。Hadoop的开发语言是Java，没有官方提供Python支持，不过有很多第三方库封装了Hadoop的API接口（pydoop，hadoopy等等）。

Hadoop MapReduce的替代者，是号称快上100倍的 Spark ，其开发语言是Scala，但是提供了Scala，Java，Python的开发接口，想要讨好那么多用Python开发的数据科学家，不支持Python，真是说不过去。HDFS的替代品，比如GlusterFS， Ceph 等，都是直接提供Python支持。Yarn的替代者， Mesos 是C++实现，除C++外，提供了Java和Python的支持包。

DevOps

DevOps有个中文名字，叫做开发自运维。互联网时代，只有能够快速试验新想法，并在第一时间，安全、可靠的交付业务价值，才能保持竞争力。DevOps推崇的自动化构建/测试/部署，以及系统度量等技术实践，是互联网时代必不可少的。

自动化构建是因应用而易的，如果是Python应用，因为有setuptools, pip, virtualenv, tox, flake8等工具的存在，自动化构建非常简单。而且，因为几乎所有Linux系统都内置Python解释器，所以用Python做自动化，不需要系统预安装什么软件。

自动化测试方面，基于Python的 Robot
 Framework 企业级应用最喜欢的自动化测试框架，而且和语言无关。Cucumber也有很多支持者，Python对应的Lettuce可以做到完全一样的事情。 Locust 在自动化性能测试方面也开始受到越来越多的关注。

自动化配置管理工具，老牌的如Chef和Puppet，是Ruby开发，目前仍保持着强劲的势头。不过，新生代 Ansible 和 SaltStack ——均为Python开发——因为较前两者设计更为轻量化，受到越来越多开发这的欢迎，已经开始给前辈们制造了不少的压力。

在系统监控与度量方面，传统的Nagios逐渐没落，新贵如 Sensu 大受好评，云服务形式的New
 Relic已经成为创业公司的标配，这些都不是直接通过Python实现的，不过Python要接入这些工具，并不困难。

除了上述这些工具，基于Python，提供完整DevOps功能的PaaS平台，如 Cloudify和 Deis ，虽未成气候，但已经得到大量关注。

网络爬虫

大数据的数据从哪里来？除了部分企业有能力自己产生大量的数据，大部分时候，是需要靠爬虫来抓取互联网数据来做分析。

网络爬虫是Python的传统强势领域，最流行的爬虫框架Scrapy，HTTP工具包urlib2，HTML解析工具beautifulsoup，XML解析器lxml，等等，都是能够独当一面的类库。

不过，网络爬虫并不仅仅是打开网页，解析HTML这么简单。高效的爬虫要能够支持大量灵活的并发操作，常常要能够同时几千甚至上万个网页同时抓取，传统的线程池方式资源浪费比较大，线程数上千之后系统资源基本上就全浪费在线程调度上了。Python由于能够很好的支持协程（ Coroutine ）操作，基于此发展起来很多并发库，如Gevent，Eventlet，还有Celery之类的分布式任务框架。被认为是比AMQP更高效的ZeroMQ也是最早就提供了Python版本。有了对高并发的支持，网络爬虫才真正可以达到大数据规模。

抓取下来的数据，需要做分词处理，Python在这方面也不逊色，著名的自然语言处理程序包NLTK，还有专门做中文分词的Jieba，都是做分词的利器。

数据处理

万事俱备，只欠东风。这东风，就是数据处理算法。从统计理论，到数据挖掘，机器学习，再到最近几年提出来的深度学习理论，数据科学正处于百花齐放的时代。数据科学家们都用什么编程？

如果是在理论研究领域，R语言也许是最受数据科学家欢迎的，但是R语言的问题也很明显，因为是统计学家们创建了R语言，所以其语法略显怪异。而且R语言要想实现大规模分布式系统，还需要很长一段时间的工程之路要走。所以很多公司使用R语言做原型试验，算法确定之后，再翻译成工程语言。

Python也是数据科学家最喜欢的语言之一。和R语言不同，Python本身就是一门工程性语言，数据科学家用Python实现的算法，可以直接用在产品中，这对于大数据初创公司节省成本是非常有帮助的。正式因为数据科学家对Python和R的热爱，Spark为了讨好数据科学家，对这两种语言提供了非常好的支持。

Python的数据处理相关类库非常多。高性能的科学计算类库NumPy和SciPy，给其他高级算法打了非常好的基础，matploglib让Python画图变得像Matlab一样简单。Scikit-learn和Milk实现了很多机器学习算法，基于这两个库实现的 Pylearn2 ，是深度学习领域的重要成员。 Theano 利用GPU加速，实现了高性能数学符号计算和多维矩阵计算。当然，还有 Pandas ，一个在工程领域已经广泛使用的大数据处理类库，其DataFrame的设计借鉴自R语言，后来又启发了Spark项目实现了类似机制。


对了，还有 iPython ，这个工具如此有用，以至于我差点把他当成标准库而忘了介绍。iPython是一个交互式Python运行环境，能够实时看到每一段Python代码的结果。默认情况下，iPython运行在命令行，可以执行 ipython
 notebook 在网页中运行。用matplotlib绘制的图可以直接嵌入式的显示在iPython Notebook中。

iPython Notebook的笔记本文件可以共享给其他人，这样其他人就可以在自己的环境中重现你的工作成果；如果对方没有运行环境，还可以直接转换成HTML或者PDF。


正是因为应用开发工程师、运维工程师、数据科学家都喜欢Python，才使得Python成为大数据系统的全栈式开发语言。

对于开发工程师而言，Python的优雅和简洁无疑是最大的吸引力，在Python交互式环境中，执行 import this ，读一读Python之禅，你就明白Python为什么如此吸引人。Python社区一直非常有活力，和NodeJS社区软件包爆炸式增长不同，Python的软件包增长速度一直比较稳定，同时软件包的质量也相对较高。有很多人诟病Python对于空格的要求过于苛刻，但正是因为这个要求，才使得Python在做大型项目时比其他语言有优势。OpenStack项目总共超过200万行代码，证明了这一点。

对于运维工程师而言，Python的最大优势在于，几乎所有Linux发行版都内置了Python解释器。Shell虽然功能强大，但毕竟语法不够优雅，写比较复杂的任务会很痛苦。用Python替代Shell，做一些复杂的任务，对运维人员来说，是一次解放。

对于数据科学家而言，Python简单又不失强大。和C/C++相比，不用做很多的底层工作，可以快速进行模型验证；和Java相比，Python语法简洁，表达能力强，同样的工作只需要1/3代码；和Matlab，Octave相比，Python的工程成熟度更高。不止一个编程大牛表达过，Python是最适合作为大学计算机科学编程课程使用的语言——MIT的计算机入门课程就是使用的Python——因为Python能够让人学到编程最重要的东西——如何解决问题。

   微软参加2015年PyCon ，高调宣布提高Python在Windows上的编程体验，包括Visual
 Studio支持Python，优化Python的C扩展在Windows上的编译等等。补充了未来Python作为Windows默认组件的场景。


·        大数据应用正在从概念走向现实，而企业在大数据应用开发时，软件的弹性（Resilient）正在成为决定大数据应用成败的关键因素。弹性差的应用无法应对大规模的数据集，在测试和运营中也缺乏透明度，而且也不安全。
                                                    

·        避免大数据应用在生产环境中掉链子的最佳办法就是在开发阶段就开发弹性应用，例如：健壮性、经过测试、可改变、可审计、高安全、可监控。
·        可以说，开发出弹性大数据应用既是一个技术工作，也是一个哲学问题。Concurrent的SupreetOberoi近日撰文提出大数据应用开发八大基本原则:
·        一、为弹性大数据应用描绘一个蓝图
·        第一步是为企业大数据应用创建一个系统的架构和方法，要处理什么数据？那些类型的分析最重要？软件架构需要承载那些指标、审计、安全和运营功能？
·        另外一些需要考虑的问题：那些技术最关键？哪些技术只是图一时之便？你的蓝图需要准确评估当前架构的问题所在。
·        二、数据规模不再是问题
·        如果应用无法处理更大规模的数据集，那么它就缺乏弹性，弹性应用应当能够处理任意规模的数据集（包括数据深度、广度、频度等），数据弹性还只对新技术的兼容，缺乏弹性的应用需要不断配置修改应用来适应不断更新的大数据技术，对于企业来说是时间、资源和金钱上的无底洞。
·        三、透明度
·        对于复杂应用来说，查找扩展性等弹性相关问题还很难实现自动化。关键是锁定问题的根源所在：是代码、数据还是架构抑或网络问题？并非每个应用都要具备这种透明度，但大一些的平台应当具备足够的透明度，让所有开发者和运营人员都能在问题发生时立刻找到根源并采取措施。
·        一旦发现问题，最为关键的是将找到应用行为对应的代码——最好是通过发现问题的监控应用。大多数情况下，访问代码会涉及到多个开发人员，执行起来流程将非常曲折。
·        四、抽象，事关高效和简洁
·        弹性应用总是面向未来的，通常采用抽象层来简化开发、提升效率，允许采用不同的技术实现。作为架构的一部分，弹性开发的抽象层能够避免开发者陷入技术实现的细节泥潭中。简洁性则能方便数据科学家使用应用访问所有类型的数据源。如果没有抽象技术，产品的生产力会大打折扣，修改成本增高，而用户则为复杂性所困扰。
·        五、安全：审计与合规
·        弹性应用能自我审计，能够显示谁使用了应用，谁有权限使用，访问了哪些数据以及政策如何实施。在应用开发阶段就将这些功能考虑进去是应对日益增长的大数据隐私、安全、治理和控制挑战的关键所在。
·        六、完整度与测试驱动的开发
·        弹性应用的一个基本要求就是不能遗失任何数据，数据完整性的丧失往往会导致严重的后果，例如金融企业会因为程序代码弄丢了一两行交易数据而在反洗钱或金融欺诈调查中遭受处罚。
·        七、数据便携性
·        不断发展的业务需求驱动技术不断做出改变，因此，大数据应用也应当能够在多个平台和产品上运行。最终的目标是让最终用户能够通过SQL和标准API访问数据（无论是否实时）。例如，一个先进的大数据平台应当允许原本由Hadoop存储MapReduce处理的数据，转移到Spark或Tez中进进行处理，而且这个过程不需要或尽可能少地改动代码。
·        八、不要搞个人主义
·       大数据应用的开发不应当依赖某个高手的个人才华，代码应当在多个开发者之间分享、评估和保有。这个策略让整个团队，而不是个人，对应用质量负责。


       数据科学家目前可谓是炙手可热的职业。 关于数据科学家的职业发展的讨论有很多。最近Louis Dorard在GigaOM上发表了一篇关于数据科学家职业发展的博文。观点是随着数据科学的发展， 目前数据科学家的许多工作将被自动化的工具取代。 而数据科学家这个职业也将不复存在。

                                                                  

       数据科学家的工作的一部分就是把他们的工作自动化。 比如说通过一些预测性的API工具来实现工作的自动化。 然而， 这些API已经在某些领域开始取代数据科学家的工作了。这对这个职业来说可不是什么好事。

       我们现在处于大数据2.0时代。 利用机器学习来进行预测性分析的需求越来越强劲。正如InsightsOne的CEO Waqar Hasan指出的那样“预测分析是大数据时代的‘杀手级应用’。”而麦肯锡也预测说在今后的几年内，关于机器学习的人才将会出现短缺。 与此同时， 我们也开始看到有一些公司开始针对大众提供机器学习和预测分析的服务。 例如Apigee在收购了InsightsOne后就推出了预测性分析的API平台。

       我在上大学计算机科学的时候学到的第一课就是“我们的工作的终极目标就是要让我们自己没有工作。”我们的工作就是要让程序把我们现在的工作做得更快，更好，更可靠。数据科学也是如此。

技术将取代数据科学家

      数据科学家的绝大部分工作花在了建立预测模型：选取与预测相关的变量。选择合适的模型，确定最优的参数等等。目前，这类的工作已经能够有一些自动化的解决方案了。如Emerald Logic的FACET以及Google和Erastz Labs提供的API。这些API把复杂的机器学习模型从数据中抽象出来。用户可以专注于数据的采集以及清洗，而把数据送给这些API，就能够生成一个预测模型了。 

      这些新的工具意味着，在这种新的模式下，不需要数据科学家的参与了，公司里的每个人都能够参与数据科学的项目。高管确定战略方向，中层经理们确定分析预测的具体目标，软件工程师可以专注于项目实施。这里需要每个人都多少懂一些机器学习。不过如果不去深究算法和理论，只关注基本概念和一些具体的应用实例的话，机器学习即使对于非技术人员来说也能够很快了解。

      事实上，如果由具体应用领域的专家来负责机器学习项目的话，往往能够更好地将应用领域的知识结合到机器学习项目中去，比如能够更好的选出那些合适的特征变量，从而能够做出更好的预测模型。

      机器学习是“人工智能“的技术。通过数据来建立更好的”智能“。那么我们在人工智能领域还需要手工去进行模型和算法的选择吗？我们当然有智能的自动的方式来实现。在人工智能领域有一个趋势，就是”元人工智能算法（meta AI Algorithm）“，也就是对给定问题，能够自动的找到合适的人工智能算法和合适的参数。

      利用这种方式来进行机器学习的原理就是利用如概率推理来进行参数设定以及对特征变量设定不同权重等等。也可以采用穷举的方法来进行。       今天我们的计算能力已经足以让我们进行这样大量的测试。穷举测试可以采用常规的交叉验证，或者采用类似于FACET那样的渐进式技术。

测试可以从对数据的最简单分析开始，比如如果我们发现数据在二元分类时有明显的不平衡性时，我们可以试着选择异常检测的算法。

      数据科学家将来做什么呢？

     有人会说， 目前不能自动化的领域太多了。 确实，把所有机器学习领域都自动化是很困难的。 不过， 目前API在预测方面已经能够比拟那些“传统“的分析技术了。 这方面API创造的价值巨大。

     由于这些新的工具的出现， 数据科学家的角色也在发生变化。现在要成为数据科学家可能要比以前容易了。 由于预测性API的出现， 原来由数据科学家来做的工作变得更加容易了。这些工作可以由数据库工程师或者软件工程师来进行了。 这也就是有些人说的“数据科学不是科学”。 而我要说的是， 数据科学正在不断演进。

     在预测API领域，数据科学家依然在团队里扮演重要角色。 他帮助团队成员自主地使用这些API。 他们更多地是作为一个主管的角色来指导大家使用，而不像以前那样需要亲自动手。

     更重要的是， 数据科学家还需要不断开发机器学习的自动化工具。例如，出来目前的“监督学习（Supervised Learning “的API外，也开始出现了“强化学习(Reinforcement Learning)“的API。此外， 还需要提供一些工具能够使得具体应用领域专家能够把他们的知识更方便地融入到算法中去。

           大数据是一个含义广泛的术语，是指数据集，如此庞大而复杂的，他们需要专门设计的硬件和软件工具进行处理。该数据集通常是万亿或EB的大小。这些数据集收集自各种各样的来源：传感器，气候信息，公开的信息，如杂志，报纸，文章。大数据产生的其他例子包括购买交易记录，网络日志，病历，军事监控，视频和图像档案，及大型电子商务。 

           在大数据和大数据分析，他们对企业的影响有一个兴趣高涨。大数据分析是研究大量的数据的过程中寻找模式，相关性和其他有用的信息，可以帮助企业更好地适应变化，并做出更明智的决策。 


一、Hadoop
Hadoop 是一个能够对大量数据进行分布式处理的软件框架。但是 Hadoop
是以一种可靠、高效、可伸缩的方式进行处理的。Hadoop 是可靠的，因为它假设计算元素和存储会失败，因此它维护多个工作数据副本，确保能够针对失败的节点重新分布处理。Hadoop 是高效的，因为它以并行的方式工作，通过并行处理加快处理速度。Hadoop 还是可伸缩的，能够处理 PB 级数据。此外，Hadoop 依赖于社区服务器，因此它的成本比较低，任何人都可以使用。
                                                                     

Hadoop是一个能够让用户轻松架构和使用的分布式计算平台。用户可以轻松地在Hadoop上开发和运行处理海量数据的应用程序。它主要有以下几个优点：
     ⒈高可靠性。Hadoop按位存储和处理数据的能力值得人们信赖。
     ⒉高扩展性。Hadoop是在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便地扩展到数以千计的节点中。
     ⒊高效性。Hadoop能够在节点之间动态地移动数据，并保证各个节点的动态平衡，因此处理速度非常快。
     ⒋高容错性。Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。
Hadoop带有用 Java 
语言编写的框架，因此运行在Linux 生产平台上是非常理想的。Hadoop 上的应用程序也可以使用其他语言编写，比如 C++。 
二、HPCC
     HPCC，High Performance Computing and Communications（高性能计算与通信）的缩写。1993年，由美国科学、工程、技术联邦协调理事会向国会提交了“重大挑战项目：高性能计算与
 通信”的报告，也就是被称为HPCC计划的报告，即美国总统科学战略项目，其目的是通过加强研究与开发解决一批重要的科学与技术挑战问题。HPCC是美国 实施信息高速公路而上实施的计划，该计划的实施将耗资百亿美元，其主要目标要达到：开发可扩展的计算系统及相关软件，以支持太位级网络传输性能，开发千兆比特网络技术，扩展研究和教育机构及网络连接能力。 
                                
该项目主要由五部分组成：
1、高性能计算机系统（HPCS），内容包括今后几代计算机系统的研究、系统设计工具、先进的典型系统及原有系统的评价等；
2、先进软件技术与算法（ASTA），内容有巨大挑战问题的软件支撑、新算法设计、软件分支与工具、计算计算及高性能计算研究中心等；
3、国家科研与教育网格（NREN），内容有中接站及10亿位级传输的研究与开发；
4、基本研究与人类资源（BRHR），内容有基础研究、培训、教育及课程教材，被设计通过奖励调查者-开始的，长期 的调查在可升级的高性能计算中来增加创新意识流，通过提高教育和高性能的计算训练和通信来加大熟练的和训练有素的人员的联营，和来提供必需的基础架构来支持这些调查和研究活动；
5、信息基础结构技术和应用（IITA
），目的在于保证美国在先进信息技术开发方面的领先地位。 
三、Storm
     Storm是自由的开源软件，一个分布式的、容错的实时计算系统。Storm可以非常可靠的处理庞大的数据流，用于处理Hadoop的批量数据。Storm很简单，支持许多种编程语言，使用起来非常有趣。Storm由Twitter开源而来，其它知名的应用企业包括Groupon、淘宝、支付宝、阿里巴巴、乐元素、Admaster等等。 
    Storm有许多应用领域：实时分析、在线机器学习、不停顿的计算、分布式RPC（远过程调用协议，一种通过网络从远程计算机程序上请求服务）、 ETL（Extraction-Transformation-Loading的缩写，即数据抽取、转换和加载）等等。Storm的处理速度惊人：经测 试，每个节点每秒钟可以处理100万个数据元组。Storm是可扩展、容错，很容易设置和操作。
                                                                 
四、Apache Drill
    为了帮助企业用户寻找更为有效、加快Hadoop数据查询的方法，Apache软件基金会近日发起了一项名为“Drill”的开源项目。Apache
 Drill 实现了 Google's Dremel.
据Hadoop厂商MapR Technologies公司产品经理Tomer Shiran介绍，“Drill”已经作为Apache孵化器项目来运作，将面向全球软件工程师持续推广。
    该项目将会创建出开源版本的谷歌Dremel Hadoop工具（谷歌使用该工具来为Hadoop数据分析工具的互联网应用提速）。而“Drill”将有助于Hadoop用户实现更快查询海量数据集的目的。
“Drill”项目其实也是从谷歌的Dremel项目中获得灵感：该项目帮助谷歌实现海量数据集的分析处理，包括分析抓取Web文档、跟踪安装在Android Market上的应用程序数据、分析垃圾邮件、分析谷歌分布式构建系统上的测试结果等等。
    通过开发“Drill”Apache开源项目，组织机构将有望建立Drill所属的API接口和灵活强大的体系架构，从而帮助支持广泛的数据源、数据格式和查询语言。
五、RapidMiner
RapidMiner是世界领先的数据挖掘解决方案，在一个非常大的程度上有着先进技术。它数据挖掘任务涉及范围广泛，包括各种数据艺术，能简化数据挖掘过程的设计和评价。
功能和特点
免费提供数据挖掘技术和库
100%用Java代码（可运行在操作系统）
数据挖掘过程简单，强大和直观
内部XML保证了标准化的格式来表示交换数据挖掘过程 
可以用简单脚本语言自动进行大规模进程
多层次的数据视图，确保有效和透明的数据
图形用户界面的互动原型
命令行（批处理模式）自动大规模应用
Java API（应用编程接口）
简单的插件和推广机制
强大的可视化引擎，许多尖端的高维数据的可视化建模 
400多个数据挖掘运营商支持
耶鲁大学已成功地应用在许多不同的应用领域，包括文本挖掘，多媒体挖掘，功能设计，数据流挖掘，集成开发的方法和分布式数据挖掘。
六、   Pentaho BI
     Pentaho BI 平台不同于传统的BI产品，它是一个以流程为中心的，面向解决方案（Solution）的框架。其目的在于将一系列企业级BI产品、开源软件、API等等组件集成起来，方便商务智能应用的开发。它的出现，使得一系列的面向商务智能的独立产品如Jfree、Quartz等等，能够集成在一起，构成一项项复杂的、完整的商务智能解决方案。
    Pentaho BI 平台，Pentaho Open BI 套件的核心架构和基础，是以流程为中心的，因为其中枢控制器是一个工作流引擎。工作流引擎使用流程定义来定义在BI 平台上执行的商业智能流程。流程可以很容易的被定制，也可以添加新的流程。BI 平台包含组件和报表，用以分析这些流程的性能。目前，Pentaho的主要组成元素包括报表生成、分析、数据挖掘和工作流管理等等。这些组件通过J2EE、WebService、SOAP、HTTP、Java、JavaScript、Portals等技术集成到Pentaho平台中来。
 Pentaho的发行，主要以Pentaho SDK的形式进行。
    Pentaho SDK共包含五个部分：Pentaho平台、Pentaho示例数据库、可独立运行的Pentaho平台、Pentaho解决方案示例和一个预先配制好的 Pentaho网络服务器。其中Pentaho平台是Pentaho平台最主要的部分，囊括了Pentaho平台源代码的主体；Pentaho数据库为 Pentaho平台的正常运行提供的数据服务，包括配置信息、Solution相关的信息等等，对于Pentaho平台来说它不是必须的，通过配置是可以用其它数据库服务取代的；可独立运行的Pentaho平台是Pentaho平台的独立运行模式的示例，它演示了如何使Pentaho平台在没有应用服务器支持的情况下独立运行；Pentaho解决方案示例是一个Eclipse工程，用来演示如何为Pentaho平台开发相关的商业智能解决方案。
    Pentaho BI 平台构建于服务器，引擎和组件的基础之上。这些提供了系统的J2EE 服务器，安全，portal，工作流，规则引擎，图表，协作，内容管理，数据集成，分析和建模功能。这些组件的大部分是基于标准的，可使用其他产品替换之。
 

大数据应用开发的12个辅助开发工具：

                                            

    在大数据应用的开发中，除了基础的Hadoop或者R语言之外，还有很多优秀的开发工具，能使开发者如虎添翼。
    “兵欲善其事，必先利其器”，无论你是从事大数据应用的开发，还是希望分析你的移动应用，这些工具都可以帮助你更快更好的发展。
 1） BitDeli
    BitDeli是一家刚刚成立的公司，他的产品就是可以让开发者利用Python脚本， 对应用进行分析。 Python脚本可以很简单， 也可以很复杂。一切可以由开发者来定制。 BitDeli的产品甚至包含了一些机器学习的模块。相比较庞大的Hadoop分析工具， BitDeli把自己比作是应用分析方面的RoR (Ruby on Rail)。
                                   

2） Continuity
    Continuity的两个创始人是雅虎的前首席云架构师Todd Papaioannou和前Facebook的HiBase工程师Jonathan Gray。 他们的目的， 就是想使客户能够像雅虎和Facebook
利用大数据。 Continuity的产品App Fabric主要是把复杂的与Hadoop和HBase集群的工作作为一个抽象层， 提供各种大数据开发工具，来满足企业内部或外部数据需求。
                                         

3） Flurry
     Flurry是个一站式的移动应用商店，它每年已经有100美元的营收了。 它不但帮助开发者在它的平台上开发，还可以帮助开发者通过运营数据， 对应用进行分析以改善应用。而且， Flurry平台还可以和广告网络相对接，帮助开发者更好的实现盈利。
                                             

4） Google Predictive API
      在Google提供的众多开发工具中， Google Predictive API 是最酷的一个了。如果你有好的训练样本数据， 那么GooglePredictive API将会利用机器学习算法来为你的应用建立模型，并把这些模型整合到你的应用中去。 在Google给出的例程里，包括了反垃圾邮件， 推荐引擎，以及情绪分析等模型的搭建步骤和代码。
                                               

5） Infochimps
      尽管Infochimps把自己定义为面向企业的IT平台， 他们大数据平台对开发者来说也非常有帮助。它的技术平台叫做Wukong（悟空）， 用来进行大数据环境的配置和管理。从起一个Hadoop进程到用Ruby脚本来传递数据流。 Wukong都可以让开发者的工作变得更简单。此外， Infochimps的平台还包括了一个数据市场，以API或者下载方式提供各类数据。
                                              

6） Keen IO
       Keen IO是个强大的移动应用分析工具。开发者只需要简单到一行代码， 就可以跟踪他们想要的关于他们应用的任何信息。开发者接下来只需要做一些Dashboard或者查询的工作就可以了。
                                             

7）Kontagent
       Kontagent是一个移动，社交， 网站应用的分析平台。它基于Hadoop大数据平台而建。今年， Kontagent平台增添了新功能。它可以让用户直接采用Hive查询语言对数据进行任意查询和分析，而不像原来那样只能进行预定义的查询和分析
               
8） Mortar Data
       Mortar Data是专为开发者打造的Hadoop开发平台，它用Pig和Python的组合替代了MapReduce以便开发者能简单地编写Hadoop管道（Pipeline）。 今年11月， 它推出了Mortar Data开源开发框架，利用开源社区来推动数据共享。 Mortar Data平台运行在亚马逊的云平台上。 支持亚马逊S3以及MangoDB。
9） Placed Analytics
       利用脚本语言以及API， PlacedAnalytics能够提供针对移动和网络应用的详细用户行为分析。包括， 用户使用时间和地理位置信息。 这些可以帮助开发者的应用更好地吸引广告商， 也可以帮助开发者对自己的应用进行改善。
                                                          


10） Precog
      Precog提供的是一个基于开源查询语言Quirrel的交互式开发环境，名为Labcoat。可以帮助开发者进行应用分析的开发。这个IDE环境还提供了关于Quirrel的教程，以及其他一些复杂的函数。 用公司首席运营官Jeff Carr的话来说：“就算是一个非技术人员， 几个小时也能掌握基本功能。”
11） Spring For Apache Hadoop
     尽管Hadoop是用Java写的，但是这并不意味着， 在Hadoop上编程或者使用Hadoop对Java程序员来说就很简单。因此， 在2012年初， SpringSource宣布推出了SpringFor Apache Hadoop项目。 把Spring开发框架和Hadoop结合起来。 这样也便于其他的Spring应用或者基于Java虚拟机的脚本， 更好地和Hadoop以及利用Hadoop的其他技术如Hive或者HBase进行整合。
12） StatMix
      和BitDeli以及Keen IO一样，StatMix也希望能够使开发者用他们所熟知的语言来进行数据查询与分析。 因此， 出来提供预定义的查询之外， StatMix也提供API和代码库来让开发者定制化的查询。并可以把不同数据源的查询结果整合在一个定制的Dashboard里。
                                                     


13） Spark
         Spark是一个基于内存计算的开源集群计算系统，目的是更快速的进行数据分析。Spark由加州伯克利大学AMP实验室Matei为主的小团队使用Scala开发开发，其核心部分的代码只有63个Scala文件，非常轻量级。 Spark 提供了与 Hadoop 相似的开源集群计算环境，但基于内存和迭代优化的设计，Spark 在某些工作负载表现更优秀。
         在2014上半年，Spark开源生态系统得到了大幅增长，已成为大数据领域最活跃的开源项目之一，当下已活跃在Hortonworks、IBM、Cloudera、MapR和Pivotal等众多知名大数据公司。那么Spark究竟以什么吸引了如此多的关注，这里我们看向Dzone上的6个总结。
（1）      轻量级快速处理。着眼大数据处理，速度往往被置于第一位，我们经常寻找能尽快处理我们数据的工具。Spark允许Hadoop集群中的应用程序在内存中以100倍的速度运行，即使在磁盘上运行也能快10倍。Spark通过减少磁盘IO来达到性能提升，它们将中间处理数据全部放到了内存中。
 
        Spark使用了RDD（Resilient Distributed Dataset）的理念，这允许它可以透明的内存中存储数据，只在需要时才持久化到磁盘。这种做法大大的减少了数据处理过程中磁盘的读写，大幅度的降低了所需时间。
（2）      易于使用，Spark支持多语言。Spark允许Java、Scala及Python，这允许开发者在自己熟悉的语言环境下进行工作。它自带了80多个高等级操作符，允许在shell中进行交互式查询。
（3）      支持复杂查询。在简单的“map”及“reduce”操作之外，Spark还支持SQL查询、流式查询及复杂查询，比如开箱即用的机器学习机图算法。同时，用户可以在同一个工作流中无缝的搭配这些能力。
（4）       实时的流处理。对比MapReduce只能处理离线数据，Spark支持实时的流计算。Spark依赖Spark Streaming对数据进行实时的处理，当然在YARN之后Hadoop也可以借助其他的工具进行流式计算。对于Spark Streaming，Cloudera的评价是：
 
                                                                                                                       
·        简单：轻量级且具备功能强大的API，Sparks Streaming允许你快速开发流应用程序。
·        容错：不像其他的流解决方案，比如Storm，无需额外的代码和配置，Spark Streaming就可以做大量的恢复和交付工作。
·        集成：为流处理和批处理重用了同样的代码，甚至可以将流数据保存到历史数据中。
 5.   可以与Hadoop和已存Hadoop数据整合。Spark可以独立的运行，除了可以运行在当下的YARN集群管理之外，它还可以读取已有的任何Hadoop数据。这是个非常大的优势，它可以运行在任何Hadoop数据源上，比如HBase、HDFS等。这个特性让用户可以轻易迁移已有Hadoop应用，如果合适的话。
6.   活跃和无限壮大的社区。Spark起源于2009年，当下已有超过50个机构250个工程师贡献过代码，和去年六月相比，代码行数几乎扩大三倍，这是个令人艳羡的增长。

大数据的顶级开源工具：
         大数据方面的顶级开源工具，分为四个领域：数据存储，开发平台，开发工具和集成，分析和报告工具。
随着大数据与预测分析的成熟，开源作为底层技术授权解决方案的最大贡献者的优势越来越明显。
如今，从小型初创企业到行业巨头，各种规模的供应商都在使用开源来处理大数据和运行预测分析。借助开源与云计算技术，新兴公司甚至在很多方面都可以与大厂商抗衡。

         以下是一些大数据方面的顶级开源工具，分为四个领域：数据存储、开发平台、开发工具和集成、分析和报告工具。
数据存储：
·        ApacheHadoop– Cloud Foundry(VMware), Hortonworks, Hadapt
·        NoSql 数据库 – MongoDB, Cassandra, Hbase
·        SQL 数据库 – MySql(Oracle), MariaDB, PostgreSQL, TokuDB
开发平台：
·        Apache Hadoop平台 – Impala(开源大数据分析引擎); Lingual(ANSISQL); Pattern(analytics);Cascading(开源大数据应用程序开发框架)
·        ApacheLucene和 Solr平台
·        OpenStack(构建私有云和公有云)
·        Red Hat (搭载 Hadoop 服务器的标准 Linux 发行版)
·        REEF(微软的Hadoop开发者平台)
·        Storm(集成了各种排队系统和数据库系统)
开发工具和集成：
·        ApacheMahout(机器学习的编程语言)
·        Python 和 R(预测分析编程语言)
分析和报告工具：
·        Jaspersoft(报告和分析服务器)
·        Pentaho(数据集成和业务分析)
·        Splunk(IT分析平台)
·        Talend(大数据集成，数据管理和应用集成)
    不足之处有待补充，欢迎大家评论留言！

                                      
　　近年来，随着云和大数据时代的来临，数据可视化产品已经不再满足于使用传统的数据可视化工具来对数据仓库中的数据抽取、归纳并简单的展现。传统的数据可视化工具仅仅将数据加以组合，通过不同的展现方式提供给用户，用于发现数据之间的关联信息。新型的数据可视化产品必须满足互联网爆发的大数据需求，必须快速的收集、筛选、分析、归纳、展现决策者所需要的信息，并根据新增的数据进行实时更新。因此，在大数据时代，数据可视化工具必须具有以下特性：
(1)实时性：数据可视化工具必须适应大数据时代数据量的爆炸式增长需求，必须快速的收集分析数据、并对数据信息进行实时更新;
(2)简单操作：数据可视化工具满足快速开发、易于操作的特性，能满足互联网时代信息多变的特点;
(3)更丰富的展现：数据可视化工具需具有更丰富的展现方式，能充分满足数据展现的多维度要求;
(4)多种数据集成支持方式：数据的来源不仅仅局限于数据库，数据可视化工具将支持团队协作数据、数据仓库、文本等多种方式，并能够通过互联网进行展现。
                                                                                          


一、       目前全球备受欢迎的的可视化工具：
1.Excel
Excel作为一个入门级工具，是快速分析数据的理想工具，也能创建供内部使用的数据图，但是Excel在颜色、线条和样式上课选择的范围有限，这也意味着用Excel很难制作出能符合专业出版物和网站需要的数据图。
2.Google Chart API
Google Chart提供了一种非常完美的方式来可视化数据，提供了大量现成的图标类型，从简单的线图表到复杂的分层树地图等。它还内置了动画和用户交互控制。
3.D3
D3(Data Driven Documents)是支持SVG渲染的另一种JavaScript库。但是D3能够提供大量线性图和条形图之外的复杂图表样式，例如Voronoi图、树形图、圆形集群和单词云等。
4.R
R语言是主要用于统计分析、绘图的语言和操作环境。虽然R主要用于统计分析或者开发统计相关的软件，但也有用作矩阵计算。其分析速度可比美GNUOctave甚至商业软件MATLAB。
5.Visual.ly
　　如果你需要制作信息图而不仅仅是数据可视化，Visual.ly是最流行的一个选择。
6. Processing
Processing是数据可视化的招牌工具。你只需要编写一些简单的代码，然后编译成Java。Processing可以在几乎所有平台上运行。
7.Leaflet
Leaflet是一个开源的JavaScript库，用来开发移动友好地交互地图。
8.Openlayers
Openlayers可能是所有地图库中可靠性最高的一个。虽然文档注释并不完善。且学习曲线非常陡峭，但是对于特定的任务来说，Openlayers能够提供一些其他地图库都没有的特殊工具。
9.PolyMaps
PolyMaps是一个地图库，主要面向数据可视化用户。PolyMaps在地图风格化方面有独到之处，类似CSS样式表的选择器。
10.Charting Fonts
Charting Fonts是将符号字体与字体整合(把符号变成字体)，创建出漂亮的矢量化图标。
11.Gephi
Gephi是进行社会图谱数据可视化分析的工具，不但能处理大规模数据集并且Gephi是一个可视化的网络探索平台，用于构建动态的、分层的数据图表。
12.CartoDB
CartoDB是一个不可错过的网站，你可以用CartoDB很轻易就把表格数据和地图关联起来，这方面CartoDB是最优秀的选择。
13.Weka
Weka是一个能根据属性分类和集群大量数据的优秀工具，Weka不但是数据分析的强大工具，还能生成一些简单的图表。
14.NodeBox
NodeBox是OS X上创建二维图形和可视化的应用程序，你需要了解Python程序，NodeBox与Processing类似，但没有Processing的互动功能。
        https://www.nodebox.net/code/index.php/Home
15.Kartograph
Kartograph不需要任何地图提供者像Google Maps，用来建立互动式地图，由两个libraries组成，从空间数据开放格式，利用向量投影的Python
 library以及post GIS，并将两者结合到SVG和JavaScript library，并把这些SVG资料转变成互动性地图。
16.Modest Maps
Modest Maps是一个很小的地图库，在一些扩展库的配合下，例如Wax、Modest Maps立刻会变成一个强大的地图工具。
17.Tangle
Tangle是一个用来探索，Play和可以立即查看文档更新的交互工具。
18.Crossfilter
Crossfilter既是图表，又是互动图形用户界面的小程序，当你调整一个图表中的输入范围时，其他关联图表的数据也会随之改变
19.Raphael
Raphael是创建图表和图形的JavaScript库，与其他库最大的不同是输出格式仅限SVG和VML. http://raphaeljs.com/
20.jsDraw2DX
jsDraw2DX是一个标准的JavaScript库，用来创建任意类型的SVG交互式图形，可生成包括线、矩形、多边形、椭圆、弧线等图形。http://jsdraw2dx.jsfiction.com/
21.BPizza Pie Charts
  BPizza Pie Charts是个响应式饼图图表，基于Adobe Snap SVG框架，通过HTML标记和CSS来替代JavaScript对象，更容易集成各种先进的技术。
22.Fusion Charts Suit XT
Fusion Charts Suit XT是一款跨平台、跨浏览器的JavaScript图表组件，为你提供令人愉悦的JavaScript图表体验。它是最全面的图表解决方案，包含90+图表类型和众多交互功能，包括3D、各种仪表、工具提示、向下钻取、缩放和滚动等。它拥有完整的文档以及现成的演示，可以帮助你快速创建图表。
23.iCharts
iCharts提供可一个用于创建并呈现引人注目图表的托管解决方案。有许多不同种类的图表可供选择，每种类型都完全可定制，以适合网站的主题。iCharts有交互元素，可以从Google
 Doc、Excel表单和其他来源中获取数据。
24.Modest Maps
Modest Maps是一个轻量级、可扩展的、可定制的和免费的地图显示类库，这个类库能帮助开发人员在他们自己的项目里能够与地图进行交互。
25.Raw
Raw局域非常流行的D3.js库开发，支持很多图表类型，例如泡泡图、映射图、环图等。它可以使数据集在途、复制、粘贴、拖曳、删除于一体，并且允许我们定制化试图和层次。
26.Springy
Springy设计清凉并且简答。它提供了一个抽象的图形处理和计算的布局，支持Canvas、SVG、WebGL、HTML元素。
27.Bonsai
Bonsai使用SVG作为输出方式来生成图形和动画效果，拥有非常完整的图形处理API，可以使得你更加方便的处理图形效果。它还支持渐变和过滤器(灰度、模糊、不透明度)等效果。
28.Cube
Cube是一个开源的系统，用来可视化时间系列数据。它是基于MongoDB、NodeJS和D3.js开发。用户可以使用它为内部仪表板构建实时可视化的仪表板指标。
29.Gantti
Gantti是一个开源的PHP类，帮助用户即时生成Gantti图表。使用Gantti创建图表无需使用JavaScript，纯HTML-CSS3实现。图表默认输出非常漂亮，但用户可以自定义样式进行输出(SASS样式表)。
30.Smoothie Charts
Smoothie Charts是一个十分小的动态流数据图表路。通过推送一个webSocket来显示实时数据流。Smoothie Charts只支持Chorme和Safari浏览器，并且不支持刻印文字或饼图，它很擅长显示流媒体数据。
31.Flot
Flot是一个优秀的线框图表库，支持所有支持canvas的浏览器(目前主流的浏览器如火狐、IE、Chrome等都支持)。
32.Tableau Public
Tableau Public是一款桌面可视化工具，用户可以创建自己的数据可视化，并将交互性数据可视化发布到网页上。
33.Many Eyes
Many Eyes是一个Web应用程序，用来创建、分享和讨论用户上传图形数据。
34.Anychart
Anychart是一个灵活的基于Flash/JavaScript(HTML5)的图表解决方案、跨浏览器、跨平台。除了图表功能外，它还有一款收费的交互式图表和仪表。
35.Dundas Chart
Dundas Chart处于行业领先地位的NET图表处理控件，于2009年被微软收购，并将图表产品的一部分功能集成到Visual
 Studio中。
36.TimeFlow
TimeFlow Analytical Timeline是为了暂时性资料的视觉化工具，现在有alpha版本因此有机会可以发现差错，提供以下不同的呈现方式：时间轴、日历、柱状图、表格等。
37.Protovis
Protovis是一个可视化JavaScript图表生成工具。
38.Choosel
Choosel是可扩展的模块化Google网络工具框架，可用来创建基于网络的整合了数据工作台和信息图表的可视化平台。
39.Zoho Reports
Zoho Reports支持丰富的功能帮助不同的用户解决各种个性化需求，支持SQL查询、类四暗自表格界面等。
40.Quantum GIS(QDIS)
Quantum GIS(QDIS)是一个用户界面友好、开源代码的GIS客户端程序，支持数据的可视化、管理、编辑与分析和印刷地图的制作。
41.NodeXL
NodeXLDE 主要功能是社交网络可视化。
42.OpenStreetMap
OpenStreetMap是一个世界地图，由像您一样的人们所构筑，可依据开放协议自由使用。
43.OpenHeatMap
OpenHeatMap简单易用，用户可以用它上传数据、创建地图、交流信息。它可以把数据(如Google Spreadsheet的表单)转化为交互式的地图应用，并在网上分享。
44.Circos
Circos最初主要用于基因组序列相关数据的可视化，目前已应用于多个领域，例如：影视作品中的人物关系分析，物流公司的订单来源和流向分析等，大多数关系型数据都可以尝试用Circos来可视化。
45.Impure
Impure是一个可视化编程语言，旨在收集、处理可视化信息。
46.Polymaps
Polymaps是一个基于矢量和tile创建动态、交互式的动态地图。
47.Rickshaw
Rickshaw是一个基于D3.JS来创建序交互式的时间序列图表库。
48.Sigma.js
Sigma.js是一个开源的轻量级库，用来显示交互式的静态和动态图表。
49.Timeline
Timeline即时间轴，用户通过这个工具可以一目了然的知道自己在何时做了什么。
50.BirdEye
BirdEye是Decearative Visual Analytics，它属于一个群体专案，为了要提升设计和广泛的开源资料视觉化发展，并且为了Adobe
 Flex建视觉分析图库，这个动作以叙述性的资料库为主，让使用者能够建立多元资料视觉化界面来分析以及呈现资讯。
51.Arbor.Js
Arbor.Js提供有效率、以力导向的版面配置演算法，抽象画图表组织以及筛选更新的处理。
52.Highchart.js
Highchart.js是单纯由JavaScript所写的图表资料库，提供简单的方法来增加互动性图表来表达你的网站或网站应用程式。目前它能支援线图、样条函数图。
53.Paper.js
Paper.js是一个开源向量图表叙述架构，能够在HTML5 Canvas
运作，对于初学者来说它是很容易学习的，其中也有很多专业面向可以提供中阶及高阶使用者。
54.Visualize Free
Visualize Free是一个建立在高阶商业后台集游InetScoft开发的视觉化软体免费的视觉分析工具，可从多元变量资料筛选并看其趋势，或是利用简单地点及方法来切割资料或是小范围的资料。
55.GeoCommons
GeoCommons可以使用户构建富交互可视化应用来解决问题，即使他们没有任何传统地图使用经验。你可以将实社会化数据或者GeoCommons保存的超5万份开源数据在地图上可视化，创造带交互的可视化分析作品，并将作品嵌入网站、博客或分享到社交网络上。
 

         量化交易就是一种基于数据和数学模型，对金融市场进行分析、判断和交易的策略和算法。量化交易通过复杂算法和计算机程序，可在金融市场每一个微小的波动中，快速买进（做多）和卖出（做空）股票等金融产品，这样不管金融市场的基本面是向上还是向下，都能从波动中赚钱。此外，由于是程序化交易，也打消了人为交易过程中的情绪影响。
        量化交易通过大数据和云计算的数据分析计算，一方面也可以很容易地让证券公司的交易数据变化生成自己的交易策略，另一方面，可以避开小散交易微数据的影响，做出及时准确的市场判断和合理及时的交易策略。
         量化交易及时准确的把策略方案展现给客户，让选择租用符合风险和收益预期的策略，这些策略都是100万起投、年化收益可高达50%以上、策略月租费在几百至千元左右。普通投资者还可以按投票分类、投资风格、持股数量等搜索相应的交易策略，再通过与各大券商，如，恒泰证券、国信证券、中信证券和海信证券进行实盘操作。也可以以众筹得方式来下单，通过各持资比例进行基金债券分红和股息分配。
         近两年随着大数据的快速发展，我们正在进入算法经济时代。以众包的方式做量化投资，让普通投资人通过也能有自己的交易机器人，正成为新经济下的新常态。

       应用程序接口API:英文全称Application Program Interface。
    实际就是一组定义、程序及协议的集合，通过 API 接口实现计算机软件之间的相互通信。API 的一个主要功能是提供通用功能集。程序员通过调用 API 函数对应用程序进行开发，可以减轻编程任务。
 API 同时也是一种中间件，为各种不同平台提供数据共享。


根据单个或分布式平台上不同软件应用程序间的数据共享性能，可以将 API 分为四种类型:

(1)远程过程调用(RPC)API:通过作用在共享数据缓存器上的过程(或任务)实现程序间的通信。

(2)标准查询语言(SQL)API:是标准的访问数据的查询语言，通过数据库实现应用程序间的数据共享。

(3)文件传输API:文件传输通过发送格式化文件实现应用程序间数据共享。

(4)信息交付API:指松耦合或紧耦合应用程序间的小型格式化信息，通过程序间的直接通信实现数据共享。
       比较典型的就是我们VC，MFC，VC++开发常用的Windows API；Windows 这个多作业系统除了协调应用程式的执行、分配内存、管理系统资源…之外，
 她同时也是一个很大的服务中心，调用这个服务中心的各种服务(每一种服务就是一个函数)，可以帮应用程式达到开启视窗、描绘图形、使用周边设备…等目的，由於这些函数服务的对象是应用程式(Application)， 所以便称之为 Application Programming Interface，简称 API 函数。可视化编程环境操作简单、界面友好(诸如VB、VC++、DELPHI等)，在这些工具中提供了大量的类库和各种控件，它们替代了API的神秘功能，事实上这些类库和控件都是构架在WIN32
 API函数基础之上的，是封装了的API函数的集合。它们把常用的API函数的组合在一起成为一个控件或类库，并赋予其方便的使用方法，所以极大的加速了WINDOWS应用程序开发的过程。有了这些控件和类库，程序员便可以把主要精力放在程序整体功能的设计上，而不必过于关注技术细节。
 实际上如果我们要开发出更灵活、更实用、更具效率的应用程序，必然要涉及到直接使用API函数，虽然类库和控件使应用程序的开发简单的多，但它们只提供WINDOWS的一般功能，对于比较复杂和特殊的功能来说，使用类库和控件是非常难以实现的，这时就需要采用API函数来实现。
 
     API函数包含在位于系统目录下的DLL文件中.我们可以自己输入API函数的声明来调用,有些开发平台自己提供更加简单的调用方式，如VB提供了一种更简单的方法,即使用API Text Viewer. 要想在你的工程中声明API函数,只需运行API Text Viewer,打开Win32api.txt(或.MDB如果你已经把它转换成了数据库的话,这样可以加快速度.
 
     开放API(OpenAPI)：


开放API(OpenAPI)是服务型网站常见的一种应用，网站的服务商将自己的网站服务封装成一系列API(Application
 Programming Interface，应用编程接口)开放出去，供第三方开发者使用，这种行为就叫做开放网站的API，所开放的API就被称作OpenAPI(开放API)。

网站提供开放平台的API后，可以吸引一些第三方的开发人员在该平台上开发商业应用，平台提供商可以获得更多的流量与市场份额，第三方开发者不需要庞大的硬件与技术投资就可以轻松快捷的创业，从而达到双赢的目的，开放API是大平台发展、共享的途径，让开发者开发一个有价值应用，付出的成本更少，成功的机会更多。今天，OpenAPI作为互联网在线服务的发展基础，已经成为越来越多互联网企业发展服务的必然选择。


     OpenCV是 Open Source Computer Vision Library的缩写，是一个基于(开源)发行的跨平台计算机视觉库，可以运行在Linux、Windows和Mac
 OS操作系统上。它轻量级而且高效--由一系列 C 函数和少量 C++ 类构成，同时提供了Python、Ruby、MATLAB等语言的接口，实现了图像处理和计算机视觉方面的很多通用算法。OpenCV用C++语言编写，它的主要接口也是C++语言，但是依然保留了大量的C语言接口。该库也有大量的Python,
 Java and MATLAB/OCTAVE (版本2.5)的接口。这些语言的API接口函数可以通过在线文档获得。如今也提供对于C#,Ch, Ruby的支持。所有新的开发和算法都是用C++接口。一个使用CUDA的GPU接口也于2010年底开始实现。
    OpenCV（Open Source
 Computer Vision Library） 由Intel公司在背后提供支持。它包含了超过500个函数来实现用于图似懂非懂形处理和计算机视觉方面的飞车通用算法。

    OpenCV致力于视觉市场新标准的API研发；将简化计算机视觉程序和解决方案的开发。打破了传统多依赖硬件的一些特别的解决方案（比如视频监控，制造控制系统，医疗设备）
这是目前的现状。简化计算机视觉程序和解决方案的开发。
 
   OpenCV致力于真实世界的实时应用，通过优化的C代码的编写对其执行速度带来了可观的提升，并且可以通过购买Intel的IPP高性能多媒体函数库(Integrated
 Performance Primitives)得到更快的处理速度。
 
  OpenCV将为视觉开发和虚拟现实开发带来一场革命！

        首先，有几个主要的IT和开发趋势将在未来几年给测试带来重要的影响。了解这些，对我们接下来的准备工作和策略制定是至关重要的。
这些即将到来的广泛趋势，让测试人员了解到会面对怎样的挑战。
连接一切


        移动技术的定义正在快速的转变，以适应新的类别，如“wearables”和“smart car”技术。日益复杂的软件能够满足满足我们生活的方方面面，从家庭安全到家庭应用，并将它们连接在一起。无线连接有着小巧、便宜的优点，并推动着物联网（IOT）的前进。Gartner预测2016年将会有64亿的事务连接在一起，比2015年增长30%。
随着物理和虚拟世界之间的距离不断缩小，对测试也提出了一些新的问题。范围在扩大，情景设置更加动态化（也许变化更大）。为了确保全线质量，测试者需要看看所测试的设备范围，以及它们如何在新的系统环境中与其他东西进行交互。我们不可能控制每一台物理设备，因此移动设备 farm模式会广泛运用。固体仿真服务器（Solid
 emulationservices）对于降低成本和管理测试覆盖率是至关重要的。
速度是关键


        敏捷开发的方法已经成为常态。随着企业进一步推崇持续集成和持续交付，秉承追求速度而不妥协质量的信条，对测试团队的要求也更高了。这一趋势将促使打破各部门之间的壁垒。根据Gartner的数据显示，2016年将有25%的全球2000强企业采用DevOps。开发和运营的结合，意味着测试也要追随它们的步伐。
为了解决这一需求，一部分要借助于自动化测试。自动化测试可以满足更快、更频繁的软件发布。如果有机会，测试人员应该尽可能的学习编程知识，以便能够编写自动化测试脚本。我们不应该将手动测试人员和软件测试工程师区分开来。团队对于测试人员的需求数量正在飙升。
现在对于可重复任务进行基于风险的测试评估，大家持有肯定的态度。测试人员可以从商业的角度制定出自动化的最佳人选。对于哪些组件需要进行手动测试，他们需要做出快速、战略性决策。若是对商业价值没有很好的理解，那么很难做出有效地决策。
数据处理和安全性


        我们不能再把设备看做一个个单独的事物。底层软件将以跨多种设备为目标，这意味着测试人员必须找到验证数据交换和同步的方法。功能测试套件必须采用数据处理机制。
现在，安全漏洞造成的影响越来越严重。数据必须安全地收集和存储。合规性是强制性的，它必须经过验证。如果不能正确地测试应用程序的安全性可能会导致严重的违规行为，其后果可能是灾难性的。
云端后


        许多现代设备仅仅在windows上与其他软件交互。通常使用API来访问一个远处基于云的架构。为了测试云端幕后的一切，需要一个有效的方法论。我们为了确保数据包不丢失，需要对数据处理进行一些分析，以及满足性能基准。
        验证对于数据和活动记录是必要的。为测试人员提供诊断评估的访问，可窥见幕后情况，降低风险，最大限度地减少解决问题所花费的时间。由于企业将原有的系统转移到基于云计算的架构中，或者更常见的是，两者的混合系统。因此，我们需要对现有的测试技术进行发展和创新，以满足快速灵活的业务决策。
 

   机器学习流水线（Machine Learning Pipelines）


     大数据决定企业的竞争力！
   


        互联网+引领者嵌入式发展的未来！
   

P2P金融：
      P2P金融指个人与个人间的小额借贷交易，一般需要借助电子商务专业网络平台帮助借贷双方确立借贷关系并完成相关交易手续。借款者可自行发布借款信息，包括金额、利息、还款方式和时间，实现自助式借款；借出者根据借款人发布的信息，自行决定借出金额，实现自助式借贷。
云计算：

    云计算（英语：Cloud Computing），是一种基于互联网的计算方式，通过这种方式，共享的软硬件资源和信息可以按需提供给计算机和其他设备。典型的云计算提供商往往提供通用的网络业务应用，可以通过浏览器等软件或者其他Web服务来访问，而软件和数据都存储在服务器上。云计算服务通常提供通用的通过浏览器访问的在线商业应用，软件和数据可存储在数据中心。

人工智能：


“人工智能”一词最初是在1956 年Dartmouth学会上提出的。从那以后，研究者们发展了众多理论和原理，人工智能的概念也随之扩展。人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。

人工智能是对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。
对等计算：
     对等计算（Peer to Peer，简称p2p）可以简单的定义成通过直接交换来共享计算机资源和服务，而对等计算模型应用层形成的网络通常称为对等网络。在P2P网络环境中，成千上万台彼此连接的计算机都处于对等的地位，整个网络一般来说不依赖专用的集中服务器。网络中的每一台计算机既能充当网络服务的请求者，又对其它计算机的请求作出响应，提供资源和服务。通常这些资源和服务包括：信息的共享和交换、计算资源（如CPU的共享）、存储共享（如缓存和磁盘空间的使用）等。

对等网络：
 
     对等网络又称工作组，网上各台计算机有相同的功能，无主从之分，一台计算机都是既可作为服务器，设定共享资源供网络中其他计算机所使用，又可以作为工作站，没有专用的服务器，也没有专用的工作站。对等网络是小型局域网常用的组网方式。
大数据：



大数据(big data,mega data)，或称巨量资料，指的是需要新处理模式才能具有更强的决策力、洞察力和流程优化能力的海量、高增长率和多样化的信息资产。

在维克托.迈尔-舍恩贝格及肯尼斯·库克耶编写的《大数据时代》中大数据指不用随机分析法(抽样调查)这样的捷径，而采用所有数据进行分析处理。大数据的4V特点:Volume(大量)、Velocity(高速)、Variety(多样)、Value(价值)。
LBS：


基于位置的服务，它是通过电信移动运营商的无线电通讯网络(如GSM网、CDMA网)或外部定位方式(如GPS)获取移动终端用户的位置信息(地理坐标，或大地坐标)，在地理信息系统(外语缩写:GIS、外语全称:Geographic Information System)平台的支持下，为用户提供相应服务的一种增值业务。

基于位置的服务，是指通过电信移动运营商的无线电通讯网络或外部定位方式，获取移动终端用户的位置信息，在GIS平台的支持下，为用户提供相应服务的一种增值业务。

它包括两层含义:首先是确定移动设备或用户所在的地理位置;其次是提供与位置相关的各类信息服务。意指与定位相关的各类服务系统，简称"定位服务"，另外一种叫法为MPS-Mobile Position Services, 也称为"移动定位服务"系统。如找到手机用户的当前地理位置，然后在上海市6340平方公里范围内寻找手机用户当前位置处1公里范围内的宾馆、影院、图书馆、加油站等的名称和地址。所以说LBS就是要借助互联网或无线网络，在固定用户或移动用户之间，完成定位和服务两大功能。
O2O：


O2O即Online To Offline(在线离线/线上到线下)，是指将线下的商务机会与互联网结合，让互联网成为线下交易的平台，这个概念最早来源于美国。O2O的概念非常广泛，既可涉及到线上，又可涉及到线下,可以通称为O2O。主流商业管理课程均对O2O这种新型的商业模式有所介绍及关注。

2013年O2O进入高速发展阶段，开始了本地化及移动设备的整合和完善，于是O2O商业模式横空出世，成为O2O模式的本地化分支。

O2O的优势在于把网上和网下的优势完美结合。通过网购导购机，把互联网与地面店完美对接，实现互联网落地。让消费者在享受线上优惠价格的同时，又可享受线下贴身的服务。同时，O2O模式还可实现不同商家的联盟。

1、O2O模式充分利用了互联网跨地域、无边界、海量信息、海量用户的优势，同时充分挖掘线下资源，进而促成线上用户与线下商品与服务的交易，团购就是O2O的典型代表。

2、O2O模式可以对商家的营销效果进行直观的统计和追踪评估，规避了传统营销模式的推广效果不可预测性，O2O将线上订单和线下消费结合，所有的消费行为均可以准确统计，进而吸引更多的商家进来，为消费者提供更多优质的产品和服务。

3、O2O在服务业中具有优势，价格便宜，购买方便，且折扣信息等能及时获知。

4、将拓宽电子商务的发展方向，由规模化走向多元化。

5、O2O模式打通了线上线下的信息和体验环节，让线下消费者避免了因信息不对称而遭受的“价格蒙蔽”，同时实现线上消费者“售前体验”。

整体来看O2O模式运行的好，将会达成“三赢”的效果；

对本地商家来说，O2O模式要求消费者网站支付，支付信息会成为商家了解消费者购物信息的渠道，方便商家对消费者购买数据的搜集，进而达成精准营销的目的，更好地维护并拓展客户。通过线上资源增加的顾客并不会给商家带来太多的成本，反而带来更多利润。此外，O2O模式在一定程度上降低了商家对店铺地理位置的依赖，减少了租金方面的支出。

对消费者而言，O2O提供丰富、全面、及时的商家折扣信息，能够快捷筛选并订购适宜的商品或服务，且价格实惠。

对服务提供商来说，O2O模式可带来大规模高黏度的消费者，进而能争取到更多的商家资源。掌握庞大的消费者数据资源，且本地化程度较高的垂直网站借助O2O模式，还能为商家提供其他增值服务。

中国经济依托互联网，经济虚拟化，是中国经济形态全球化的一场博弈豪赌，现在网上关于这方面的教程有很多，但是很多都是过时的，因为互联网是在不断的变化的，为了避免让大家学到错误的过时的知识，我联合互联网上的牛人，组建了一个群，想学SEO和电商的小伙伴，可以来这里学习，群号：62775887，我想说的是，除非你想学习这方面的知识，让自己获取互联网机会，如果只是凑热闹的话，就不要来了。

